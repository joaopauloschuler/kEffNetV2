{"cells":[{"cell_type":"markdown","metadata":{"id":"2JXWocLnz38N"},"source":["You might need to install this on your system:\n","\n","apt-get install python3-opencv git"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6740,"status":"ok","timestamp":1644795531840,"user":{"displayName":"Joao Schuler","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10970916006391371112"},"user_tz":180},"id":"PQKpflNl7m63","outputId":"a2b5de48-3b04-43bf-fb4b-4f6b4f159b76"},"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'k'...\n","remote: Enumerating objects: 1581, done.\u001b[K\n","remote: Counting objects: 100% (1030/1030), done.\u001b[K\n","remote: Compressing objects: 100% (674/674), done.\u001b[K\n","remote: Total 1581 (delta 747), reused 608 (delta 351), pack-reused 551\u001b[K\n","Receiving objects: 100% (1581/1581), 15.08 MiB | 22.68 MiB/s, done.\n","Resolving deltas: 100% (1103/1103), done.\n","Processing /content/k\n","\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n","   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n","Requirement already satisfied: pandas>=0.22.0 in /usr/local/lib/python3.7/dist-packages (from cai==0.1.6) (1.3.5)\n","Requirement already satisfied: scikit-image>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from cai==0.1.6) (0.18.3)\n","Requirement already satisfied: opencv-python>=4.1.2.30 in /usr/local/lib/python3.7/dist-packages (from cai==0.1.6) (4.1.2.30)\n","Requirement already satisfied: scikit-learn>=0.21.0numpy in /usr/local/lib/python3.7/dist-packages (from cai==0.1.6) (1.0.2)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python>=4.1.2.30->cai==0.1.6) (1.19.5)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.22.0->cai==0.1.6) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.22.0->cai==0.1.6) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.22.0->cai==0.1.6) (1.15.0)\n","Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->cai==0.1.6) (1.4.1)\n","Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->cai==0.1.6) (3.2.2)\n","Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->cai==0.1.6) (2.6.3)\n","Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->cai==0.1.6) (2.4.1)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->cai==0.1.6) (7.1.2)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->cai==0.1.6) (1.2.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.15.0->cai==0.1.6) (2021.11.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->cai==0.1.6) (1.3.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->cai==0.1.6) (3.0.7)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.15.0->cai==0.1.6) (0.11.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.0numpy->cai==0.1.6) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.0numpy->cai==0.1.6) (1.1.0)\n","Building wheels for collected packages: cai\n","  Building wheel for cai (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for cai: filename=cai-0.1.6-py3-none-any.whl size=59916 sha256=56207c4136d908543f21f3facb8de2c755f851b6028ca98a013d8e9e4d5cc2e0\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-e98kt6lu/wheels/c1/8a/57/56dbba25eff58e52e5365435c4fa102ad8d6f9787d3b4db13a\n","Successfully built cai\n","Installing collected packages: cai\n","Successfully installed cai-0.1.6\n"]}],"source":["import os\n","if not os.path.isdir('k'):\n"," !git clone -b development15 https://github.com/joaopauloschuler/k-neural-api.git k\n","else:\n"," !cd k && git pull\n","!cd k && pip install ."]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4459,"status":"ok","timestamp":1644795536285,"user":{"displayName":"Joao Schuler","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10970916006391371112"},"user_tz":180},"id":"2FWmCCX96ndE","outputId":"5cd13972-0465-40e4-ddc6-8895c8c5bc95"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tensorflow version: 2.7.0\n","Keras version: 2.7.0\n","CPU cores: 8\n","RAM: 54.767017984 GB\n","[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"]}],"source":["import cai.layers\n","import cai.datasets\n","import cai.models\n","import cai.densenet\n","import cai.efficientnet\n","import numpy as np\n","from tensorflow import keras\n","import gc\n","import multiprocessing\n","import random\n","import tensorflow as tf\n","print(\"Tensorflow version:\", tf.version.VERSION)\n","print(\"Keras version:\", keras.__version__)\n","print(\"CPU cores:\", multiprocessing.cpu_count())\n","import psutil\n","print('RAM:', (psutil.virtual_memory().total / 1e9),'GB')\n","print(tf.config.list_physical_devices('GPU'))\n","import matplotlib.pylab as plt"]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"quolyeIQGVOe","executionInfo":{"status":"ok","timestamp":1644795536287,"user_tz":180,"elapsed":25,"user":{"displayName":"Joao Schuler","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10970916006391371112"}},"outputId":"c8b8fbc1-ddf0-4d00-80bd-7eac5cb6df09"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Feb 13 23:38:56 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   38C    P0    25W / 300W |      2MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZH7GTLCqGbIf","executionInfo":{"status":"ok","timestamp":1644795554803,"user_tz":180,"elapsed":18534,"user":{"displayName":"Joao Schuler","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10970916006391371112"}},"outputId":"f27262e9-ff93-4bf1-b698-93d5c39de6ce"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1644795554805,"user":{"displayName":"Joao Schuler","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10970916006391371112"},"user_tz":180},"id":"v5C5xGzHD9lu"},"outputs":[],"source":["from tensorflow.python.profiler.model_analyzer import profile\n","from tensorflow.python.profiler.option_builder import ProfileOptionBuilder\n","\n","def get_flops(model):\n","  forward_pass = tf.function(\n","      model.call,\n","      input_signature=[tf.TensorSpec(shape=(1,) + model.input_shape[1:])])\n","\n","  graph_info = profile(forward_pass.get_concrete_function().graph,\n","                          options=ProfileOptionBuilder.float_operation())\n","\n","  # The //2 is necessary since `profile` counts multiply and accumulate\n","  # as two flops, here we report the total number of multiply accumulate ops\n","  flops = graph_info.total_float_ops // 2\n","  return flops"]},{"cell_type":"markdown","metadata":{"id":"fl4NUmiLLs2J"},"source":["# Download Files and Create Validation and Test Datasets"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"E7hD0zwbcKIq","executionInfo":{"status":"ok","timestamp":1644795588055,"user_tz":180,"elapsed":33262,"user":{"displayName":"Joao Schuler","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10970916006391371112"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"85db8b51-e861-4536-864c-183998d05326"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading:  https://zenodo.org/record/53169/files/Kather_texture_2016_image_tiles_5000.zip?download=1  to  colorectal_5000.zip\n","Decompressing into:  colorectal_5000\n","Creating folder colorectal_5000/val\n","Creating folder colorectal_5000/val/07_ADIPOSE\n","Creating folder colorectal_5000/val/02_STROMA\n","Creating folder colorectal_5000/val/08_EMPTY\n","Creating folder colorectal_5000/val/03_COMPLEX\n","Creating folder colorectal_5000/val/06_MUCOSA\n","Creating folder colorectal_5000/val/04_LYMPHO\n","Creating folder colorectal_5000/val/01_TUMOR\n","Creating folder colorectal_5000/val/05_DEBRIS\n","63 files have been moved from colorectal_5000/Kather_texture_2016_image_tiles_5000/07_ADIPOSE to colorectal_5000/val/07_ADIPOSE.\n","63 files have been moved from colorectal_5000/Kather_texture_2016_image_tiles_5000/02_STROMA to colorectal_5000/val/02_STROMA.\n","63 files have been moved from colorectal_5000/Kather_texture_2016_image_tiles_5000/08_EMPTY to colorectal_5000/val/08_EMPTY.\n","63 files have been moved from colorectal_5000/Kather_texture_2016_image_tiles_5000/03_COMPLEX to colorectal_5000/val/03_COMPLEX.\n","63 files have been moved from colorectal_5000/Kather_texture_2016_image_tiles_5000/06_MUCOSA to colorectal_5000/val/06_MUCOSA.\n","63 files have been moved from colorectal_5000/Kather_texture_2016_image_tiles_5000/04_LYMPHO to colorectal_5000/val/04_LYMPHO.\n","63 files have been moved from colorectal_5000/Kather_texture_2016_image_tiles_5000/01_TUMOR to colorectal_5000/val/01_TUMOR.\n","63 files have been moved from colorectal_5000/Kather_texture_2016_image_tiles_5000/05_DEBRIS to colorectal_5000/val/05_DEBRIS.\n","Creating folder colorectal_5000/test\n","Creating folder colorectal_5000/test/07_ADIPOSE\n","Creating folder colorectal_5000/test/02_STROMA\n","Creating folder colorectal_5000/test/08_EMPTY\n","Creating folder colorectal_5000/test/03_COMPLEX\n","Creating folder colorectal_5000/test/06_MUCOSA\n","Creating folder colorectal_5000/test/04_LYMPHO\n","Creating folder colorectal_5000/test/01_TUMOR\n","Creating folder colorectal_5000/test/05_DEBRIS\n","63 files have been moved from colorectal_5000/Kather_texture_2016_image_tiles_5000/07_ADIPOSE to colorectal_5000/test/07_ADIPOSE.\n","63 files have been moved from colorectal_5000/Kather_texture_2016_image_tiles_5000/02_STROMA to colorectal_5000/test/02_STROMA.\n","63 files have been moved from colorectal_5000/Kather_texture_2016_image_tiles_5000/08_EMPTY to colorectal_5000/test/08_EMPTY.\n","63 files have been moved from colorectal_5000/Kather_texture_2016_image_tiles_5000/03_COMPLEX to colorectal_5000/test/03_COMPLEX.\n","63 files have been moved from colorectal_5000/Kather_texture_2016_image_tiles_5000/06_MUCOSA to colorectal_5000/test/06_MUCOSA.\n","63 files have been moved from colorectal_5000/Kather_texture_2016_image_tiles_5000/04_LYMPHO to colorectal_5000/test/04_LYMPHO.\n","63 files have been moved from colorectal_5000/Kather_texture_2016_image_tiles_5000/01_TUMOR to colorectal_5000/test/01_TUMOR.\n","63 files have been moved from colorectal_5000/Kather_texture_2016_image_tiles_5000/05_DEBRIS to colorectal_5000/test/05_DEBRIS.\n"]}],"source":["verbose=True\n","root_folder = 'colorectal_5000'\n","train_dir = root_folder + '/Kather_texture_2016_image_tiles_5000'\n","val_dir = root_folder + '/val'\n","test_dir = root_folder + '/test'\n","\n","url_zip_file = 'https://zenodo.org/record/53169/files/Kather_texture_2016_image_tiles_5000.zip?download=1'\n","local_zip_file = 'colorectal_5000.zip'\n","expected_folder_name = root_folder\n","\n","if not os.path.isdir(root_folder):\n","  cai.datasets.download_zip_and_extract(url_zip_file, local_zip_file, expected_folder_name, verbose)\n","  cai.datasets.extract_subset_every(train_dir, val_dir, move_every=10, shift=0, verbose=verbose)\n","  cai.datasets.extract_subset_every(train_dir, test_dir, move_every=9, shift=0, verbose=verbose)"]},{"cell_type":"code","source":["!ls -l colorectal_5000/Kather_texture_2016_image_tiles_5000/01_TUMOR/ | wc -l"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zLtfSYvRZv6W","executionInfo":{"status":"ok","timestamp":1644795588056,"user_tz":180,"elapsed":44,"user":{"displayName":"Joao Schuler","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10970916006391371112"}},"outputId":"470bf491-cd98-4fa8-91f6-6cb98efb9df8"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["500\n"]}]},{"cell_type":"code","execution_count":8,"metadata":{"id":"-6WYfTY_h8zn","executionInfo":{"status":"ok","timestamp":1644795588057,"user_tz":180,"elapsed":10,"user":{"displayName":"Joao Schuler","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10970916006391371112"}}},"outputs":[],"source":["num_classes = 8\n","batch_size = 64\n","epochs = 1000\n","target_size_x = 224\n","target_size_y = 224\n","seed = 12\n","lab=False\n","bipolar=False\n","smart_resize=True"]},{"cell_type":"code","source":["# load train dataset\n","x_train, aux_x_val, aux_x_test, y_train, aux_y_val, aux_y_test, class_weight, classes = \\\n","  cai.datasets.load_images_from_folders(seed=seed, root_dir=train_dir, lab=lab, \n","  verbose=verbose, bipolar=bipolar, base_model_name='train',\n","  training_size=1.0, validation_size=0.0, test_size=0.0,\n","  target_size=(target_size_x, target_size_y), \n","  has_training=True, has_validation=False, has_testing=False, \n","  smart_resize=smart_resize)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RfkMQrXEDfxD","executionInfo":{"status":"ok","timestamp":1644795604554,"user_tz":180,"elapsed":16506,"user":{"displayName":"Joao Schuler","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10970916006391371112"}},"outputId":"3de14bfd-b8f1-4731-95d8-5df35231f19c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading  8  classes.\n","smart resize is enabled.\n","loading train images\n","train shape is: (3992, 224, 224, 3)\n","Channel  0  min: 0.0  max: 1.0\n","Channel  1  min: 0.0  max: 1.0\n","Channel  2  min: 0.0  max: 1.0\n","Loaded.\n"]}]},{"cell_type":"code","source":["# load validation dataset\n","aux_x_train, x_val, aux_x_test, aux_y_train, y_val, aux_y_test, class_weight, classes = \\\n","  cai.datasets.load_images_from_folders(seed=seed, root_dir=val_dir, lab=lab, \n","  verbose=verbose, bipolar=bipolar, base_model_name='val',\n","  training_size=0.0, validation_size=1.0, test_size=0.0,\n","  target_size=(target_size_x, target_size_y), \n","  has_training=False, has_validation=True, has_testing=False, \n","  smart_resize=smart_resize)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1psVhRRMESZg","executionInfo":{"status":"ok","timestamp":1644795605708,"user_tz":180,"elapsed":1169,"user":{"displayName":"Joao Schuler","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10970916006391371112"}},"outputId":"e5e1c4bf-ed2f-4413-e1fb-2abe8d475187"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading  8  classes.\n","smart resize is enabled.\n","loading validation images\n","validation shape is: (504, 224, 224, 3)\n","Loaded.\n"]}]},{"cell_type":"code","source":["# Duplicate Validation by Flipping Horizontally\n","x_val = np.concatenate( (x_val, np.flip(x_val, 2)), axis=0)\n","y_val = np.concatenate( (y_val, y_val), axis=0)\n","\n","# Duplicate Validation Again by Flipping Vertically\n","x_val = np.concatenate( (x_val, np.flip(x_val, 1)), axis=0)\n","y_val = np.concatenate( (y_val, y_val), axis=0)\n","\n","print(\"x_val_shape:\", x_val.shape)\n","print(\"y_val_shape:\", y_val.shape)"],"metadata":{"id":"t6QhGcJB4ceh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644795606160,"user_tz":180,"elapsed":459,"user":{"displayName":"Joao Schuler","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10970916006391371112"}},"outputId":"b8ef92f1-526e-4912-90e8-2e2b0281b85a"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["x_val_shape: (2016, 224, 224, 3)\n","y_val_shape: (2016, 8)\n"]}]},{"cell_type":"code","source":["# load test dataset\n","aux_x_train, aux_x_val, x_test, aux_y_train, aux_y_val, y_test, class_weight, classes = \\\n","  cai.datasets.load_images_from_folders(seed=seed, root_dir=test_dir, lab=lab, \n","  verbose=verbose, bipolar=bipolar, base_model_name='test',\n","  training_size=0.0, validation_size=0.0, test_size=1.0,\n","  target_size=(target_size_x, target_size_y), \n","  has_training=False, has_validation=False, has_testing=True, \n","  smart_resize=smart_resize)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p9ftIAzOIXwn","executionInfo":{"status":"ok","timestamp":1644795606898,"user_tz":180,"elapsed":742,"user":{"displayName":"Joao Schuler","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10970916006391371112"}},"outputId":"1a5a20a9-de0c-48bd-a717-519e92e8cbb9"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading  8  classes.\n","smart resize is enabled.\n","loading test images\n","test shape is: (504, 224, 224, 3)\n","Loaded.\n"]}]},{"cell_type":"code","execution_count":13,"metadata":{"id":"qN8LdqheVEkj","executionInfo":{"status":"ok","timestamp":1644795606901,"user_tz":180,"elapsed":12,"user":{"displayName":"Joao Schuler","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10970916006391371112"}}},"outputs":[],"source":["train_datagen = cai.util.create_image_generator(vertical_flip=True,\n","  rotation_range=179, width_shift_range=0.3, height_shift_range=0.3,\n","  channel_shift_range=0.0)\n","valid_datagen = cai.util.create_image_generator_no_augmentation()\n","test_datagen = cai.util.create_image_generator_no_augmentation()\n","cpus_num = max([multiprocessing.cpu_count(), 8])\n","\n","def cyclical_adv_lrscheduler25(epoch):\n","    \"\"\"CAI Cyclical and Advanced Learning Rate Scheduler.\n","    # Arguments\n","        epoch: integer with current epoch count.\n","    # Returns\n","        float with desired learning rate.\n","    \"\"\"\n","    base_learning = 0.001\n","    local_epoch = epoch % 25\n","    result = base_learning\n","    if local_epoch < 7:\n","       result = base_learning * (1 + 0.5*local_epoch)\n","    else:\n","       result = (base_learning * 4) * ( 0.85**(local_epoch-7) )\n","\n","    if result < 2.5245e-04:\n","      result = 2.5245e-04\n","    return result"]},{"cell_type":"code","source":["learning_rate_test = []\n","for epoch in range(epochs):\n","  learning_rate_test.append( cyclical_adv_lrscheduler25(epoch) )\n","plt.figure()\n","plt.ylabel(\"Learning Rate\")\n","plt.xlabel(\"Epochs\")\n","plt.ylim([0.0000,0.005])\n","plt.plot(learning_rate_test)"],"metadata":{"id":"-98DQjaJmPC3","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"ok","timestamp":1644795607424,"user_tz":180,"elapsed":534,"user":{"displayName":"Joao Schuler","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10970916006391371112"}},"outputId":"fe9715c8-6b59-4731-fbc9-04d4e0043c2a"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7f90f738c790>]"]},"metadata":{},"execution_count":14},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","execution_count":15,"metadata":{"id":"8A7b9F3TVQUG","executionInfo":{"status":"ok","timestamp":1644795607427,"user_tz":180,"elapsed":12,"user":{"displayName":"Joao Schuler","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10970916006391371112"}}},"outputs":[],"source":["def work_on_keffnet(show_model=False, run_fit=False, test_results=False, calc_f1=False):\n","  monitor='val_loss'\n","  if (show_model):\n","    input_shape = (target_size_x, target_size_y, 3)\n","  else:\n","    input_shape = (None, None, 3)\n","  for kType in [-1]: # , cai.layers.D6v3_12ch(), cai.layers.D6v3_16ch(), cai.layers.D6v3_32ch()\n","      basefilename = '/content/drive/MyDrive/output/JP30N02-'+str(kType)\n","      best_result_file_name = basefilename+'-best_result.hdf5'\n","      print('Running: '+basefilename)\n","      if kType == -1:\n","        model = cai.efficientnet.EfficientNetB0(\n","          include_top=True,\n","          input_shape=input_shape,\n","          classes=num_classes)\n","      else:\n","        model = cai.efficientnet.kEfficientNetB0(\n","          include_top=True,\n","          input_shape=input_shape,\n","          classes=num_classes,\n","          # skip_stride_cnt=0,\n","          kType=kType)\n","        \n","      optimizer = keras.optimizers.RMSprop()\n","      model.compile(\n","        loss='categorical_crossentropy',\n","        optimizer=optimizer,\n","        metrics=['accuracy'])\n","\n","      if (show_model):\n","        model.summary(line_length=180)\n","        print('model flops:',get_flops(model))\n","\n","      save_best = keras.callbacks.ModelCheckpoint(\n","            filepath=best_result_file_name,\n","            monitor=monitor,\n","            verbose=1,\n","            save_best_only=True,\n","            save_weights_only=False,\n","            mode='min',\n","            save_freq='epoch')\n","\n","      if (run_fit): \n","            train_flow = train_datagen.flow(\n","                x_train, y_train,\n","                batch_size=batch_size,\n","                shuffle=True,\n","                seed=seed\n","            )\n","            validation_flow = valid_datagen.flow(\n","                x_val, y_val,\n","                batch_size=batch_size,\n","                shuffle=False,\n","                seed=seed\n","            )\n","            history = model.fit(\n","              x = train_flow,\n","              epochs=epochs,\n","              batch_size=batch_size,\n","              validation_data=validation_flow,\n","              callbacks=[save_best, tf.keras.callbacks.LearningRateScheduler(cyclical_adv_lrscheduler25)],\n","              workers=cpus_num,\n","              max_queue_size=128\n","            )\n","            plt.figure()\n","            plt.ylabel(\"Accuracy (training and validation)\")\n","            plt.xlabel(\"Epochs\")\n","            plt.ylim([0,1])\n","            plt.plot(history.history[\"accuracy\"])\n","            plt.plot(history.history[\"val_accuracy\"])\n","      if (test_results):\n","        test_flow = test_datagen.flow(\n","            x_test, y_test,\n","            batch_size=batch_size,\n","            shuffle=False,\n","            seed=seed\n","        )\n","        print('Best Model Results: '+best_result_file_name)\n","        model = cai.models.load_kereas_model(best_result_file_name)\n","        evaluated = model.evaluate(\n","            x=test_flow,\n","            batch_size=batch_size,\n","            use_multiprocessing=False,\n","            workers=cpus_num\n","        )\n","        for metric, name in zip(evaluated,[\"loss\",\"acc\"]):\n","              print(name,metric)\n","      if (calc_f1):\n","        cai.datasets.test_flips_on_saved_model(x_test, y_test, best_result_file_name, has_flip_x=True, has_flip_y=True, has_bw=True, center_crop=0.15)\n","      print('Finished: '+basefilename)"]},{"cell_type":"markdown","metadata":{"id":"HRSxTd5GeU5p"},"source":["# Show Models"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"EfjO1XESTCrY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644795612388,"user_tz":180,"elapsed":4971,"user":{"displayName":"Joao Schuler","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10970916006391371112"}},"outputId":"ae632a6d-606f-4c0d-ae11-eb3fe640c0e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running: /content/drive/MyDrive/output/JP30N02--1\n","Model: \"efficientnet-b0\"\n","____________________________________________________________________________________________________________________________________________________________________________________\n"," Layer (type)                                              Output Shape                            Param #              Connected to                                                \n","====================================================================================================================================================================================\n"," input_1 (InputLayer)                                      [(None, 224, 224, 3)]                   0                    []                                                          \n","                                                                                                                                                                                    \n"," stem_conv_pad (ZeroPadding2D)                             (None, 225, 225, 3)                     0                    ['input_1[0][0]']                                           \n","                                                                                                                                                                                    \n"," stem_conv (Conv2D)                                        (None, 112, 112, 32)                    864                  ['stem_conv_pad[0][0]']                                     \n","                                                                                                                                                                                    \n"," stem_bn (BatchNormalization)                              (None, 112, 112, 32)                    128                  ['stem_conv[0][0]']                                         \n","                                                                                                                                                                                    \n"," stem_activation (Activation)                              (None, 112, 112, 32)                    0                    ['stem_bn[0][0]']                                           \n","                                                                                                                                                                                    \n"," block1a_dwconv (DepthwiseConv2D)                          (None, 112, 112, 32)                    288                  ['stem_activation[0][0]']                                   \n","                                                                                                                                                                                    \n"," block1a_bn (BatchNormalization)                           (None, 112, 112, 32)                    128                  ['block1a_dwconv[0][0]']                                    \n","                                                                                                                                                                                    \n"," block1a_activation (Activation)                           (None, 112, 112, 32)                    0                    ['block1a_bn[0][0]']                                        \n","                                                                                                                                                                                    \n"," block1a_se_squeeze (GlobalAveragePooling2D)               (None, 32)                              0                    ['block1a_activation[0][0]']                                \n","                                                                                                                                                                                    \n"," block1a_se_reshape (Reshape)                              (None, 1, 1, 32)                        0                    ['block1a_se_squeeze[0][0]']                                \n","                                                                                                                                                                                    \n"," block1a_se_reduce (Conv2D)                                (None, 1, 1, 8)                         264                  ['block1a_se_reshape[0][0]']                                \n","                                                                                                                                                                                    \n"," block1a_se_expand (Conv2D)                                (None, 1, 1, 32)                        288                  ['block1a_se_reduce[0][0]']                                 \n","                                                                                                                                                                                    \n"," block1a_se_excite (Multiply)                              (None, 112, 112, 32)                    0                    ['block1a_activation[0][0]',                                \n","                                                                                                                         'block1a_se_expand[0][0]']                                 \n","                                                                                                                                                                                    \n"," block1a_project_conv (Conv2D)                             (None, 112, 112, 16)                    512                  ['block1a_se_excite[0][0]']                                 \n","                                                                                                                                                                                    \n"," block1a_project_bn (BatchNormalization)                   (None, 112, 112, 16)                    64                   ['block1a_project_conv[0][0]']                              \n","                                                                                                                                                                                    \n"," block2a_expand_conv (Conv2D)                              (None, 112, 112, 96)                    1536                 ['block1a_project_bn[0][0]']                                \n","                                                                                                                                                                                    \n"," block2a_expand_bn (BatchNormalization)                    (None, 112, 112, 96)                    384                  ['block2a_expand_conv[0][0]']                               \n","                                                                                                                                                                                    \n"," block2a_expand_activation (Activation)                    (None, 112, 112, 96)                    0                    ['block2a_expand_bn[0][0]']                                 \n","                                                                                                                                                                                    \n"," block2a_dwconv_pad (ZeroPadding2D)                        (None, 113, 113, 96)                    0                    ['block2a_expand_activation[0][0]']                         \n","                                                                                                                                                                                    \n"," block2a_dwconv (DepthwiseConv2D)                          (None, 56, 56, 96)                      864                  ['block2a_dwconv_pad[0][0]']                                \n","                                                                                                                                                                                    \n"," block2a_bn (BatchNormalization)                           (None, 56, 56, 96)                      384                  ['block2a_dwconv[0][0]']                                    \n","                                                                                                                                                                                    \n"," block2a_activation (Activation)                           (None, 56, 56, 96)                      0                    ['block2a_bn[0][0]']                                        \n","                                                                                                                                                                                    \n"," block2a_se_squeeze (GlobalAveragePooling2D)               (None, 96)                              0                    ['block2a_activation[0][0]']                                \n","                                                                                                                                                                                    \n"," block2a_se_reshape (Reshape)                              (None, 1, 1, 96)                        0                    ['block2a_se_squeeze[0][0]']                                \n","                                                                                                                                                                                    \n"," block2a_se_reduce (Conv2D)                                (None, 1, 1, 4)                         388                  ['block2a_se_reshape[0][0]']                                \n","                                                                                                                                                                                    \n"," block2a_se_expand (Conv2D)                                (None, 1, 1, 96)                        480                  ['block2a_se_reduce[0][0]']                                 \n","                                                                                                                                                                                    \n"," block2a_se_excite (Multiply)                              (None, 56, 56, 96)                      0                    ['block2a_activation[0][0]',                                \n","                                                                                                                         'block2a_se_expand[0][0]']                                 \n","                                                                                                                                                                                    \n"," block2a_project_conv (Conv2D)                             (None, 56, 56, 24)                      2304                 ['block2a_se_excite[0][0]']                                 \n","                                                                                                                                                                                    \n"," block2a_project_bn (BatchNormalization)                   (None, 56, 56, 24)                      96                   ['block2a_project_conv[0][0]']                              \n","                                                                                                                                                                                    \n"," block2b_expand_conv (Conv2D)                              (None, 56, 56, 144)                     3456                 ['block2a_project_bn[0][0]']                                \n","                                                                                                                                                                                    \n"," block2b_expand_bn (BatchNormalization)                    (None, 56, 56, 144)                     576                  ['block2b_expand_conv[0][0]']                               \n","                                                                                                                                                                                    \n"," block2b_expand_activation (Activation)                    (None, 56, 56, 144)                     0                    ['block2b_expand_bn[0][0]']                                 \n","                                                                                                                                                                                    \n"," block2b_dwconv (DepthwiseConv2D)                          (None, 56, 56, 144)                     1296                 ['block2b_expand_activation[0][0]']                         \n","                                                                                                                                                                                    \n"," block2b_bn (BatchNormalization)                           (None, 56, 56, 144)                     576                  ['block2b_dwconv[0][0]']                                    \n","                                                                                                                                                                                    \n"," block2b_activation (Activation)                           (None, 56, 56, 144)                     0                    ['block2b_bn[0][0]']                                        \n","                                                                                                                                                                                    \n"," block2b_se_squeeze (GlobalAveragePooling2D)               (None, 144)                             0                    ['block2b_activation[0][0]']                                \n","                                                                                                                                                                                    \n"," block2b_se_reshape (Reshape)                              (None, 1, 1, 144)                       0                    ['block2b_se_squeeze[0][0]']                                \n","                                                                                                                                                                                    \n"," block2b_se_reduce (Conv2D)                                (None, 1, 1, 6)                         870                  ['block2b_se_reshape[0][0]']                                \n","                                                                                                                                                                                    \n"," block2b_se_expand (Conv2D)                                (None, 1, 1, 144)                       1008                 ['block2b_se_reduce[0][0]']                                 \n","                                                                                                                                                                                    \n"," block2b_se_excite (Multiply)                              (None, 56, 56, 144)                     0                    ['block2b_activation[0][0]',                                \n","                                                                                                                         'block2b_se_expand[0][0]']                                 \n","                                                                                                                                                                                    \n"," block2b_project_conv (Conv2D)                             (None, 56, 56, 24)                      3456                 ['block2b_se_excite[0][0]']                                 \n","                                                                                                                                                                                    \n"," block2b_project_bn (BatchNormalization)                   (None, 56, 56, 24)                      96                   ['block2b_project_conv[0][0]']                              \n","                                                                                                                                                                                    \n"," block2b_drop (Dropout)                                    (None, 56, 56, 24)                      0                    ['block2b_project_bn[0][0]']                                \n","                                                                                                                                                                                    \n"," block2b_add (Add)                                         (None, 56, 56, 24)                      0                    ['block2b_drop[0][0]',                                      \n","                                                                                                                         'block2a_project_bn[0][0]']                                \n","                                                                                                                                                                                    \n"," block3a_expand_conv (Conv2D)                              (None, 56, 56, 144)                     3456                 ['block2b_add[0][0]']                                       \n","                                                                                                                                                                                    \n"," block3a_expand_bn (BatchNormalization)                    (None, 56, 56, 144)                     576                  ['block3a_expand_conv[0][0]']                               \n","                                                                                                                                                                                    \n"," block3a_expand_activation (Activation)                    (None, 56, 56, 144)                     0                    ['block3a_expand_bn[0][0]']                                 \n","                                                                                                                                                                                    \n"," block3a_dwconv_pad (ZeroPadding2D)                        (None, 59, 59, 144)                     0                    ['block3a_expand_activation[0][0]']                         \n","                                                                                                                                                                                    \n"," block3a_dwconv (DepthwiseConv2D)                          (None, 28, 28, 144)                     3600                 ['block3a_dwconv_pad[0][0]']                                \n","                                                                                                                                                                                    \n"," block3a_bn (BatchNormalization)                           (None, 28, 28, 144)                     576                  ['block3a_dwconv[0][0]']                                    \n","                                                                                                                                                                                    \n"," block3a_activation (Activation)                           (None, 28, 28, 144)                     0                    ['block3a_bn[0][0]']                                        \n","                                                                                                                                                                                    \n"," block3a_se_squeeze (GlobalAveragePooling2D)               (None, 144)                             0                    ['block3a_activation[0][0]']                                \n","                                                                                                                                                                                    \n"," block3a_se_reshape (Reshape)                              (None, 1, 1, 144)                       0                    ['block3a_se_squeeze[0][0]']                                \n","                                                                                                                                                                                    \n"," block3a_se_reduce (Conv2D)                                (None, 1, 1, 6)                         870                  ['block3a_se_reshape[0][0]']                                \n","                                                                                                                                                                                    \n"," block3a_se_expand (Conv2D)                                (None, 1, 1, 144)                       1008                 ['block3a_se_reduce[0][0]']                                 \n","                                                                                                                                                                                    \n"," block3a_se_excite (Multiply)                              (None, 28, 28, 144)                     0                    ['block3a_activation[0][0]',                                \n","                                                                                                                         'block3a_se_expand[0][0]']                                 \n","                                                                                                                                                                                    \n"," block3a_project_conv (Conv2D)                             (None, 28, 28, 40)                      5760                 ['block3a_se_excite[0][0]']                                 \n","                                                                                                                                                                                    \n"," block3a_project_bn (BatchNormalization)                   (None, 28, 28, 40)                      160                  ['block3a_project_conv[0][0]']                              \n","                                                                                                                                                                                    \n"," block3b_expand_conv (Conv2D)                              (None, 28, 28, 240)                     9600                 ['block3a_project_bn[0][0]']                                \n","                                                                                                                                                                                    \n"," block3b_expand_bn (BatchNormalization)                    (None, 28, 28, 240)                     960                  ['block3b_expand_conv[0][0]']                               \n","                                                                                                                                                                                    \n"," block3b_expand_activation (Activation)                    (None, 28, 28, 240)                     0                    ['block3b_expand_bn[0][0]']                                 \n","                                                                                                                                                                                    \n"," block3b_dwconv (DepthwiseConv2D)                          (None, 28, 28, 240)                     6000                 ['block3b_expand_activation[0][0]']                         \n","                                                                                                                                                                                    \n"," block3b_bn (BatchNormalization)                           (None, 28, 28, 240)                     960                  ['block3b_dwconv[0][0]']                                    \n","                                                                                                                                                                                    \n"," block3b_activation (Activation)                           (None, 28, 28, 240)                     0                    ['block3b_bn[0][0]']                                        \n","                                                                                                                                                                                    \n"," block3b_se_squeeze (GlobalAveragePooling2D)               (None, 240)                             0                    ['block3b_activation[0][0]']                                \n","                                                                                                                                                                                    \n"," block3b_se_reshape (Reshape)                              (None, 1, 1, 240)                       0                    ['block3b_se_squeeze[0][0]']                                \n","                                                                                                                                                                                    \n"," block3b_se_reduce (Conv2D)                                (None, 1, 1, 10)                        2410                 ['block3b_se_reshape[0][0]']                                \n","                                                                                                                                                                                    \n"," block3b_se_expand (Conv2D)                                (None, 1, 1, 240)                       2640                 ['block3b_se_reduce[0][0]']                                 \n","                                                                                                                                                                                    \n"," block3b_se_excite (Multiply)                              (None, 28, 28, 240)                     0                    ['block3b_activation[0][0]',                                \n","                                                                                                                         'block3b_se_expand[0][0]']                                 \n","                                                                                                                                                                                    \n"," block3b_project_conv (Conv2D)                             (None, 28, 28, 40)                      9600                 ['block3b_se_excite[0][0]']                                 \n","                                                                                                                                                                                    \n"," block3b_project_bn (BatchNormalization)                   (None, 28, 28, 40)                      160                  ['block3b_project_conv[0][0]']                              \n","                                                                                                                                                                                    \n"," block3b_drop (Dropout)                                    (None, 28, 28, 40)                      0                    ['block3b_project_bn[0][0]']                                \n","                                                                                                                                                                                    \n"," block3b_add (Add)                                         (None, 28, 28, 40)                      0                    ['block3b_drop[0][0]',                                      \n","                                                                                                                         'block3a_project_bn[0][0]']                                \n","                                                                                                                                                                                    \n"," block4a_expand_conv (Conv2D)                              (None, 28, 28, 240)                     9600                 ['block3b_add[0][0]']                                       \n","                                                                                                                                                                                    \n"," block4a_expand_bn (BatchNormalization)                    (None, 28, 28, 240)                     960                  ['block4a_expand_conv[0][0]']                               \n","                                                                                                                                                                                    \n"," block4a_expand_activation (Activation)                    (None, 28, 28, 240)                     0                    ['block4a_expand_bn[0][0]']                                 \n","                                                                                                                                                                                    \n"," block4a_dwconv_pad (ZeroPadding2D)                        (None, 29, 29, 240)                     0                    ['block4a_expand_activation[0][0]']                         \n","                                                                                                                                                                                    \n"," block4a_dwconv (DepthwiseConv2D)                          (None, 14, 14, 240)                     2160                 ['block4a_dwconv_pad[0][0]']                                \n","                                                                                                                                                                                    \n"," block4a_bn (BatchNormalization)                           (None, 14, 14, 240)                     960                  ['block4a_dwconv[0][0]']                                    \n","                                                                                                                                                                                    \n"," block4a_activation (Activation)                           (None, 14, 14, 240)                     0                    ['block4a_bn[0][0]']                                        \n","                                                                                                                                                                                    \n"," block4a_se_squeeze (GlobalAveragePooling2D)               (None, 240)                             0                    ['block4a_activation[0][0]']                                \n","                                                                                                                                                                                    \n"," block4a_se_reshape (Reshape)                              (None, 1, 1, 240)                       0                    ['block4a_se_squeeze[0][0]']                                \n","                                                                                                                                                                                    \n"," block4a_se_reduce (Conv2D)                                (None, 1, 1, 10)                        2410                 ['block4a_se_reshape[0][0]']                                \n","                                                                                                                                                                                    \n"," block4a_se_expand (Conv2D)                                (None, 1, 1, 240)                       2640                 ['block4a_se_reduce[0][0]']                                 \n","                                                                                                                                                                                    \n"," block4a_se_excite (Multiply)                              (None, 14, 14, 240)                     0                    ['block4a_activation[0][0]',                                \n","                                                                                                                         'block4a_se_expand[0][0]']                                 \n","                                                                                                                                                                                    \n"," block4a_project_conv (Conv2D)                             (None, 14, 14, 80)                      19200                ['block4a_se_excite[0][0]']                                 \n","                                                                                                                                                                                    \n"," block4a_project_bn (BatchNormalization)                   (None, 14, 14, 80)                      320                  ['block4a_project_conv[0][0]']                              \n","                                                                                                                                                                                    \n"," block4b_expand_conv (Conv2D)                              (None, 14, 14, 480)                     38400                ['block4a_project_bn[0][0]']                                \n","                                                                                                                                                                                    \n"," block4b_expand_bn (BatchNormalization)                    (None, 14, 14, 480)                     1920                 ['block4b_expand_conv[0][0]']                               \n","                                                                                                                                                                                    \n"," block4b_expand_activation (Activation)                    (None, 14, 14, 480)                     0                    ['block4b_expand_bn[0][0]']                                 \n","                                                                                                                                                                                    \n"," block4b_dwconv (DepthwiseConv2D)                          (None, 14, 14, 480)                     4320                 ['block4b_expand_activation[0][0]']                         \n","                                                                                                                                                                                    \n"," block4b_bn (BatchNormalization)                           (None, 14, 14, 480)                     1920                 ['block4b_dwconv[0][0]']                                    \n","                                                                                                                                                                                    \n"," block4b_activation (Activation)                           (None, 14, 14, 480)                     0                    ['block4b_bn[0][0]']                                        \n","                                                                                                                                                                                    \n"," block4b_se_squeeze (GlobalAveragePooling2D)               (None, 480)                             0                    ['block4b_activation[0][0]']                                \n","                                                                                                                                                                                    \n"," block4b_se_reshape (Reshape)                              (None, 1, 1, 480)                       0                    ['block4b_se_squeeze[0][0]']                                \n","                                                                                                                                                                                    \n"," block4b_se_reduce (Conv2D)                                (None, 1, 1, 20)                        9620                 ['block4b_se_reshape[0][0]']                                \n","                                                                                                                                                                                    \n"," block4b_se_expand (Conv2D)                                (None, 1, 1, 480)                       10080                ['block4b_se_reduce[0][0]']                                 \n","                                                                                                                                                                                    \n"," block4b_se_excite (Multiply)                              (None, 14, 14, 480)                     0                    ['block4b_activation[0][0]',                                \n","                                                                                                                         'block4b_se_expand[0][0]']                                 \n","                                                                                                                                                                                    \n"," block4b_project_conv (Conv2D)                             (None, 14, 14, 80)                      38400                ['block4b_se_excite[0][0]']                                 \n","                                                                                                                                                                                    \n"," block4b_project_bn (BatchNormalization)                   (None, 14, 14, 80)                      320                  ['block4b_project_conv[0][0]']                              \n","                                                                                                                                                                                    \n"," block4b_drop (Dropout)                                    (None, 14, 14, 80)                      0                    ['block4b_project_bn[0][0]']                                \n","                                                                                                                                                                                    \n"," block4b_add (Add)                                         (None, 14, 14, 80)                      0                    ['block4b_drop[0][0]',                                      \n","                                                                                                                         'block4a_project_bn[0][0]']                                \n","                                                                                                                                                                                    \n"," block4c_expand_conv (Conv2D)                              (None, 14, 14, 480)                     38400                ['block4b_add[0][0]']                                       \n","                                                                                                                                                                                    \n"," block4c_expand_bn (BatchNormalization)                    (None, 14, 14, 480)                     1920                 ['block4c_expand_conv[0][0]']                               \n","                                                                                                                                                                                    \n"," block4c_expand_activation (Activation)                    (None, 14, 14, 480)                     0                    ['block4c_expand_bn[0][0]']                                 \n","                                                                                                                                                                                    \n"," block4c_dwconv (DepthwiseConv2D)                          (None, 14, 14, 480)                     4320                 ['block4c_expand_activation[0][0]']                         \n","                                                                                                                                                                                    \n"," block4c_bn (BatchNormalization)                           (None, 14, 14, 480)                     1920                 ['block4c_dwconv[0][0]']                                    \n","                                                                                                                                                                                    \n"," block4c_activation (Activation)                           (None, 14, 14, 480)                     0                    ['block4c_bn[0][0]']                                        \n","                                                                                                                                                                                    \n"," block4c_se_squeeze (GlobalAveragePooling2D)               (None, 480)                             0                    ['block4c_activation[0][0]']                                \n","                                                                                                                                                                                    \n"," block4c_se_reshape (Reshape)                              (None, 1, 1, 480)                       0                    ['block4c_se_squeeze[0][0]']                                \n","                                                                                                                                                                                    \n"," block4c_se_reduce (Conv2D)                                (None, 1, 1, 20)                        9620                 ['block4c_se_reshape[0][0]']                                \n","                                                                                                                                                                                    \n"," block4c_se_expand (Conv2D)                                (None, 1, 1, 480)                       10080                ['block4c_se_reduce[0][0]']                                 \n","                                                                                                                                                                                    \n"," block4c_se_excite (Multiply)                              (None, 14, 14, 480)                     0                    ['block4c_activation[0][0]',                                \n","                                                                                                                         'block4c_se_expand[0][0]']                                 \n","                                                                                                                                                                                    \n"," block4c_project_conv (Conv2D)                             (None, 14, 14, 80)                      38400                ['block4c_se_excite[0][0]']                                 \n","                                                                                                                                                                                    \n"," block4c_project_bn (BatchNormalization)                   (None, 14, 14, 80)                      320                  ['block4c_project_conv[0][0]']                              \n","                                                                                                                                                                                    \n"," block4c_drop (Dropout)                                    (None, 14, 14, 80)                      0                    ['block4c_project_bn[0][0]']                                \n","                                                                                                                                                                                    \n"," block4c_add (Add)                                         (None, 14, 14, 80)                      0                    ['block4c_drop[0][0]',                                      \n","                                                                                                                         'block4b_add[0][0]']                                       \n","                                                                                                                                                                                    \n"," block5a_expand_conv (Conv2D)                              (None, 14, 14, 480)                     38400                ['block4c_add[0][0]']                                       \n","                                                                                                                                                                                    \n"," block5a_expand_bn (BatchNormalization)                    (None, 14, 14, 480)                     1920                 ['block5a_expand_conv[0][0]']                               \n","                                                                                                                                                                                    \n"," block5a_expand_activation (Activation)                    (None, 14, 14, 480)                     0                    ['block5a_expand_bn[0][0]']                                 \n","                                                                                                                                                                                    \n"," block5a_dwconv (DepthwiseConv2D)                          (None, 14, 14, 480)                     12000                ['block5a_expand_activation[0][0]']                         \n","                                                                                                                                                                                    \n"," block5a_bn (BatchNormalization)                           (None, 14, 14, 480)                     1920                 ['block5a_dwconv[0][0]']                                    \n","                                                                                                                                                                                    \n"," block5a_activation (Activation)                           (None, 14, 14, 480)                     0                    ['block5a_bn[0][0]']                                        \n","                                                                                                                                                                                    \n"," block5a_se_squeeze (GlobalAveragePooling2D)               (None, 480)                             0                    ['block5a_activation[0][0]']                                \n","                                                                                                                                                                                    \n"," block5a_se_reshape (Reshape)                              (None, 1, 1, 480)                       0                    ['block5a_se_squeeze[0][0]']                                \n","                                                                                                                                                                                    \n"," block5a_se_reduce (Conv2D)                                (None, 1, 1, 20)                        9620                 ['block5a_se_reshape[0][0]']                                \n","                                                                                                                                                                                    \n"," block5a_se_expand (Conv2D)                                (None, 1, 1, 480)                       10080                ['block5a_se_reduce[0][0]']                                 \n","                                                                                                                                                                                    \n"," block5a_se_excite (Multiply)                              (None, 14, 14, 480)                     0                    ['block5a_activation[0][0]',                                \n","                                                                                                                         'block5a_se_expand[0][0]']                                 \n","                                                                                                                                                                                    \n"," block5a_project_conv (Conv2D)                             (None, 14, 14, 112)                     53760                ['block5a_se_excite[0][0]']                                 \n","                                                                                                                                                                                    \n"," block5a_project_bn (BatchNormalization)                   (None, 14, 14, 112)                     448                  ['block5a_project_conv[0][0]']                              \n","                                                                                                                                                                                    \n"," block5b_expand_conv (Conv2D)                              (None, 14, 14, 672)                     75264                ['block5a_project_bn[0][0]']                                \n","                                                                                                                                                                                    \n"," block5b_expand_bn (BatchNormalization)                    (None, 14, 14, 672)                     2688                 ['block5b_expand_conv[0][0]']                               \n","                                                                                                                                                                                    \n"," block5b_expand_activation (Activation)                    (None, 14, 14, 672)                     0                    ['block5b_expand_bn[0][0]']                                 \n","                                                                                                                                                                                    \n"," block5b_dwconv (DepthwiseConv2D)                          (None, 14, 14, 672)                     16800                ['block5b_expand_activation[0][0]']                         \n","                                                                                                                                                                                    \n"," block5b_bn (BatchNormalization)                           (None, 14, 14, 672)                     2688                 ['block5b_dwconv[0][0]']                                    \n","                                                                                                                                                                                    \n"," block5b_activation (Activation)                           (None, 14, 14, 672)                     0                    ['block5b_bn[0][0]']                                        \n","                                                                                                                                                                                    \n"," block5b_se_squeeze (GlobalAveragePooling2D)               (None, 672)                             0                    ['block5b_activation[0][0]']                                \n","                                                                                                                                                                                    \n"," block5b_se_reshape (Reshape)                              (None, 1, 1, 672)                       0                    ['block5b_se_squeeze[0][0]']                                \n","                                                                                                                                                                                    \n"," block5b_se_reduce (Conv2D)                                (None, 1, 1, 28)                        18844                ['block5b_se_reshape[0][0]']                                \n","                                                                                                                                                                                    \n"," block5b_se_expand (Conv2D)                                (None, 1, 1, 672)                       19488                ['block5b_se_reduce[0][0]']                                 \n","                                                                                                                                                                                    \n"," block5b_se_excite (Multiply)                              (None, 14, 14, 672)                     0                    ['block5b_activation[0][0]',                                \n","                                                                                                                         'block5b_se_expand[0][0]']                                 \n","                                                                                                                                                                                    \n"," block5b_project_conv (Conv2D)                             (None, 14, 14, 112)                     75264                ['block5b_se_excite[0][0]']                                 \n","                                                                                                                                                                                    \n"," block5b_project_bn (BatchNormalization)                   (None, 14, 14, 112)                     448                  ['block5b_project_conv[0][0]']                              \n","                                                                                                                                                                                    \n"," block5b_drop (Dropout)                                    (None, 14, 14, 112)                     0                    ['block5b_project_bn[0][0]']                                \n","                                                                                                                                                                                    \n"," block5b_add (Add)                                         (None, 14, 14, 112)                     0                    ['block5b_drop[0][0]',                                      \n","                                                                                                                         'block5a_project_bn[0][0]']                                \n","                                                                                                                                                                                    \n"," block5c_expand_conv (Conv2D)                              (None, 14, 14, 672)                     75264                ['block5b_add[0][0]']                                       \n","                                                                                                                                                                                    \n"," block5c_expand_bn (BatchNormalization)                    (None, 14, 14, 672)                     2688                 ['block5c_expand_conv[0][0]']                               \n","                                                                                                                                                                                    \n"," block5c_expand_activation (Activation)                    (None, 14, 14, 672)                     0                    ['block5c_expand_bn[0][0]']                                 \n","                                                                                                                                                                                    \n"," block5c_dwconv (DepthwiseConv2D)                          (None, 14, 14, 672)                     16800                ['block5c_expand_activation[0][0]']                         \n","                                                                                                                                                                                    \n"," block5c_bn (BatchNormalization)                           (None, 14, 14, 672)                     2688                 ['block5c_dwconv[0][0]']                                    \n","                                                                                                                                                                                    \n"," block5c_activation (Activation)                           (None, 14, 14, 672)                     0                    ['block5c_bn[0][0]']                                        \n","                                                                                                                                                                                    \n"," block5c_se_squeeze (GlobalAveragePooling2D)               (None, 672)                             0                    ['block5c_activation[0][0]']                                \n","                                                                                                                                                                                    \n"," block5c_se_reshape (Reshape)                              (None, 1, 1, 672)                       0                    ['block5c_se_squeeze[0][0]']                                \n","                                                                                                                                                                                    \n"," block5c_se_reduce (Conv2D)                                (None, 1, 1, 28)                        18844                ['block5c_se_reshape[0][0]']                                \n","                                                                                                                                                                                    \n"," block5c_se_expand (Conv2D)                                (None, 1, 1, 672)                       19488                ['block5c_se_reduce[0][0]']                                 \n","                                                                                                                                                                                    \n"," block5c_se_excite (Multiply)                              (None, 14, 14, 672)                     0                    ['block5c_activation[0][0]',                                \n","                                                                                                                         'block5c_se_expand[0][0]']                                 \n","                                                                                                                                                                                    \n"," block5c_project_conv (Conv2D)                             (None, 14, 14, 112)                     75264                ['block5c_se_excite[0][0]']                                 \n","                                                                                                                                                                                    \n"," block5c_project_bn (BatchNormalization)                   (None, 14, 14, 112)                     448                  ['block5c_project_conv[0][0]']                              \n","                                                                                                                                                                                    \n"," block5c_drop (Dropout)                                    (None, 14, 14, 112)                     0                    ['block5c_project_bn[0][0]']                                \n","                                                                                                                                                                                    \n"," block5c_add (Add)                                         (None, 14, 14, 112)                     0                    ['block5c_drop[0][0]',                                      \n","                                                                                                                         'block5b_add[0][0]']                                       \n","                                                                                                                                                                                    \n"," block6a_expand_conv (Conv2D)                              (None, 14, 14, 672)                     75264                ['block5c_add[0][0]']                                       \n","                                                                                                                                                                                    \n"," block6a_expand_bn (BatchNormalization)                    (None, 14, 14, 672)                     2688                 ['block6a_expand_conv[0][0]']                               \n","                                                                                                                                                                                    \n"," block6a_expand_activation (Activation)                    (None, 14, 14, 672)                     0                    ['block6a_expand_bn[0][0]']                                 \n","                                                                                                                                                                                    \n"," block6a_dwconv_pad (ZeroPadding2D)                        (None, 17, 17, 672)                     0                    ['block6a_expand_activation[0][0]']                         \n","                                                                                                                                                                                    \n"," block6a_dwconv (DepthwiseConv2D)                          (None, 7, 7, 672)                       16800                ['block6a_dwconv_pad[0][0]']                                \n","                                                                                                                                                                                    \n"," block6a_bn (BatchNormalization)                           (None, 7, 7, 672)                       2688                 ['block6a_dwconv[0][0]']                                    \n","                                                                                                                                                                                    \n"," block6a_activation (Activation)                           (None, 7, 7, 672)                       0                    ['block6a_bn[0][0]']                                        \n","                                                                                                                                                                                    \n"," block6a_se_squeeze (GlobalAveragePooling2D)               (None, 672)                             0                    ['block6a_activation[0][0]']                                \n","                                                                                                                                                                                    \n"," block6a_se_reshape (Reshape)                              (None, 1, 1, 672)                       0                    ['block6a_se_squeeze[0][0]']                                \n","                                                                                                                                                                                    \n"," block6a_se_reduce (Conv2D)                                (None, 1, 1, 28)                        18844                ['block6a_se_reshape[0][0]']                                \n","                                                                                                                                                                                    \n"," block6a_se_expand (Conv2D)                                (None, 1, 1, 672)                       19488                ['block6a_se_reduce[0][0]']                                 \n","                                                                                                                                                                                    \n"," block6a_se_excite (Multiply)                              (None, 7, 7, 672)                       0                    ['block6a_activation[0][0]',                                \n","                                                                                                                         'block6a_se_expand[0][0]']                                 \n","                                                                                                                                                                                    \n"," block6a_project_conv (Conv2D)                             (None, 7, 7, 192)                       129024               ['block6a_se_excite[0][0]']                                 \n","                                                                                                                                                                                    \n"," block6a_project_bn (BatchNormalization)                   (None, 7, 7, 192)                       768                  ['block6a_project_conv[0][0]']                              \n","                                                                                                                                                                                    \n"," block6b_expand_conv (Conv2D)                              (None, 7, 7, 1152)                      221184               ['block6a_project_bn[0][0]']                                \n","                                                                                                                                                                                    \n"," block6b_expand_bn (BatchNormalization)                    (None, 7, 7, 1152)                      4608                 ['block6b_expand_conv[0][0]']                               \n","                                                                                                                                                                                    \n"," block6b_expand_activation (Activation)                    (None, 7, 7, 1152)                      0                    ['block6b_expand_bn[0][0]']                                 \n","                                                                                                                                                                                    \n"," block6b_dwconv (DepthwiseConv2D)                          (None, 7, 7, 1152)                      28800                ['block6b_expand_activation[0][0]']                         \n","                                                                                                                                                                                    \n"," block6b_bn (BatchNormalization)                           (None, 7, 7, 1152)                      4608                 ['block6b_dwconv[0][0]']                                    \n","                                                                                                                                                                                    \n"," block6b_activation (Activation)                           (None, 7, 7, 1152)                      0                    ['block6b_bn[0][0]']                                        \n","                                                                                                                                                                                    \n"," block6b_se_squeeze (GlobalAveragePooling2D)               (None, 1152)                            0                    ['block6b_activation[0][0]']                                \n","                                                                                                                                                                                    \n"," block6b_se_reshape (Reshape)                              (None, 1, 1, 1152)                      0                    ['block6b_se_squeeze[0][0]']                                \n","                                                                                                                                                                                    \n"," block6b_se_reduce (Conv2D)                                (None, 1, 1, 48)                        55344                ['block6b_se_reshape[0][0]']                                \n","                                                                                                                                                                                    \n"," block6b_se_expand (Conv2D)                                (None, 1, 1, 1152)                      56448                ['block6b_se_reduce[0][0]']                                 \n","                                                                                                                                                                                    \n"," block6b_se_excite (Multiply)                              (None, 7, 7, 1152)                      0                    ['block6b_activation[0][0]',                                \n","                                                                                                                         'block6b_se_expand[0][0]']                                 \n","                                                                                                                                                                                    \n"," block6b_project_conv (Conv2D)                             (None, 7, 7, 192)                       221184               ['block6b_se_excite[0][0]']                                 \n","                                                                                                                                                                                    \n"," block6b_project_bn (BatchNormalization)                   (None, 7, 7, 192)                       768                  ['block6b_project_conv[0][0]']                              \n","                                                                                                                                                                                    \n"," block6b_drop (Dropout)                                    (None, 7, 7, 192)                       0                    ['block6b_project_bn[0][0]']                                \n","                                                                                                                                                                                    \n"," block6b_add (Add)                                         (None, 7, 7, 192)                       0                    ['block6b_drop[0][0]',                                      \n","                                                                                                                         'block6a_project_bn[0][0]']                                \n","                                                                                                                                                                                    \n"," block6c_expand_conv (Conv2D)                              (None, 7, 7, 1152)                      221184               ['block6b_add[0][0]']                                       \n","                                                                                                                                                                                    \n"," block6c_expand_bn (BatchNormalization)                    (None, 7, 7, 1152)                      4608                 ['block6c_expand_conv[0][0]']                               \n","                                                                                                                                                                                    \n"," block6c_expand_activation (Activation)                    (None, 7, 7, 1152)                      0                    ['block6c_expand_bn[0][0]']                                 \n","                                                                                                                                                                                    \n"," block6c_dwconv (DepthwiseConv2D)                          (None, 7, 7, 1152)                      28800                ['block6c_expand_activation[0][0]']                         \n","                                                                                                                                                                                    \n"," block6c_bn (BatchNormalization)                           (None, 7, 7, 1152)                      4608                 ['block6c_dwconv[0][0]']                                    \n","                                                                                                                                                                                    \n"," block6c_activation (Activation)                           (None, 7, 7, 1152)                      0                    ['block6c_bn[0][0]']                                        \n","                                                                                                                                                                                    \n"," block6c_se_squeeze (GlobalAveragePooling2D)               (None, 1152)                            0                    ['block6c_activation[0][0]']                                \n","                                                                                                                                                                                    \n"," block6c_se_reshape (Reshape)                              (None, 1, 1, 1152)                      0                    ['block6c_se_squeeze[0][0]']                                \n","                                                                                                                                                                                    \n"," block6c_se_reduce (Conv2D)                                (None, 1, 1, 48)                        55344                ['block6c_se_reshape[0][0]']                                \n","                                                                                                                                                                                    \n"," block6c_se_expand (Conv2D)                                (None, 1, 1, 1152)                      56448                ['block6c_se_reduce[0][0]']                                 \n","                                                                                                                                                                                    \n"," block6c_se_excite (Multiply)                              (None, 7, 7, 1152)                      0                    ['block6c_activation[0][0]',                                \n","                                                                                                                         'block6c_se_expand[0][0]']                                 \n","                                                                                                                                                                                    \n"," block6c_project_conv (Conv2D)                             (None, 7, 7, 192)                       221184               ['block6c_se_excite[0][0]']                                 \n","                                                                                                                                                                                    \n"," block6c_project_bn (BatchNormalization)                   (None, 7, 7, 192)                       768                  ['block6c_project_conv[0][0]']                              \n","                                                                                                                                                                                    \n"," block6c_drop (Dropout)                                    (None, 7, 7, 192)                       0                    ['block6c_project_bn[0][0]']                                \n","                                                                                                                                                                                    \n"," block6c_add (Add)                                         (None, 7, 7, 192)                       0                    ['block6c_drop[0][0]',                                      \n","                                                                                                                         'block6b_add[0][0]']                                       \n","                                                                                                                                                                                    \n"," block6d_expand_conv (Conv2D)                              (None, 7, 7, 1152)                      221184               ['block6c_add[0][0]']                                       \n","                                                                                                                                                                                    \n"," block6d_expand_bn (BatchNormalization)                    (None, 7, 7, 1152)                      4608                 ['block6d_expand_conv[0][0]']                               \n","                                                                                                                                                                                    \n"," block6d_expand_activation (Activation)                    (None, 7, 7, 1152)                      0                    ['block6d_expand_bn[0][0]']                                 \n","                                                                                                                                                                                    \n"," block6d_dwconv (DepthwiseConv2D)                          (None, 7, 7, 1152)                      28800                ['block6d_expand_activation[0][0]']                         \n","                                                                                                                                                                                    \n"," block6d_bn (BatchNormalization)                           (None, 7, 7, 1152)                      4608                 ['block6d_dwconv[0][0]']                                    \n","                                                                                                                                                                                    \n"," block6d_activation (Activation)                           (None, 7, 7, 1152)                      0                    ['block6d_bn[0][0]']                                        \n","                                                                                                                                                                                    \n"," block6d_se_squeeze (GlobalAveragePooling2D)               (None, 1152)                            0                    ['block6d_activation[0][0]']                                \n","                                                                                                                                                                                    \n"," block6d_se_reshape (Reshape)                              (None, 1, 1, 1152)                      0                    ['block6d_se_squeeze[0][0]']                                \n","                                                                                                                                                                                    \n"," block6d_se_reduce (Conv2D)                                (None, 1, 1, 48)                        55344                ['block6d_se_reshape[0][0]']                                \n","                                                                                                                                                                                    \n"," block6d_se_expand (Conv2D)                                (None, 1, 1, 1152)                      56448                ['block6d_se_reduce[0][0]']                                 \n","                                                                                                                                                                                    \n"," block6d_se_excite (Multiply)                              (None, 7, 7, 1152)                      0                    ['block6d_activation[0][0]',                                \n","                                                                                                                         'block6d_se_expand[0][0]']                                 \n","                                                                                                                                                                                    \n"," block6d_project_conv (Conv2D)                             (None, 7, 7, 192)                       221184               ['block6d_se_excite[0][0]']                                 \n","                                                                                                                                                                                    \n"," block6d_project_bn (BatchNormalization)                   (None, 7, 7, 192)                       768                  ['block6d_project_conv[0][0]']                              \n","                                                                                                                                                                                    \n"," block6d_drop (Dropout)                                    (None, 7, 7, 192)                       0                    ['block6d_project_bn[0][0]']                                \n","                                                                                                                                                                                    \n"," block6d_add (Add)                                         (None, 7, 7, 192)                       0                    ['block6d_drop[0][0]',                                      \n","                                                                                                                         'block6c_add[0][0]']                                       \n","                                                                                                                                                                                    \n"," block7a_expand_conv (Conv2D)                              (None, 7, 7, 1152)                      221184               ['block6d_add[0][0]']                                       \n","                                                                                                                                                                                    \n"," block7a_expand_bn (BatchNormalization)                    (None, 7, 7, 1152)                      4608                 ['block7a_expand_conv[0][0]']                               \n","                                                                                                                                                                                    \n"," block7a_expand_activation (Activation)                    (None, 7, 7, 1152)                      0                    ['block7a_expand_bn[0][0]']                                 \n","                                                                                                                                                                                    \n"," block7a_dwconv (DepthwiseConv2D)                          (None, 7, 7, 1152)                      10368                ['block7a_expand_activation[0][0]']                         \n","                                                                                                                                                                                    \n"," block7a_bn (BatchNormalization)                           (None, 7, 7, 1152)                      4608                 ['block7a_dwconv[0][0]']                                    \n","                                                                                                                                                                                    \n"," block7a_activation (Activation)                           (None, 7, 7, 1152)                      0                    ['block7a_bn[0][0]']                                        \n","                                                                                                                                                                                    \n"," block7a_se_squeeze (GlobalAveragePooling2D)               (None, 1152)                            0                    ['block7a_activation[0][0]']                                \n","                                                                                                                                                                                    \n"," block7a_se_reshape (Reshape)                              (None, 1, 1, 1152)                      0                    ['block7a_se_squeeze[0][0]']                                \n","                                                                                                                                                                                    \n"," block7a_se_reduce (Conv2D)                                (None, 1, 1, 48)                        55344                ['block7a_se_reshape[0][0]']                                \n","                                                                                                                                                                                    \n"," block7a_se_expand (Conv2D)                                (None, 1, 1, 1152)                      56448                ['block7a_se_reduce[0][0]']                                 \n","                                                                                                                                                                                    \n"," block7a_se_excite (Multiply)                              (None, 7, 7, 1152)                      0                    ['block7a_activation[0][0]',                                \n","                                                                                                                         'block7a_se_expand[0][0]']                                 \n","                                                                                                                                                                                    \n"," block7a_project_conv (Conv2D)                             (None, 7, 7, 320)                       368640               ['block7a_se_excite[0][0]']                                 \n","                                                                                                                                                                                    \n"," block7a_project_bn (BatchNormalization)                   (None, 7, 7, 320)                       1280                 ['block7a_project_conv[0][0]']                              \n","                                                                                                                                                                                    \n"," top_conv (Conv2D)                                         (None, 7, 7, 1280)                      409600               ['block7a_project_bn[0][0]']                                \n","                                                                                                                                                                                    \n"," top_bn (BatchNormalization)                               (None, 7, 7, 1280)                      5120                 ['top_conv[0][0]']                                          \n","                                                                                                                                                                                    \n"," top_activation (Activation)                               (None, 7, 7, 1280)                      0                    ['top_bn[0][0]']                                            \n","                                                                                                                                                                                    \n"," avg_pool (GlobalAveragePooling2D)                         (None, 1280)                            0                    ['top_activation[0][0]']                                    \n","                                                                                                                                                                                    \n"," top_dropout (Dropout)                                     (None, 1280)                            0                    ['avg_pool[0][0]']                                          \n","                                                                                                                                                                                    \n"," probs (Dense)                                             (None, 8)                               10248                ['top_dropout[0][0]']                                       \n","                                                                                                                                                                                    \n","====================================================================================================================================================================================\n","Total params: 4,059,812\n","Trainable params: 4,017,796\n","Non-trainable params: 42,016\n","____________________________________________________________________________________________________________________________________________________________________________________\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_ops.py:5161: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n","model flops: 389966532\n","Finished: /content/drive/MyDrive/output/JP30N02--1\n"]}],"source":["work_on_keffnet(show_model=True, run_fit=False, test_results=False)"]},{"cell_type":"markdown","metadata":{"id":"x7RjCRzmxhce"},"source":["# Fitting"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"edbu3-Y6THos","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"7e5889a4-2e07-42e4-e4d8-6c6a9bee2a41","executionInfo":{"status":"ok","timestamp":1644823399492,"user_tz":180,"elapsed":3847055,"user":{"displayName":"Joao Schuler","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10970916006391371112"}}},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Running: /content/drive/MyDrive/output/JP30N02--1\n","Epoch 1/1000\n","63/63 [==============================] - ETA: 0s - loss: 1.8772 - accuracy: 0.4404\n","Epoch 00001: val_loss improved from inf to 2.53909, saving model to /content/drive/MyDrive/output/JP30N02--1-best_result.hdf5\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py:1410: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n","  layer_config = serialize_layer_fn(layer)\n"]},{"output_type":"stream","name":"stdout","text":["63/63 [==============================] - 52s 433ms/step - loss: 1.8772 - accuracy: 0.4404 - val_loss: 2.5391 - val_accuracy: 0.1250 - lr: 0.0010\n","Epoch 2/1000\n","63/63 [==============================] - ETA: 0s - loss: 1.3373 - accuracy: 0.4947\n","Epoch 00002: val_loss did not improve from 2.53909\n","63/63 [==============================] - 22s 324ms/step - loss: 1.3373 - accuracy: 0.4947 - val_loss: 4.1069 - val_accuracy: 0.1250 - lr: 0.0015\n","Epoch 3/1000\n","63/63 [==============================] - ETA: 0s - loss: 1.0649 - accuracy: 0.6180\n","Epoch 00003: val_loss did not improve from 2.53909\n","63/63 [==============================] - 23s 326ms/step - loss: 1.0649 - accuracy: 0.6180 - val_loss: 6.7736 - val_accuracy: 0.1801 - lr: 0.0020\n","Epoch 4/1000\n","63/63 [==============================] - ETA: 0s - loss: 1.1089 - accuracy: 0.6315\n","Epoch 00004: val_loss did not improve from 2.53909\n","63/63 [==============================] - 22s 324ms/step - loss: 1.1089 - accuracy: 0.6315 - val_loss: 198.1289 - val_accuracy: 0.1250 - lr: 0.0025\n","Epoch 5/1000\n","63/63 [==============================] - ETA: 0s - loss: 1.0874 - accuracy: 0.6283\n","Epoch 00005: val_loss did not improve from 2.53909\n","63/63 [==============================] - 23s 327ms/step - loss: 1.0874 - accuracy: 0.6283 - val_loss: 4.5179 - val_accuracy: 0.3175 - lr: 0.0030\n","Epoch 6/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.9045 - accuracy: 0.6718\n","Epoch 00006: val_loss did not improve from 2.53909\n","63/63 [==============================] - 23s 326ms/step - loss: 0.9045 - accuracy: 0.6718 - val_loss: 206.6918 - val_accuracy: 0.3591 - lr: 0.0035\n","Epoch 7/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.7669 - accuracy: 0.7204\n","Epoch 00007: val_loss did not improve from 2.53909\n","63/63 [==============================] - 22s 322ms/step - loss: 0.7669 - accuracy: 0.7204 - val_loss: 257.8653 - val_accuracy: 0.3368 - lr: 0.0040\n","Epoch 8/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.7157 - accuracy: 0.7362\n","Epoch 00008: val_loss did not improve from 2.53909\n","63/63 [==============================] - 23s 323ms/step - loss: 0.7157 - accuracy: 0.7362 - val_loss: 283.7991 - val_accuracy: 0.3571 - lr: 0.0040\n","Epoch 9/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.6264 - accuracy: 0.7761\n","Epoch 00009: val_loss did not improve from 2.53909\n","63/63 [==============================] - 23s 326ms/step - loss: 0.6264 - accuracy: 0.7761 - val_loss: 3.0991 - val_accuracy: 0.5134 - lr: 0.0034\n","Epoch 10/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.5327 - accuracy: 0.8019\n","Epoch 00010: val_loss did not improve from 2.53909\n","63/63 [==============================] - 23s 325ms/step - loss: 0.5327 - accuracy: 0.8019 - val_loss: 8.3325 - val_accuracy: 0.5322 - lr: 0.0029\n","Epoch 11/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.4967 - accuracy: 0.8191\n","Epoch 00011: val_loss improved from 2.53909 to 0.79467, saving model to /content/drive/MyDrive/output/JP30N02--1-best_result.hdf5\n","63/63 [==============================] - 25s 367ms/step - loss: 0.4967 - accuracy: 0.8191 - val_loss: 0.7947 - val_accuracy: 0.7619 - lr: 0.0025\n","Epoch 12/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.4476 - accuracy: 0.8302\n","Epoch 00012: val_loss did not improve from 0.79467\n","63/63 [==============================] - 23s 324ms/step - loss: 0.4476 - accuracy: 0.8302 - val_loss: 2.0027 - val_accuracy: 0.6230 - lr: 0.0021\n","Epoch 13/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.4404 - accuracy: 0.8397\n","Epoch 00013: val_loss did not improve from 0.79467\n","63/63 [==============================] - 22s 324ms/step - loss: 0.4404 - accuracy: 0.8397 - val_loss: 0.8310 - val_accuracy: 0.7009 - lr: 0.0018\n","Epoch 14/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.3968 - accuracy: 0.8545\n","Epoch 00014: val_loss improved from 0.79467 to 0.77609, saving model to /content/drive/MyDrive/output/JP30N02--1-best_result.hdf5\n","63/63 [==============================] - 25s 369ms/step - loss: 0.3968 - accuracy: 0.8545 - val_loss: 0.7761 - val_accuracy: 0.7103 - lr: 0.0015\n","Epoch 15/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.3609 - accuracy: 0.8707\n","Epoch 00015: val_loss improved from 0.77609 to 0.50717, saving model to /content/drive/MyDrive/output/JP30N02--1-best_result.hdf5\n","63/63 [==============================] - 25s 369ms/step - loss: 0.3609 - accuracy: 0.8707 - val_loss: 0.5072 - val_accuracy: 0.7991 - lr: 0.0013\n","Epoch 16/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.3458 - accuracy: 0.8737\n","Epoch 00016: val_loss did not improve from 0.50717\n","63/63 [==============================] - 23s 327ms/step - loss: 0.3458 - accuracy: 0.8737 - val_loss: 1.6887 - val_accuracy: 0.6840 - lr: 0.0011\n","Epoch 17/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.3268 - accuracy: 0.8830\n","Epoch 00017: val_loss did not improve from 0.50717\n","63/63 [==============================] - 23s 326ms/step - loss: 0.3268 - accuracy: 0.8830 - val_loss: 2.6722 - val_accuracy: 0.5977 - lr: 9.2647e-04\n","Epoch 18/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.3054 - accuracy: 0.8925\n","Epoch 00018: val_loss did not improve from 0.50717\n","63/63 [==============================] - 23s 323ms/step - loss: 0.3054 - accuracy: 0.8925 - val_loss: 2.5597 - val_accuracy: 0.6691 - lr: 7.8750e-04\n","Epoch 19/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.3120 - accuracy: 0.8938\n","Epoch 00019: val_loss did not improve from 0.50717\n","63/63 [==============================] - 23s 327ms/step - loss: 0.3120 - accuracy: 0.8938 - val_loss: 0.8363 - val_accuracy: 0.6964 - lr: 6.6937e-04\n","Epoch 20/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2956 - accuracy: 0.8910\n","Epoch 00020: val_loss did not improve from 0.50717\n","63/63 [==============================] - 23s 324ms/step - loss: 0.2956 - accuracy: 0.8910 - val_loss: 2.5785 - val_accuracy: 0.6429 - lr: 5.6897e-04\n","Epoch 21/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2890 - accuracy: 0.9003\n","Epoch 00021: val_loss did not improve from 0.50717\n","63/63 [==============================] - 23s 326ms/step - loss: 0.2890 - accuracy: 0.9003 - val_loss: 1.8043 - val_accuracy: 0.6756 - lr: 4.8362e-04\n","Epoch 22/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2705 - accuracy: 0.9023\n","Epoch 00022: val_loss did not improve from 0.50717\n","63/63 [==============================] - 22s 324ms/step - loss: 0.2705 - accuracy: 0.9023 - val_loss: 0.8930 - val_accuracy: 0.7857 - lr: 4.1108e-04\n","Epoch 23/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2564 - accuracy: 0.9073\n","Epoch 00023: val_loss did not improve from 0.50717\n","63/63 [==============================] - 23s 325ms/step - loss: 0.2564 - accuracy: 0.9073 - val_loss: 0.5906 - val_accuracy: 0.7664 - lr: 3.4942e-04\n","Epoch 24/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2701 - accuracy: 0.9043\n","Epoch 00024: val_loss improved from 0.50717 to 0.37576, saving model to /content/drive/MyDrive/output/JP30N02--1-best_result.hdf5\n","63/63 [==============================] - 25s 373ms/step - loss: 0.2701 - accuracy: 0.9043 - val_loss: 0.3758 - val_accuracy: 0.8398 - lr: 2.9700e-04\n","Epoch 25/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2445 - accuracy: 0.9126\n","Epoch 00025: val_loss did not improve from 0.37576\n","63/63 [==============================] - 22s 327ms/step - loss: 0.2445 - accuracy: 0.9126 - val_loss: 0.9893 - val_accuracy: 0.7475 - lr: 2.5245e-04\n","Epoch 26/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.3045 - accuracy: 0.8875\n","Epoch 00026: val_loss did not improve from 0.37576\n","63/63 [==============================] - 23s 328ms/step - loss: 0.3045 - accuracy: 0.8875 - val_loss: 18.1899 - val_accuracy: 0.6915 - lr: 0.0010\n","Epoch 27/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.3111 - accuracy: 0.8908\n","Epoch 00027: val_loss did not improve from 0.37576\n","63/63 [==============================] - 22s 324ms/step - loss: 0.3111 - accuracy: 0.8908 - val_loss: 9.9864 - val_accuracy: 0.7108 - lr: 0.0015\n","Epoch 28/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.3568 - accuracy: 0.8780\n","Epoch 00028: val_loss did not improve from 0.37576\n","63/63 [==============================] - 22s 324ms/step - loss: 0.3568 - accuracy: 0.8780 - val_loss: 9.8520 - val_accuracy: 0.6230 - lr: 0.0020\n","Epoch 29/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.3806 - accuracy: 0.8682\n","Epoch 00029: val_loss did not improve from 0.37576\n","63/63 [==============================] - 23s 328ms/step - loss: 0.3806 - accuracy: 0.8682 - val_loss: 4.0777 - val_accuracy: 0.7073 - lr: 0.0025\n","Epoch 30/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.3872 - accuracy: 0.8620\n","Epoch 00030: val_loss did not improve from 0.37576\n","63/63 [==============================] - 22s 325ms/step - loss: 0.3872 - accuracy: 0.8620 - val_loss: 2.5587 - val_accuracy: 0.6181 - lr: 0.0030\n","Epoch 31/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.4231 - accuracy: 0.8497\n","Epoch 00031: val_loss did not improve from 0.37576\n","63/63 [==============================] - 22s 324ms/step - loss: 0.4231 - accuracy: 0.8497 - val_loss: 5.7411 - val_accuracy: 0.5918 - lr: 0.0035\n","Epoch 32/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.4223 - accuracy: 0.8545\n","Epoch 00032: val_loss did not improve from 0.37576\n","63/63 [==============================] - 22s 324ms/step - loss: 0.4223 - accuracy: 0.8545 - val_loss: 37.4385 - val_accuracy: 0.6558 - lr: 0.0040\n","Epoch 33/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.4162 - accuracy: 0.8502\n","Epoch 00033: val_loss did not improve from 0.37576\n","63/63 [==============================] - 22s 324ms/step - loss: 0.4162 - accuracy: 0.8502 - val_loss: 36.5974 - val_accuracy: 0.3760 - lr: 0.0040\n","Epoch 34/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.4045 - accuracy: 0.8610\n","Epoch 00034: val_loss did not improve from 0.37576\n","63/63 [==============================] - 23s 326ms/step - loss: 0.4045 - accuracy: 0.8610 - val_loss: 2.8735 - val_accuracy: 0.5640 - lr: 0.0034\n","Epoch 35/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.3536 - accuracy: 0.8780\n","Epoch 00035: val_loss did not improve from 0.37576\n","63/63 [==============================] - 22s 323ms/step - loss: 0.3536 - accuracy: 0.8780 - val_loss: 1.7555 - val_accuracy: 0.7396 - lr: 0.0029\n","Epoch 36/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.3340 - accuracy: 0.8835\n","Epoch 00036: val_loss did not improve from 0.37576\n","63/63 [==============================] - 22s 327ms/step - loss: 0.3340 - accuracy: 0.8835 - val_loss: 2.1875 - val_accuracy: 0.6880 - lr: 0.0025\n","Epoch 37/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.3091 - accuracy: 0.8933\n","Epoch 00037: val_loss did not improve from 0.37576\n","63/63 [==============================] - 22s 325ms/step - loss: 0.3091 - accuracy: 0.8933 - val_loss: 2.5521 - val_accuracy: 0.6414 - lr: 0.0021\n","Epoch 38/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2775 - accuracy: 0.8995\n","Epoch 00038: val_loss did not improve from 0.37576\n","63/63 [==============================] - 22s 323ms/step - loss: 0.2775 - accuracy: 0.8995 - val_loss: 1.0044 - val_accuracy: 0.6761 - lr: 0.0018\n","Epoch 39/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2478 - accuracy: 0.9128\n","Epoch 00039: val_loss did not improve from 0.37576\n","63/63 [==============================] - 22s 325ms/step - loss: 0.2478 - accuracy: 0.9128 - val_loss: 2.2471 - val_accuracy: 0.6037 - lr: 0.0015\n","Epoch 40/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2651 - accuracy: 0.9106\n","Epoch 00040: val_loss did not improve from 0.37576\n","63/63 [==============================] - 22s 326ms/step - loss: 0.2651 - accuracy: 0.9106 - val_loss: 3.6442 - val_accuracy: 0.7232 - lr: 0.0013\n","Epoch 41/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2296 - accuracy: 0.9198\n","Epoch 00041: val_loss did not improve from 0.37576\n","63/63 [==============================] - 23s 327ms/step - loss: 0.2296 - accuracy: 0.9198 - val_loss: 0.3758 - val_accuracy: 0.8690 - lr: 0.0011\n","Epoch 42/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2242 - accuracy: 0.9269\n","Epoch 00042: val_loss improved from 0.37576 to 0.33463, saving model to /content/drive/MyDrive/output/JP30N02--1-best_result.hdf5\n","63/63 [==============================] - 25s 373ms/step - loss: 0.2242 - accuracy: 0.9269 - val_loss: 0.3346 - val_accuracy: 0.8695 - lr: 9.2647e-04\n","Epoch 43/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2172 - accuracy: 0.9238\n","Epoch 00043: val_loss did not improve from 0.33463\n","63/63 [==============================] - 23s 327ms/step - loss: 0.2172 - accuracy: 0.9238 - val_loss: 0.7669 - val_accuracy: 0.7966 - lr: 7.8750e-04\n","Epoch 44/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2102 - accuracy: 0.9279\n","Epoch 00044: val_loss did not improve from 0.33463\n","63/63 [==============================] - 22s 322ms/step - loss: 0.2102 - accuracy: 0.9279 - val_loss: 0.7434 - val_accuracy: 0.7550 - lr: 6.6937e-04\n","Epoch 45/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2116 - accuracy: 0.9251\n","Epoch 00045: val_loss did not improve from 0.33463\n","63/63 [==============================] - 22s 323ms/step - loss: 0.2116 - accuracy: 0.9251 - val_loss: 0.7142 - val_accuracy: 0.7798 - lr: 5.6897e-04\n","Epoch 46/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1935 - accuracy: 0.9321\n","Epoch 00046: val_loss improved from 0.33463 to 0.21672, saving model to /content/drive/MyDrive/output/JP30N02--1-best_result.hdf5\n","63/63 [==============================] - 26s 372ms/step - loss: 0.1935 - accuracy: 0.9321 - val_loss: 0.2167 - val_accuracy: 0.9182 - lr: 4.8362e-04\n","Epoch 47/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1820 - accuracy: 0.9339\n","Epoch 00047: val_loss did not improve from 0.21672\n","63/63 [==============================] - 23s 326ms/step - loss: 0.1820 - accuracy: 0.9339 - val_loss: 0.2741 - val_accuracy: 0.8938 - lr: 4.1108e-04\n","Epoch 48/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1675 - accuracy: 0.9406\n","Epoch 00048: val_loss improved from 0.21672 to 0.19870, saving model to /content/drive/MyDrive/output/JP30N02--1-best_result.hdf5\n","63/63 [==============================] - 25s 370ms/step - loss: 0.1675 - accuracy: 0.9406 - val_loss: 0.1987 - val_accuracy: 0.9410 - lr: 3.4942e-04\n","Epoch 49/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1885 - accuracy: 0.9354\n","Epoch 00049: val_loss did not improve from 0.19870\n","63/63 [==============================] - 22s 325ms/step - loss: 0.1885 - accuracy: 0.9354 - val_loss: 0.2755 - val_accuracy: 0.8834 - lr: 2.9700e-04\n","Epoch 50/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1747 - accuracy: 0.9409\n","Epoch 00050: val_loss improved from 0.19870 to 0.17515, saving model to /content/drive/MyDrive/output/JP30N02--1-best_result.hdf5\n","63/63 [==============================] - 24s 367ms/step - loss: 0.1747 - accuracy: 0.9409 - val_loss: 0.1752 - val_accuracy: 0.9395 - lr: 2.5245e-04\n","Epoch 51/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2003 - accuracy: 0.9351\n","Epoch 00051: val_loss did not improve from 0.17515\n","63/63 [==============================] - 22s 326ms/step - loss: 0.2003 - accuracy: 0.9351 - val_loss: 0.9228 - val_accuracy: 0.7073 - lr: 0.0010\n","Epoch 52/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2090 - accuracy: 0.9256\n","Epoch 00052: val_loss did not improve from 0.17515\n","63/63 [==============================] - 22s 326ms/step - loss: 0.2090 - accuracy: 0.9256 - val_loss: 0.7514 - val_accuracy: 0.8472 - lr: 0.0015\n","Epoch 53/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2365 - accuracy: 0.9198\n","Epoch 00053: val_loss did not improve from 0.17515\n","63/63 [==============================] - 23s 325ms/step - loss: 0.2365 - accuracy: 0.9198 - val_loss: 4.1156 - val_accuracy: 0.7396 - lr: 0.0020\n","Epoch 54/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2653 - accuracy: 0.9061\n","Epoch 00054: val_loss did not improve from 0.17515\n","63/63 [==============================] - 22s 328ms/step - loss: 0.2653 - accuracy: 0.9061 - val_loss: 5.3963 - val_accuracy: 0.7034 - lr: 0.0025\n","Epoch 55/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.3230 - accuracy: 0.8940\n","Epoch 00055: val_loss did not improve from 0.17515\n","63/63 [==============================] - 22s 327ms/step - loss: 0.3230 - accuracy: 0.8940 - val_loss: 2.9341 - val_accuracy: 0.6791 - lr: 0.0030\n","Epoch 56/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.3039 - accuracy: 0.8925\n","Epoch 00056: val_loss did not improve from 0.17515\n","63/63 [==============================] - 23s 325ms/step - loss: 0.3039 - accuracy: 0.8925 - val_loss: 1.8268 - val_accuracy: 0.7535 - lr: 0.0035\n","Epoch 57/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.3306 - accuracy: 0.8880\n","Epoch 00057: val_loss did not improve from 0.17515\n","63/63 [==============================] - 23s 326ms/step - loss: 0.3306 - accuracy: 0.8880 - val_loss: 62.8771 - val_accuracy: 0.5317 - lr: 0.0040\n","Epoch 58/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.3301 - accuracy: 0.8863\n","Epoch 00058: val_loss did not improve from 0.17515\n","63/63 [==============================] - 23s 327ms/step - loss: 0.3301 - accuracy: 0.8863 - val_loss: 1.2170 - val_accuracy: 0.7485 - lr: 0.0040\n","Epoch 59/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2829 - accuracy: 0.9001\n","Epoch 00059: val_loss did not improve from 0.17515\n","63/63 [==============================] - 22s 324ms/step - loss: 0.2829 - accuracy: 0.9001 - val_loss: 18.3468 - val_accuracy: 0.6448 - lr: 0.0034\n","Epoch 60/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2592 - accuracy: 0.9091\n","Epoch 00060: val_loss did not improve from 0.17515\n","63/63 [==============================] - 23s 326ms/step - loss: 0.2592 - accuracy: 0.9091 - val_loss: 4.2940 - val_accuracy: 0.6141 - lr: 0.0029\n","Epoch 61/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2212 - accuracy: 0.9269\n","Epoch 00061: val_loss did not improve from 0.17515\n","63/63 [==============================] - 22s 326ms/step - loss: 0.2212 - accuracy: 0.9269 - val_loss: 3.6927 - val_accuracy: 0.6751 - lr: 0.0025\n","Epoch 62/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2384 - accuracy: 0.9183\n","Epoch 00062: val_loss did not improve from 0.17515\n","63/63 [==============================] - 22s 323ms/step - loss: 0.2384 - accuracy: 0.9183 - val_loss: 2.1226 - val_accuracy: 0.8105 - lr: 0.0021\n","Epoch 63/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2272 - accuracy: 0.9226\n","Epoch 00063: val_loss did not improve from 0.17515\n","63/63 [==============================] - 22s 325ms/step - loss: 0.2272 - accuracy: 0.9226 - val_loss: 0.6161 - val_accuracy: 0.8155 - lr: 0.0018\n","Epoch 64/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1965 - accuracy: 0.9324\n","Epoch 00064: val_loss did not improve from 0.17515\n","63/63 [==============================] - 22s 326ms/step - loss: 0.1965 - accuracy: 0.9324 - val_loss: 0.8101 - val_accuracy: 0.7138 - lr: 0.0015\n","Epoch 65/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1870 - accuracy: 0.9356\n","Epoch 00065: val_loss did not improve from 0.17515\n","63/63 [==============================] - 22s 325ms/step - loss: 0.1870 - accuracy: 0.9356 - val_loss: 1.3891 - val_accuracy: 0.8313 - lr: 0.0013\n","Epoch 66/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1800 - accuracy: 0.9414\n","Epoch 00066: val_loss did not improve from 0.17515\n","63/63 [==============================] - 22s 327ms/step - loss: 0.1800 - accuracy: 0.9414 - val_loss: 0.2595 - val_accuracy: 0.9191 - lr: 0.0011\n","Epoch 67/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1692 - accuracy: 0.9414\n","Epoch 00067: val_loss did not improve from 0.17515\n","63/63 [==============================] - 22s 326ms/step - loss: 0.1692 - accuracy: 0.9414 - val_loss: 1.2444 - val_accuracy: 0.8061 - lr: 9.2647e-04\n","Epoch 68/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1676 - accuracy: 0.9421\n","Epoch 00068: val_loss did not improve from 0.17515\n","63/63 [==============================] - 23s 325ms/step - loss: 0.1676 - accuracy: 0.9421 - val_loss: 0.4635 - val_accuracy: 0.8214 - lr: 7.8750e-04\n","Epoch 69/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1548 - accuracy: 0.9499\n","Epoch 00069: val_loss did not improve from 0.17515\n","63/63 [==============================] - 22s 325ms/step - loss: 0.1548 - accuracy: 0.9499 - val_loss: 0.1768 - val_accuracy: 0.9350 - lr: 6.6937e-04\n","Epoch 70/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1527 - accuracy: 0.9512\n","Epoch 00070: val_loss did not improve from 0.17515\n","63/63 [==============================] - 23s 325ms/step - loss: 0.1527 - accuracy: 0.9512 - val_loss: 0.2554 - val_accuracy: 0.9147 - lr: 5.6897e-04\n","Epoch 71/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1562 - accuracy: 0.9431\n","Epoch 00071: val_loss did not improve from 0.17515\n","63/63 [==============================] - 22s 326ms/step - loss: 0.1562 - accuracy: 0.9431 - val_loss: 0.7099 - val_accuracy: 0.8194 - lr: 4.8362e-04\n","Epoch 72/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1532 - accuracy: 0.9464\n","Epoch 00072: val_loss did not improve from 0.17515\n","63/63 [==============================] - 23s 326ms/step - loss: 0.1532 - accuracy: 0.9464 - val_loss: 0.3746 - val_accuracy: 0.8750 - lr: 4.1108e-04\n","Epoch 73/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1515 - accuracy: 0.9489\n","Epoch 00073: val_loss improved from 0.17515 to 0.16168, saving model to /content/drive/MyDrive/output/JP30N02--1-best_result.hdf5\n","63/63 [==============================] - 24s 363ms/step - loss: 0.1515 - accuracy: 0.9489 - val_loss: 0.1617 - val_accuracy: 0.9410 - lr: 3.4942e-04\n","Epoch 74/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1367 - accuracy: 0.9547\n","Epoch 00074: val_loss did not improve from 0.16168\n","63/63 [==============================] - 22s 327ms/step - loss: 0.1367 - accuracy: 0.9547 - val_loss: 0.2072 - val_accuracy: 0.9221 - lr: 2.9700e-04\n","Epoch 75/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1326 - accuracy: 0.9549\n","Epoch 00075: val_loss did not improve from 0.16168\n","63/63 [==============================] - 22s 325ms/step - loss: 0.1326 - accuracy: 0.9549 - val_loss: 0.1765 - val_accuracy: 0.9405 - lr: 2.5245e-04\n","Epoch 76/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1633 - accuracy: 0.9454\n","Epoch 00076: val_loss did not improve from 0.16168\n","63/63 [==============================] - 23s 325ms/step - loss: 0.1633 - accuracy: 0.9454 - val_loss: 1.2650 - val_accuracy: 0.8596 - lr: 0.0010\n","Epoch 77/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1850 - accuracy: 0.9344\n","Epoch 00077: val_loss did not improve from 0.16168\n","63/63 [==============================] - 23s 327ms/step - loss: 0.1850 - accuracy: 0.9344 - val_loss: 1.0311 - val_accuracy: 0.7872 - lr: 0.0015\n","Epoch 78/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2024 - accuracy: 0.9339\n","Epoch 00078: val_loss did not improve from 0.16168\n","63/63 [==============================] - 22s 323ms/step - loss: 0.2024 - accuracy: 0.9339 - val_loss: 4.0529 - val_accuracy: 0.6984 - lr: 0.0020\n","Epoch 79/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2180 - accuracy: 0.9299\n","Epoch 00079: val_loss did not improve from 0.16168\n","63/63 [==============================] - 22s 323ms/step - loss: 0.2180 - accuracy: 0.9299 - val_loss: 0.5322 - val_accuracy: 0.8601 - lr: 0.0025\n","Epoch 80/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2464 - accuracy: 0.9133\n","Epoch 00080: val_loss did not improve from 0.16168\n","63/63 [==============================] - 23s 326ms/step - loss: 0.2464 - accuracy: 0.9133 - val_loss: 1.3171 - val_accuracy: 0.7088 - lr: 0.0030\n","Epoch 81/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2549 - accuracy: 0.9138\n","Epoch 00081: val_loss did not improve from 0.16168\n","63/63 [==============================] - 22s 326ms/step - loss: 0.2549 - accuracy: 0.9138 - val_loss: 2.1114 - val_accuracy: 0.6538 - lr: 0.0035\n","Epoch 82/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2793 - accuracy: 0.9058\n","Epoch 00082: val_loss did not improve from 0.16168\n","63/63 [==============================] - 23s 326ms/step - loss: 0.2793 - accuracy: 0.9058 - val_loss: 1.2420 - val_accuracy: 0.8408 - lr: 0.0040\n","Epoch 83/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2743 - accuracy: 0.9103\n","Epoch 00083: val_loss did not improve from 0.16168\n","63/63 [==============================] - 22s 342ms/step - loss: 0.2743 - accuracy: 0.9103 - val_loss: 57.8010 - val_accuracy: 0.4385 - lr: 0.0040\n","Epoch 84/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2614 - accuracy: 0.9126\n","Epoch 00084: val_loss did not improve from 0.16168\n","63/63 [==============================] - 22s 329ms/step - loss: 0.2614 - accuracy: 0.9126 - val_loss: 1.7403 - val_accuracy: 0.7316 - lr: 0.0034\n","Epoch 85/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2440 - accuracy: 0.9198\n","Epoch 00085: val_loss did not improve from 0.16168\n","63/63 [==============================] - 22s 327ms/step - loss: 0.2440 - accuracy: 0.9198 - val_loss: 3.1659 - val_accuracy: 0.5918 - lr: 0.0029\n","Epoch 86/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2167 - accuracy: 0.9256\n","Epoch 00086: val_loss did not improve from 0.16168\n","63/63 [==============================] - 22s 326ms/step - loss: 0.2167 - accuracy: 0.9256 - val_loss: 3.3833 - val_accuracy: 0.6642 - lr: 0.0025\n","Epoch 87/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1859 - accuracy: 0.9369\n","Epoch 00087: val_loss did not improve from 0.16168\n","63/63 [==============================] - 23s 328ms/step - loss: 0.1859 - accuracy: 0.9369 - val_loss: 0.9251 - val_accuracy: 0.7326 - lr: 0.0021\n","Epoch 88/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1799 - accuracy: 0.9404\n","Epoch 00088: val_loss did not improve from 0.16168\n","63/63 [==============================] - 22s 325ms/step - loss: 0.1799 - accuracy: 0.9404 - val_loss: 0.7372 - val_accuracy: 0.7664 - lr: 0.0018\n","Epoch 89/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1624 - accuracy: 0.9451\n","Epoch 00089: val_loss did not improve from 0.16168\n","63/63 [==============================] - 22s 323ms/step - loss: 0.1624 - accuracy: 0.9451 - val_loss: 0.9660 - val_accuracy: 0.8611 - lr: 0.0015\n","Epoch 90/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1574 - accuracy: 0.9414\n","Epoch 00090: val_loss did not improve from 0.16168\n","63/63 [==============================] - 22s 326ms/step - loss: 0.1574 - accuracy: 0.9414 - val_loss: 0.6218 - val_accuracy: 0.8983 - lr: 0.0013\n","Epoch 91/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1555 - accuracy: 0.9466\n","Epoch 00091: val_loss did not improve from 0.16168\n","63/63 [==============================] - 22s 324ms/step - loss: 0.1555 - accuracy: 0.9466 - val_loss: 0.8346 - val_accuracy: 0.8943 - lr: 0.0011\n","Epoch 92/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1452 - accuracy: 0.9502\n","Epoch 00092: val_loss did not improve from 0.16168\n","63/63 [==============================] - 22s 325ms/step - loss: 0.1452 - accuracy: 0.9502 - val_loss: 0.4440 - val_accuracy: 0.8537 - lr: 9.2647e-04\n","Epoch 93/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1422 - accuracy: 0.9522\n","Epoch 00093: val_loss did not improve from 0.16168\n","63/63 [==============================] - 22s 324ms/step - loss: 0.1422 - accuracy: 0.9522 - val_loss: 0.4377 - val_accuracy: 0.8492 - lr: 7.8750e-04\n","Epoch 94/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1253 - accuracy: 0.9584\n","Epoch 00094: val_loss did not improve from 0.16168\n","63/63 [==============================] - 22s 325ms/step - loss: 0.1253 - accuracy: 0.9584 - val_loss: 0.3039 - val_accuracy: 0.9201 - lr: 6.6937e-04\n","Epoch 95/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1365 - accuracy: 0.9547\n","Epoch 00095: val_loss did not improve from 0.16168\n","63/63 [==============================] - 22s 322ms/step - loss: 0.1365 - accuracy: 0.9547 - val_loss: 1.5999 - val_accuracy: 0.8636 - lr: 5.6897e-04\n","Epoch 96/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1265 - accuracy: 0.9592\n","Epoch 00096: val_loss did not improve from 0.16168\n","63/63 [==============================] - 22s 326ms/step - loss: 0.1265 - accuracy: 0.9592 - val_loss: 3.1375 - val_accuracy: 0.8110 - lr: 4.8362e-04\n","Epoch 97/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1217 - accuracy: 0.9572\n","Epoch 00097: val_loss did not improve from 0.16168\n","63/63 [==============================] - 23s 324ms/step - loss: 0.1217 - accuracy: 0.9572 - val_loss: 0.1953 - val_accuracy: 0.9256 - lr: 4.1108e-04\n","Epoch 98/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1219 - accuracy: 0.9584\n","Epoch 00098: val_loss did not improve from 0.16168\n","63/63 [==============================] - 23s 329ms/step - loss: 0.1219 - accuracy: 0.9584 - val_loss: 0.1805 - val_accuracy: 0.9345 - lr: 3.4942e-04\n","Epoch 99/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1153 - accuracy: 0.9602\n","Epoch 00099: val_loss improved from 0.16168 to 0.14847, saving model to /content/drive/MyDrive/output/JP30N02--1-best_result.hdf5\n","63/63 [==============================] - 26s 373ms/step - loss: 0.1153 - accuracy: 0.9602 - val_loss: 0.1485 - val_accuracy: 0.9444 - lr: 2.9700e-04\n","Epoch 100/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1178 - accuracy: 0.9579\n","Epoch 00100: val_loss did not improve from 0.14847\n","63/63 [==============================] - 23s 326ms/step - loss: 0.1178 - accuracy: 0.9579 - val_loss: 0.1856 - val_accuracy: 0.9296 - lr: 2.5245e-04\n","Epoch 101/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1334 - accuracy: 0.9519\n","Epoch 00101: val_loss did not improve from 0.14847\n","63/63 [==============================] - 22s 326ms/step - loss: 0.1334 - accuracy: 0.9519 - val_loss: 0.1739 - val_accuracy: 0.9425 - lr: 0.0010\n","Epoch 102/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1635 - accuracy: 0.9461\n","Epoch 00102: val_loss did not improve from 0.14847\n","63/63 [==============================] - 23s 323ms/step - loss: 0.1635 - accuracy: 0.9461 - val_loss: 0.2754 - val_accuracy: 0.9058 - lr: 0.0015\n","Epoch 103/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1668 - accuracy: 0.9391\n","Epoch 00103: val_loss did not improve from 0.14847\n","63/63 [==============================] - 22s 329ms/step - loss: 0.1668 - accuracy: 0.9391 - val_loss: 0.5583 - val_accuracy: 0.8492 - lr: 0.0020\n","Epoch 104/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1687 - accuracy: 0.9411\n","Epoch 00104: val_loss did not improve from 0.14847\n","63/63 [==============================] - 22s 329ms/step - loss: 0.1687 - accuracy: 0.9411 - val_loss: 0.6298 - val_accuracy: 0.8631 - lr: 0.0025\n","Epoch 105/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2163 - accuracy: 0.9309\n","Epoch 00105: val_loss did not improve from 0.14847\n","63/63 [==============================] - 22s 326ms/step - loss: 0.2163 - accuracy: 0.9309 - val_loss: 0.8410 - val_accuracy: 0.7460 - lr: 0.0030\n","Epoch 106/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2289 - accuracy: 0.9238\n","Epoch 00106: val_loss did not improve from 0.14847\n","63/63 [==============================] - 22s 325ms/step - loss: 0.2289 - accuracy: 0.9238 - val_loss: 7.5793 - val_accuracy: 0.6701 - lr: 0.0035\n","Epoch 107/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2609 - accuracy: 0.9136\n","Epoch 00107: val_loss did not improve from 0.14847\n","63/63 [==============================] - 22s 327ms/step - loss: 0.2609 - accuracy: 0.9136 - val_loss: 2.3885 - val_accuracy: 0.7366 - lr: 0.0040\n","Epoch 108/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2411 - accuracy: 0.9183\n","Epoch 00108: val_loss did not improve from 0.14847\n","63/63 [==============================] - 22s 325ms/step - loss: 0.2411 - accuracy: 0.9183 - val_loss: 86.8777 - val_accuracy: 0.6007 - lr: 0.0040\n","Epoch 109/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2127 - accuracy: 0.9294\n","Epoch 00109: val_loss did not improve from 0.14847\n","63/63 [==============================] - 22s 324ms/step - loss: 0.2127 - accuracy: 0.9294 - val_loss: 3.7864 - val_accuracy: 0.6334 - lr: 0.0034\n","Epoch 110/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2155 - accuracy: 0.9238\n","Epoch 00110: val_loss did not improve from 0.14847\n","63/63 [==============================] - 23s 327ms/step - loss: 0.2155 - accuracy: 0.9238 - val_loss: 4.1334 - val_accuracy: 0.6116 - lr: 0.0029\n","Epoch 111/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1911 - accuracy: 0.9351\n","Epoch 00111: val_loss did not improve from 0.14847\n","63/63 [==============================] - 22s 324ms/step - loss: 0.1911 - accuracy: 0.9351 - val_loss: 2.4143 - val_accuracy: 0.5933 - lr: 0.0025\n","Epoch 112/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1682 - accuracy: 0.9436\n","Epoch 00112: val_loss did not improve from 0.14847\n","63/63 [==============================] - 22s 323ms/step - loss: 0.1682 - accuracy: 0.9436 - val_loss: 1.0192 - val_accuracy: 0.8234 - lr: 0.0021\n","Epoch 113/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1493 - accuracy: 0.9522\n","Epoch 00113: val_loss did not improve from 0.14847\n","63/63 [==============================] - 22s 327ms/step - loss: 0.1493 - accuracy: 0.9522 - val_loss: 0.4007 - val_accuracy: 0.9137 - lr: 0.0018\n","Epoch 114/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1487 - accuracy: 0.9494\n","Epoch 00114: val_loss did not improve from 0.14847\n","63/63 [==============================] - 22s 324ms/step - loss: 0.1487 - accuracy: 0.9494 - val_loss: 1.1208 - val_accuracy: 0.7847 - lr: 0.0015\n","Epoch 115/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1318 - accuracy: 0.9554\n","Epoch 00115: val_loss did not improve from 0.14847\n","63/63 [==============================] - 22s 326ms/step - loss: 0.1318 - accuracy: 0.9554 - val_loss: 3.3587 - val_accuracy: 0.8061 - lr: 0.0013\n","Epoch 116/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1284 - accuracy: 0.9562\n","Epoch 00116: val_loss did not improve from 0.14847\n","63/63 [==============================] - 22s 321ms/step - loss: 0.1284 - accuracy: 0.9562 - val_loss: 0.2922 - val_accuracy: 0.9013 - lr: 0.0011\n","Epoch 117/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1221 - accuracy: 0.9589\n","Epoch 00117: val_loss did not improve from 0.14847\n","63/63 [==============================] - 23s 325ms/step - loss: 0.1221 - accuracy: 0.9589 - val_loss: 0.1827 - val_accuracy: 0.9350 - lr: 9.2647e-04\n","Epoch 118/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1160 - accuracy: 0.9579\n","Epoch 00118: val_loss did not improve from 0.14847\n","63/63 [==============================] - 22s 321ms/step - loss: 0.1160 - accuracy: 0.9579 - val_loss: 0.4098 - val_accuracy: 0.9340 - lr: 7.8750e-04\n","Epoch 119/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1071 - accuracy: 0.9619\n","Epoch 00119: val_loss did not improve from 0.14847\n","63/63 [==============================] - 22s 326ms/step - loss: 0.1071 - accuracy: 0.9619 - val_loss: 0.2670 - val_accuracy: 0.9107 - lr: 6.6937e-04\n","Epoch 120/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0990 - accuracy: 0.9662\n","Epoch 00120: val_loss did not improve from 0.14847\n","63/63 [==============================] - 22s 328ms/step - loss: 0.0990 - accuracy: 0.9662 - val_loss: 0.1727 - val_accuracy: 0.9360 - lr: 5.6897e-04\n","Epoch 121/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1070 - accuracy: 0.9614\n","Epoch 00121: val_loss did not improve from 0.14847\n","63/63 [==============================] - 22s 323ms/step - loss: 0.1070 - accuracy: 0.9614 - val_loss: 0.2798 - val_accuracy: 0.8854 - lr: 4.8362e-04\n","Epoch 122/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1126 - accuracy: 0.9582\n","Epoch 00122: val_loss did not improve from 0.14847\n","63/63 [==============================] - 23s 327ms/step - loss: 0.1126 - accuracy: 0.9582 - val_loss: 0.3502 - val_accuracy: 0.8611 - lr: 4.1108e-04\n","Epoch 123/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1046 - accuracy: 0.9629\n","Epoch 00123: val_loss improved from 0.14847 to 0.12098, saving model to /content/drive/MyDrive/output/JP30N02--1-best_result.hdf5\n","63/63 [==============================] - 25s 370ms/step - loss: 0.1046 - accuracy: 0.9629 - val_loss: 0.1210 - val_accuracy: 0.9519 - lr: 3.4942e-04\n","Epoch 124/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0890 - accuracy: 0.9684\n","Epoch 00124: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 329ms/step - loss: 0.0890 - accuracy: 0.9684 - val_loss: 0.1329 - val_accuracy: 0.9529 - lr: 2.9700e-04\n","Epoch 125/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0964 - accuracy: 0.9687\n","Epoch 00125: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0964 - accuracy: 0.9687 - val_loss: 0.4002 - val_accuracy: 0.8755 - lr: 2.5245e-04\n","Epoch 126/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1111 - accuracy: 0.9627\n","Epoch 00126: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 324ms/step - loss: 0.1111 - accuracy: 0.9627 - val_loss: 0.2525 - val_accuracy: 0.9196 - lr: 0.0010\n","Epoch 127/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1165 - accuracy: 0.9599\n","Epoch 00127: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 325ms/step - loss: 0.1165 - accuracy: 0.9599 - val_loss: 0.5708 - val_accuracy: 0.8983 - lr: 0.0015\n","Epoch 128/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1366 - accuracy: 0.9519\n","Epoch 00128: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 324ms/step - loss: 0.1366 - accuracy: 0.9519 - val_loss: 0.2183 - val_accuracy: 0.9271 - lr: 0.0020\n","Epoch 129/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1531 - accuracy: 0.9464\n","Epoch 00129: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 324ms/step - loss: 0.1531 - accuracy: 0.9464 - val_loss: 0.2823 - val_accuracy: 0.9102 - lr: 0.0025\n","Epoch 130/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1797 - accuracy: 0.9404\n","Epoch 00130: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 326ms/step - loss: 0.1797 - accuracy: 0.9404 - val_loss: 6.6997 - val_accuracy: 0.6796 - lr: 0.0030\n","Epoch 131/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2035 - accuracy: 0.9334\n","Epoch 00131: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 323ms/step - loss: 0.2035 - accuracy: 0.9334 - val_loss: 18.2365 - val_accuracy: 0.7257 - lr: 0.0035\n","Epoch 132/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2301 - accuracy: 0.9299\n","Epoch 00132: val_loss did not improve from 0.12098\n","63/63 [==============================] - 23s 326ms/step - loss: 0.2301 - accuracy: 0.9299 - val_loss: 5.1042 - val_accuracy: 0.6761 - lr: 0.0040\n","Epoch 133/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2056 - accuracy: 0.9294\n","Epoch 00133: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 322ms/step - loss: 0.2056 - accuracy: 0.9294 - val_loss: 1.4421 - val_accuracy: 0.7063 - lr: 0.0040\n","Epoch 134/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2094 - accuracy: 0.9319\n","Epoch 00134: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 326ms/step - loss: 0.2094 - accuracy: 0.9319 - val_loss: 5.4699 - val_accuracy: 0.5308 - lr: 0.0034\n","Epoch 135/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1756 - accuracy: 0.9431\n","Epoch 00135: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 324ms/step - loss: 0.1756 - accuracy: 0.9431 - val_loss: 1.6920 - val_accuracy: 0.6751 - lr: 0.0029\n","Epoch 136/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1553 - accuracy: 0.9479\n","Epoch 00136: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 325ms/step - loss: 0.1553 - accuracy: 0.9479 - val_loss: 1.2072 - val_accuracy: 0.7887 - lr: 0.0025\n","Epoch 137/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1581 - accuracy: 0.9461\n","Epoch 00137: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 323ms/step - loss: 0.1581 - accuracy: 0.9461 - val_loss: 0.9154 - val_accuracy: 0.8467 - lr: 0.0021\n","Epoch 138/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1397 - accuracy: 0.9524\n","Epoch 00138: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 325ms/step - loss: 0.1397 - accuracy: 0.9524 - val_loss: 0.4009 - val_accuracy: 0.8656 - lr: 0.0018\n","Epoch 139/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1310 - accuracy: 0.9542\n","Epoch 00139: val_loss did not improve from 0.12098\n","63/63 [==============================] - 23s 329ms/step - loss: 0.1310 - accuracy: 0.9542 - val_loss: 0.2871 - val_accuracy: 0.9003 - lr: 0.0015\n","Epoch 140/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1161 - accuracy: 0.9629\n","Epoch 00140: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 325ms/step - loss: 0.1161 - accuracy: 0.9629 - val_loss: 0.4204 - val_accuracy: 0.8790 - lr: 0.0013\n","Epoch 141/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1117 - accuracy: 0.9632\n","Epoch 00141: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 340ms/step - loss: 0.1117 - accuracy: 0.9632 - val_loss: 0.4496 - val_accuracy: 0.8398 - lr: 0.0011\n","Epoch 142/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1070 - accuracy: 0.9617\n","Epoch 00142: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 326ms/step - loss: 0.1070 - accuracy: 0.9617 - val_loss: 0.1726 - val_accuracy: 0.9325 - lr: 9.2647e-04\n","Epoch 143/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0957 - accuracy: 0.9679\n","Epoch 00143: val_loss did not improve from 0.12098\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0957 - accuracy: 0.9679 - val_loss: 0.2486 - val_accuracy: 0.9038 - lr: 7.8750e-04\n","Epoch 144/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0982 - accuracy: 0.9684\n","Epoch 00144: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0982 - accuracy: 0.9684 - val_loss: 0.3247 - val_accuracy: 0.8795 - lr: 6.6937e-04\n","Epoch 145/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0960 - accuracy: 0.9662\n","Epoch 00145: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0960 - accuracy: 0.9662 - val_loss: 0.2420 - val_accuracy: 0.9147 - lr: 5.6897e-04\n","Epoch 146/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0866 - accuracy: 0.9684\n","Epoch 00146: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0866 - accuracy: 0.9684 - val_loss: 0.1716 - val_accuracy: 0.9514 - lr: 4.8362e-04\n","Epoch 147/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0823 - accuracy: 0.9714\n","Epoch 00147: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0823 - accuracy: 0.9714 - val_loss: 1.1706 - val_accuracy: 0.8348 - lr: 4.1108e-04\n","Epoch 148/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0798 - accuracy: 0.9724\n","Epoch 00148: val_loss did not improve from 0.12098\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0798 - accuracy: 0.9724 - val_loss: 0.1673 - val_accuracy: 0.9400 - lr: 3.4942e-04\n","Epoch 149/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0832 - accuracy: 0.9709\n","Epoch 00149: val_loss did not improve from 0.12098\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0832 - accuracy: 0.9709 - val_loss: 0.1478 - val_accuracy: 0.9499 - lr: 2.9700e-04\n","Epoch 150/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0809 - accuracy: 0.9709\n","Epoch 00150: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0809 - accuracy: 0.9709 - val_loss: 0.1537 - val_accuracy: 0.9524 - lr: 2.5245e-04\n","Epoch 151/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1008 - accuracy: 0.9689\n","Epoch 00151: val_loss did not improve from 0.12098\n","63/63 [==============================] - 23s 329ms/step - loss: 0.1008 - accuracy: 0.9689 - val_loss: 4.0416 - val_accuracy: 0.7361 - lr: 0.0010\n","Epoch 152/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1116 - accuracy: 0.9619\n","Epoch 00152: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 325ms/step - loss: 0.1116 - accuracy: 0.9619 - val_loss: 0.3395 - val_accuracy: 0.8805 - lr: 0.0015\n","Epoch 153/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1492 - accuracy: 0.9542\n","Epoch 00153: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 323ms/step - loss: 0.1492 - accuracy: 0.9542 - val_loss: 1.9788 - val_accuracy: 0.7907 - lr: 0.0020\n","Epoch 154/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1385 - accuracy: 0.9522\n","Epoch 00154: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 323ms/step - loss: 0.1385 - accuracy: 0.9522 - val_loss: 3.2037 - val_accuracy: 0.7128 - lr: 0.0025\n","Epoch 155/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1576 - accuracy: 0.9449\n","Epoch 00155: val_loss did not improve from 0.12098\n","63/63 [==============================] - 23s 324ms/step - loss: 0.1576 - accuracy: 0.9449 - val_loss: 1.1324 - val_accuracy: 0.8065 - lr: 0.0030\n","Epoch 156/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1815 - accuracy: 0.9404\n","Epoch 00156: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 327ms/step - loss: 0.1815 - accuracy: 0.9404 - val_loss: 3.3553 - val_accuracy: 0.7148 - lr: 0.0035\n","Epoch 157/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1956 - accuracy: 0.9324\n","Epoch 00157: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 325ms/step - loss: 0.1956 - accuracy: 0.9324 - val_loss: 4.6466 - val_accuracy: 0.6458 - lr: 0.0040\n","Epoch 158/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.2159 - accuracy: 0.9271\n","Epoch 00158: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 322ms/step - loss: 0.2159 - accuracy: 0.9271 - val_loss: 1.7484 - val_accuracy: 0.7951 - lr: 0.0040\n","Epoch 159/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1823 - accuracy: 0.9386\n","Epoch 00159: val_loss did not improve from 0.12098\n","63/63 [==============================] - 23s 326ms/step - loss: 0.1823 - accuracy: 0.9386 - val_loss: 1.8669 - val_accuracy: 0.7436 - lr: 0.0034\n","Epoch 160/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1517 - accuracy: 0.9461\n","Epoch 00160: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 323ms/step - loss: 0.1517 - accuracy: 0.9461 - val_loss: 2.4787 - val_accuracy: 0.7659 - lr: 0.0029\n","Epoch 161/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1473 - accuracy: 0.9481\n","Epoch 00161: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 324ms/step - loss: 0.1473 - accuracy: 0.9481 - val_loss: 1.6107 - val_accuracy: 0.8011 - lr: 0.0025\n","Epoch 162/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1253 - accuracy: 0.9577\n","Epoch 00162: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 325ms/step - loss: 0.1253 - accuracy: 0.9577 - val_loss: 0.6704 - val_accuracy: 0.7659 - lr: 0.0021\n","Epoch 163/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1182 - accuracy: 0.9632\n","Epoch 00163: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 321ms/step - loss: 0.1182 - accuracy: 0.9632 - val_loss: 0.2583 - val_accuracy: 0.9087 - lr: 0.0018\n","Epoch 164/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.9574\n","Epoch 00164: val_loss did not improve from 0.12098\n","63/63 [==============================] - 23s 325ms/step - loss: 0.1200 - accuracy: 0.9574 - val_loss: 2.4630 - val_accuracy: 0.6384 - lr: 0.0015\n","Epoch 165/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1121 - accuracy: 0.9627\n","Epoch 00165: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 324ms/step - loss: 0.1121 - accuracy: 0.9627 - val_loss: 0.6704 - val_accuracy: 0.8462 - lr: 0.0013\n","Epoch 166/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9677\n","Epoch 00166: val_loss did not improve from 0.12098\n","63/63 [==============================] - 23s 330ms/step - loss: 0.0984 - accuracy: 0.9677 - val_loss: 0.3912 - val_accuracy: 0.9216 - lr: 0.0011\n","Epoch 167/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.9632\n","Epoch 00167: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0971 - accuracy: 0.9632 - val_loss: 0.3998 - val_accuracy: 0.9320 - lr: 9.2647e-04\n","Epoch 168/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.9614\n","Epoch 00168: val_loss did not improve from 0.12098\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0971 - accuracy: 0.9614 - val_loss: 2.0991 - val_accuracy: 0.8408 - lr: 7.8750e-04\n","Epoch 169/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0891 - accuracy: 0.9687\n","Epoch 00169: val_loss did not improve from 0.12098\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0891 - accuracy: 0.9687 - val_loss: 0.1758 - val_accuracy: 0.9563 - lr: 6.6937e-04\n","Epoch 170/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0797 - accuracy: 0.9734\n","Epoch 00170: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0797 - accuracy: 0.9734 - val_loss: 0.4357 - val_accuracy: 0.9355 - lr: 5.6897e-04\n","Epoch 171/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0778 - accuracy: 0.9712\n","Epoch 00171: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0778 - accuracy: 0.9712 - val_loss: 2.2015 - val_accuracy: 0.8323 - lr: 4.8362e-04\n","Epoch 172/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0806 - accuracy: 0.9704\n","Epoch 00172: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0806 - accuracy: 0.9704 - val_loss: 0.3025 - val_accuracy: 0.9077 - lr: 4.1108e-04\n","Epoch 173/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9775\n","Epoch 00173: val_loss did not improve from 0.12098\n","63/63 [==============================] - 22s 328ms/step - loss: 0.0675 - accuracy: 0.9775 - val_loss: 0.1330 - val_accuracy: 0.9499 - lr: 3.4942e-04\n","Epoch 174/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0692 - accuracy: 0.9760\n","Epoch 00174: val_loss improved from 0.12098 to 0.10281, saving model to /content/drive/MyDrive/output/JP30N02--1-best_result.hdf5\n","63/63 [==============================] - 25s 367ms/step - loss: 0.0692 - accuracy: 0.9760 - val_loss: 0.1028 - val_accuracy: 0.9588 - lr: 2.9700e-04\n","Epoch 175/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9752\n","Epoch 00175: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0700 - accuracy: 0.9752 - val_loss: 0.1047 - val_accuracy: 0.9658 - lr: 2.5245e-04\n","Epoch 176/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1005 - accuracy: 0.9704\n","Epoch 00176: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 324ms/step - loss: 0.1005 - accuracy: 0.9704 - val_loss: 0.7144 - val_accuracy: 0.8363 - lr: 0.0010\n","Epoch 177/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0969 - accuracy: 0.9717\n","Epoch 00177: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0969 - accuracy: 0.9717 - val_loss: 7.0639 - val_accuracy: 0.7937 - lr: 0.0015\n","Epoch 178/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1021 - accuracy: 0.9672\n","Epoch 00178: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 327ms/step - loss: 0.1021 - accuracy: 0.9672 - val_loss: 4.6460 - val_accuracy: 0.7202 - lr: 0.0020\n","Epoch 179/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1334 - accuracy: 0.9557\n","Epoch 00179: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 325ms/step - loss: 0.1334 - accuracy: 0.9557 - val_loss: 1.3301 - val_accuracy: 0.7480 - lr: 0.0025\n","Epoch 180/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1497 - accuracy: 0.9529\n","Epoch 00180: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 324ms/step - loss: 0.1497 - accuracy: 0.9529 - val_loss: 0.9346 - val_accuracy: 0.8061 - lr: 0.0030\n","Epoch 181/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1684 - accuracy: 0.9431\n","Epoch 00181: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 326ms/step - loss: 0.1684 - accuracy: 0.9431 - val_loss: 4.5827 - val_accuracy: 0.5030 - lr: 0.0035\n","Epoch 182/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1751 - accuracy: 0.9414\n","Epoch 00182: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 325ms/step - loss: 0.1751 - accuracy: 0.9414 - val_loss: 0.8205 - val_accuracy: 0.7902 - lr: 0.0040\n","Epoch 183/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1860 - accuracy: 0.9359\n","Epoch 00183: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 323ms/step - loss: 0.1860 - accuracy: 0.9359 - val_loss: 7.7237 - val_accuracy: 0.6840 - lr: 0.0040\n","Epoch 184/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1757 - accuracy: 0.9451\n","Epoch 00184: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 327ms/step - loss: 0.1757 - accuracy: 0.9451 - val_loss: 3.8047 - val_accuracy: 0.6096 - lr: 0.0034\n","Epoch 185/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1570 - accuracy: 0.9489\n","Epoch 00185: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 326ms/step - loss: 0.1570 - accuracy: 0.9489 - val_loss: 0.4487 - val_accuracy: 0.8646 - lr: 0.0029\n","Epoch 186/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1426 - accuracy: 0.9549\n","Epoch 00186: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 328ms/step - loss: 0.1426 - accuracy: 0.9549 - val_loss: 0.4890 - val_accuracy: 0.8686 - lr: 0.0025\n","Epoch 187/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1143 - accuracy: 0.9604\n","Epoch 00187: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 342ms/step - loss: 0.1143 - accuracy: 0.9604 - val_loss: 2.4804 - val_accuracy: 0.7123 - lr: 0.0021\n","Epoch 188/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1194 - accuracy: 0.9599\n","Epoch 00188: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 328ms/step - loss: 0.1194 - accuracy: 0.9599 - val_loss: 3.1674 - val_accuracy: 0.6687 - lr: 0.0018\n","Epoch 189/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1033 - accuracy: 0.9639\n","Epoch 00189: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 323ms/step - loss: 0.1033 - accuracy: 0.9639 - val_loss: 0.5476 - val_accuracy: 0.8219 - lr: 0.0015\n","Epoch 190/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0971 - accuracy: 0.9669\n","Epoch 00190: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0971 - accuracy: 0.9669 - val_loss: 0.5629 - val_accuracy: 0.8527 - lr: 0.0013\n","Epoch 191/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0841 - accuracy: 0.9717\n","Epoch 00191: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0841 - accuracy: 0.9717 - val_loss: 0.8110 - val_accuracy: 0.8194 - lr: 0.0011\n","Epoch 192/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0896 - accuracy: 0.9692\n","Epoch 00192: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0896 - accuracy: 0.9692 - val_loss: 0.1496 - val_accuracy: 0.9529 - lr: 9.2647e-04\n","Epoch 193/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0912 - accuracy: 0.9742\n","Epoch 00193: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0912 - accuracy: 0.9742 - val_loss: 0.2058 - val_accuracy: 0.9345 - lr: 7.8750e-04\n","Epoch 194/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9734\n","Epoch 00194: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0725 - accuracy: 0.9734 - val_loss: 0.2260 - val_accuracy: 0.9246 - lr: 6.6937e-04\n","Epoch 195/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0880 - accuracy: 0.9697\n","Epoch 00195: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 329ms/step - loss: 0.0880 - accuracy: 0.9697 - val_loss: 0.4306 - val_accuracy: 0.8472 - lr: 5.6897e-04\n","Epoch 196/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9770\n","Epoch 00196: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0714 - accuracy: 0.9770 - val_loss: 0.1684 - val_accuracy: 0.9449 - lr: 4.8362e-04\n","Epoch 197/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0652 - accuracy: 0.9752\n","Epoch 00197: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0652 - accuracy: 0.9752 - val_loss: 0.1996 - val_accuracy: 0.9221 - lr: 4.1108e-04\n","Epoch 198/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9734\n","Epoch 00198: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0715 - accuracy: 0.9734 - val_loss: 0.1275 - val_accuracy: 0.9504 - lr: 3.4942e-04\n","Epoch 199/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0752 - accuracy: 0.9752\n","Epoch 00199: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0752 - accuracy: 0.9752 - val_loss: 0.1876 - val_accuracy: 0.9335 - lr: 2.9700e-04\n","Epoch 200/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0642 - accuracy: 0.9792\n","Epoch 00200: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0642 - accuracy: 0.9792 - val_loss: 0.1222 - val_accuracy: 0.9623 - lr: 2.5245e-04\n","Epoch 201/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0751 - accuracy: 0.9755\n","Epoch 00201: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0751 - accuracy: 0.9755 - val_loss: 0.4783 - val_accuracy: 0.8934 - lr: 0.0010\n","Epoch 202/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0917 - accuracy: 0.9694\n","Epoch 00202: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0917 - accuracy: 0.9694 - val_loss: 29.3990 - val_accuracy: 0.6548 - lr: 0.0015\n","Epoch 203/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1026 - accuracy: 0.9689\n","Epoch 00203: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 326ms/step - loss: 0.1026 - accuracy: 0.9689 - val_loss: 0.5070 - val_accuracy: 0.8894 - lr: 0.0020\n","Epoch 204/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1047 - accuracy: 0.9672\n","Epoch 00204: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 327ms/step - loss: 0.1047 - accuracy: 0.9672 - val_loss: 5.4485 - val_accuracy: 0.7763 - lr: 0.0025\n","Epoch 205/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1414 - accuracy: 0.9577\n","Epoch 00205: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 325ms/step - loss: 0.1414 - accuracy: 0.9577 - val_loss: 11.7674 - val_accuracy: 0.6637 - lr: 0.0030\n","Epoch 206/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1452 - accuracy: 0.9494\n","Epoch 00206: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 324ms/step - loss: 0.1452 - accuracy: 0.9494 - val_loss: 2.1633 - val_accuracy: 0.6820 - lr: 0.0035\n","Epoch 207/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1689 - accuracy: 0.9426\n","Epoch 00207: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 326ms/step - loss: 0.1689 - accuracy: 0.9426 - val_loss: 5.9175 - val_accuracy: 0.7877 - lr: 0.0040\n","Epoch 208/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1661 - accuracy: 0.9426\n","Epoch 00208: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 325ms/step - loss: 0.1661 - accuracy: 0.9426 - val_loss: 6.3345 - val_accuracy: 0.6935 - lr: 0.0040\n","Epoch 209/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1522 - accuracy: 0.9476\n","Epoch 00209: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 327ms/step - loss: 0.1522 - accuracy: 0.9476 - val_loss: 0.5043 - val_accuracy: 0.8596 - lr: 0.0034\n","Epoch 210/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1551 - accuracy: 0.9552\n","Epoch 00210: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 326ms/step - loss: 0.1551 - accuracy: 0.9552 - val_loss: 2.3249 - val_accuracy: 0.6414 - lr: 0.0029\n","Epoch 211/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1251 - accuracy: 0.9612\n","Epoch 00211: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 324ms/step - loss: 0.1251 - accuracy: 0.9612 - val_loss: 2.7952 - val_accuracy: 0.7808 - lr: 0.0025\n","Epoch 212/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1179 - accuracy: 0.9619\n","Epoch 00212: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 326ms/step - loss: 0.1179 - accuracy: 0.9619 - val_loss: 3.1348 - val_accuracy: 0.7163 - lr: 0.0021\n","Epoch 213/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1169 - accuracy: 0.9627\n","Epoch 00213: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 326ms/step - loss: 0.1169 - accuracy: 0.9627 - val_loss: 4.2152 - val_accuracy: 0.6939 - lr: 0.0018\n","Epoch 214/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1013 - accuracy: 0.9667\n","Epoch 00214: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 328ms/step - loss: 0.1013 - accuracy: 0.9667 - val_loss: 1.3832 - val_accuracy: 0.8239 - lr: 0.0015\n","Epoch 215/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9732\n","Epoch 00215: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0819 - accuracy: 0.9732 - val_loss: 0.3370 - val_accuracy: 0.9018 - lr: 0.0013\n","Epoch 216/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0889 - accuracy: 0.9722\n","Epoch 00216: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0889 - accuracy: 0.9722 - val_loss: 0.5125 - val_accuracy: 0.8730 - lr: 0.0011\n","Epoch 217/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0808 - accuracy: 0.9719\n","Epoch 00217: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0808 - accuracy: 0.9719 - val_loss: 3.4356 - val_accuracy: 0.8016 - lr: 9.2647e-04\n","Epoch 218/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9757\n","Epoch 00218: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0775 - accuracy: 0.9757 - val_loss: 0.3766 - val_accuracy: 0.8547 - lr: 7.8750e-04\n","Epoch 219/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9782\n","Epoch 00219: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0728 - accuracy: 0.9782 - val_loss: 3.1410 - val_accuracy: 0.8100 - lr: 6.6937e-04\n","Epoch 220/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0602 - accuracy: 0.9825\n","Epoch 00220: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0602 - accuracy: 0.9825 - val_loss: 0.1799 - val_accuracy: 0.9405 - lr: 5.6897e-04\n","Epoch 221/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9795\n","Epoch 00221: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0611 - accuracy: 0.9795 - val_loss: 0.1362 - val_accuracy: 0.9568 - lr: 4.8362e-04\n","Epoch 222/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9802\n","Epoch 00222: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0650 - accuracy: 0.9802 - val_loss: 1.2250 - val_accuracy: 0.8730 - lr: 4.1108e-04\n","Epoch 223/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0562 - accuracy: 0.9810\n","Epoch 00223: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0562 - accuracy: 0.9810 - val_loss: 1.2015 - val_accuracy: 0.8934 - lr: 3.4942e-04\n","Epoch 224/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9835\n","Epoch 00224: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0604 - accuracy: 0.9835 - val_loss: 0.1115 - val_accuracy: 0.9683 - lr: 2.9700e-04\n","Epoch 225/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0532 - accuracy: 0.9822\n","Epoch 00225: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0532 - accuracy: 0.9822 - val_loss: 0.1094 - val_accuracy: 0.9638 - lr: 2.5245e-04\n","Epoch 226/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0766 - accuracy: 0.9734\n","Epoch 00226: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0766 - accuracy: 0.9734 - val_loss: 0.2016 - val_accuracy: 0.9583 - lr: 0.0010\n","Epoch 227/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9729\n","Epoch 00227: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0768 - accuracy: 0.9729 - val_loss: 3.5492 - val_accuracy: 0.7346 - lr: 0.0015\n","Epoch 228/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0985 - accuracy: 0.9692\n","Epoch 00228: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0985 - accuracy: 0.9692 - val_loss: 0.5930 - val_accuracy: 0.9067 - lr: 0.0020\n","Epoch 229/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1098 - accuracy: 0.9652\n","Epoch 00229: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 327ms/step - loss: 0.1098 - accuracy: 0.9652 - val_loss: 15.2661 - val_accuracy: 0.7252 - lr: 0.0025\n","Epoch 230/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1388 - accuracy: 0.9572\n","Epoch 00230: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 328ms/step - loss: 0.1388 - accuracy: 0.9572 - val_loss: 1.0285 - val_accuracy: 0.7703 - lr: 0.0030\n","Epoch 231/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1468 - accuracy: 0.9496\n","Epoch 00231: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 326ms/step - loss: 0.1468 - accuracy: 0.9496 - val_loss: 7.8008 - val_accuracy: 0.6171 - lr: 0.0035\n","Epoch 232/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1530 - accuracy: 0.9474\n","Epoch 00232: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 324ms/step - loss: 0.1530 - accuracy: 0.9474 - val_loss: 110.1206 - val_accuracy: 0.5556 - lr: 0.0040\n","Epoch 233/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1706 - accuracy: 0.9454\n","Epoch 00233: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 326ms/step - loss: 0.1706 - accuracy: 0.9454 - val_loss: 6.1548 - val_accuracy: 0.7143 - lr: 0.0040\n","Epoch 234/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1432 - accuracy: 0.9537\n","Epoch 00234: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 326ms/step - loss: 0.1432 - accuracy: 0.9537 - val_loss: 0.8336 - val_accuracy: 0.8145 - lr: 0.0034\n","Epoch 235/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1459 - accuracy: 0.9524\n","Epoch 00235: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 328ms/step - loss: 0.1459 - accuracy: 0.9524 - val_loss: 3.1464 - val_accuracy: 0.5893 - lr: 0.0029\n","Epoch 236/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1187 - accuracy: 0.9642\n","Epoch 00236: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 328ms/step - loss: 0.1187 - accuracy: 0.9642 - val_loss: 1.6131 - val_accuracy: 0.7426 - lr: 0.0025\n","Epoch 237/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1142 - accuracy: 0.9612\n","Epoch 00237: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 323ms/step - loss: 0.1142 - accuracy: 0.9612 - val_loss: 0.7343 - val_accuracy: 0.8686 - lr: 0.0021\n","Epoch 238/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0983 - accuracy: 0.9674\n","Epoch 00238: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0983 - accuracy: 0.9674 - val_loss: 6.5798 - val_accuracy: 0.8006 - lr: 0.0018\n","Epoch 239/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0970 - accuracy: 0.9664\n","Epoch 00239: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0970 - accuracy: 0.9664 - val_loss: 0.9750 - val_accuracy: 0.8338 - lr: 0.0015\n","Epoch 240/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9747\n","Epoch 00240: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 329ms/step - loss: 0.0783 - accuracy: 0.9747 - val_loss: 0.2405 - val_accuracy: 0.9261 - lr: 0.0013\n","Epoch 241/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0782 - accuracy: 0.9737\n","Epoch 00241: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0782 - accuracy: 0.9737 - val_loss: 0.2621 - val_accuracy: 0.9246 - lr: 0.0011\n","Epoch 242/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9752\n","Epoch 00242: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0713 - accuracy: 0.9752 - val_loss: 0.1704 - val_accuracy: 0.9519 - lr: 9.2647e-04\n","Epoch 243/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0650 - accuracy: 0.9767\n","Epoch 00243: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0650 - accuracy: 0.9767 - val_loss: 0.9286 - val_accuracy: 0.8209 - lr: 7.8750e-04\n","Epoch 244/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0649 - accuracy: 0.9787\n","Epoch 00244: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0649 - accuracy: 0.9787 - val_loss: 0.3141 - val_accuracy: 0.9023 - lr: 6.6937e-04\n","Epoch 245/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0653 - accuracy: 0.9790\n","Epoch 00245: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0653 - accuracy: 0.9790 - val_loss: 0.2162 - val_accuracy: 0.9390 - lr: 5.6897e-04\n","Epoch 246/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0594 - accuracy: 0.9817\n","Epoch 00246: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0594 - accuracy: 0.9817 - val_loss: 0.1576 - val_accuracy: 0.9568 - lr: 4.8362e-04\n","Epoch 247/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0518 - accuracy: 0.9840\n","Epoch 00247: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0518 - accuracy: 0.9840 - val_loss: 0.2768 - val_accuracy: 0.9400 - lr: 4.1108e-04\n","Epoch 248/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0548 - accuracy: 0.9807\n","Epoch 00248: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0548 - accuracy: 0.9807 - val_loss: 0.3582 - val_accuracy: 0.9082 - lr: 3.4942e-04\n","Epoch 249/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0504 - accuracy: 0.9822\n","Epoch 00249: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0504 - accuracy: 0.9822 - val_loss: 0.1537 - val_accuracy: 0.9583 - lr: 2.9700e-04\n","Epoch 250/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.9830\n","Epoch 00250: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0492 - accuracy: 0.9830 - val_loss: 0.1226 - val_accuracy: 0.9692 - lr: 2.5245e-04\n","Epoch 251/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0654 - accuracy: 0.9792\n","Epoch 00251: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0654 - accuracy: 0.9792 - val_loss: 1.2733 - val_accuracy: 0.9048 - lr: 0.0010\n","Epoch 252/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0675 - accuracy: 0.9787\n","Epoch 00252: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 330ms/step - loss: 0.0675 - accuracy: 0.9787 - val_loss: 76.8088 - val_accuracy: 0.5977 - lr: 0.0015\n","Epoch 253/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9755\n","Epoch 00253: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0768 - accuracy: 0.9755 - val_loss: 0.6371 - val_accuracy: 0.8447 - lr: 0.0020\n","Epoch 254/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1021 - accuracy: 0.9674\n","Epoch 00254: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 327ms/step - loss: 0.1021 - accuracy: 0.9674 - val_loss: 70.2139 - val_accuracy: 0.5883 - lr: 0.0025\n","Epoch 255/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1080 - accuracy: 0.9664\n","Epoch 00255: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 327ms/step - loss: 0.1080 - accuracy: 0.9664 - val_loss: 2.2864 - val_accuracy: 0.7951 - lr: 0.0030\n","Epoch 256/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1317 - accuracy: 0.9559\n","Epoch 00256: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 328ms/step - loss: 0.1317 - accuracy: 0.9559 - val_loss: 2.3111 - val_accuracy: 0.8100 - lr: 0.0035\n","Epoch 257/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1391 - accuracy: 0.9557\n","Epoch 00257: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 328ms/step - loss: 0.1391 - accuracy: 0.9557 - val_loss: 1.5851 - val_accuracy: 0.7197 - lr: 0.0040\n","Epoch 258/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1747 - accuracy: 0.9446\n","Epoch 00258: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 326ms/step - loss: 0.1747 - accuracy: 0.9446 - val_loss: 0.4301 - val_accuracy: 0.8353 - lr: 0.0040\n","Epoch 259/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1451 - accuracy: 0.9539\n","Epoch 00259: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 326ms/step - loss: 0.1451 - accuracy: 0.9539 - val_loss: 0.4844 - val_accuracy: 0.8442 - lr: 0.0034\n","Epoch 260/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1156 - accuracy: 0.9574\n","Epoch 00260: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 328ms/step - loss: 0.1156 - accuracy: 0.9574 - val_loss: 0.3559 - val_accuracy: 0.8978 - lr: 0.0029\n","Epoch 261/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1110 - accuracy: 0.9602\n","Epoch 00261: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 325ms/step - loss: 0.1110 - accuracy: 0.9602 - val_loss: 0.3643 - val_accuracy: 0.8934 - lr: 0.0025\n","Epoch 262/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0930 - accuracy: 0.9694\n","Epoch 00262: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0930 - accuracy: 0.9694 - val_loss: 0.2687 - val_accuracy: 0.9509 - lr: 0.0021\n","Epoch 263/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0863 - accuracy: 0.9712\n","Epoch 00263: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0863 - accuracy: 0.9712 - val_loss: 0.2338 - val_accuracy: 0.9375 - lr: 0.0018\n","Epoch 264/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0849 - accuracy: 0.9717\n","Epoch 00264: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0849 - accuracy: 0.9717 - val_loss: 0.2557 - val_accuracy: 0.9122 - lr: 0.0015\n","Epoch 265/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0648 - accuracy: 0.9805\n","Epoch 00265: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0648 - accuracy: 0.9805 - val_loss: 0.5256 - val_accuracy: 0.8819 - lr: 0.0013\n","Epoch 266/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0728 - accuracy: 0.9762\n","Epoch 00266: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0728 - accuracy: 0.9762 - val_loss: 1.5108 - val_accuracy: 0.8428 - lr: 0.0011\n","Epoch 267/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9815\n","Epoch 00267: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0673 - accuracy: 0.9815 - val_loss: 0.9618 - val_accuracy: 0.8810 - lr: 9.2647e-04\n","Epoch 268/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0775 - accuracy: 0.9760\n","Epoch 00268: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0775 - accuracy: 0.9760 - val_loss: 0.9371 - val_accuracy: 0.8824 - lr: 7.8750e-04\n","Epoch 269/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0613 - accuracy: 0.9825\n","Epoch 00269: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0613 - accuracy: 0.9825 - val_loss: 1.0082 - val_accuracy: 0.8700 - lr: 6.6937e-04\n","Epoch 270/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0613 - accuracy: 0.9812\n","Epoch 00270: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0613 - accuracy: 0.9812 - val_loss: 0.2132 - val_accuracy: 0.9415 - lr: 5.6897e-04\n","Epoch 271/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0556 - accuracy: 0.9810\n","Epoch 00271: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0556 - accuracy: 0.9810 - val_loss: 0.4901 - val_accuracy: 0.8681 - lr: 4.8362e-04\n","Epoch 272/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0460 - accuracy: 0.9842\n","Epoch 00272: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 331ms/step - loss: 0.0460 - accuracy: 0.9842 - val_loss: 0.9801 - val_accuracy: 0.8953 - lr: 4.1108e-04\n","Epoch 273/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0544 - accuracy: 0.9842\n","Epoch 00273: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0544 - accuracy: 0.9842 - val_loss: 0.2233 - val_accuracy: 0.9325 - lr: 3.4942e-04\n","Epoch 274/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9850\n","Epoch 00274: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0457 - accuracy: 0.9850 - val_loss: 0.5909 - val_accuracy: 0.9231 - lr: 2.9700e-04\n","Epoch 275/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0540 - accuracy: 0.9822\n","Epoch 00275: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0540 - accuracy: 0.9822 - val_loss: 0.4418 - val_accuracy: 0.9226 - lr: 2.5245e-04\n","Epoch 276/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0552 - accuracy: 0.9817\n","Epoch 00276: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0552 - accuracy: 0.9817 - val_loss: 4.2956 - val_accuracy: 0.8194 - lr: 0.0010\n","Epoch 277/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0737 - accuracy: 0.9747\n","Epoch 00277: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0737 - accuracy: 0.9747 - val_loss: 2.6737 - val_accuracy: 0.7401 - lr: 0.0015\n","Epoch 278/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9755\n","Epoch 00278: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0750 - accuracy: 0.9755 - val_loss: 8.6461 - val_accuracy: 0.6885 - lr: 0.0020\n","Epoch 279/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0925 - accuracy: 0.9707\n","Epoch 00279: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0925 - accuracy: 0.9707 - val_loss: 5.5148 - val_accuracy: 0.4960 - lr: 0.0025\n","Epoch 280/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1103 - accuracy: 0.9609\n","Epoch 00280: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 328ms/step - loss: 0.1103 - accuracy: 0.9609 - val_loss: 0.2752 - val_accuracy: 0.9226 - lr: 0.0030\n","Epoch 281/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1147 - accuracy: 0.9627\n","Epoch 00281: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 330ms/step - loss: 0.1147 - accuracy: 0.9627 - val_loss: 2.4289 - val_accuracy: 0.6314 - lr: 0.0035\n","Epoch 282/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1799 - accuracy: 0.9499\n","Epoch 00282: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 325ms/step - loss: 0.1799 - accuracy: 0.9499 - val_loss: 6.4860 - val_accuracy: 0.6448 - lr: 0.0040\n","Epoch 283/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1431 - accuracy: 0.9552\n","Epoch 00283: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 325ms/step - loss: 0.1431 - accuracy: 0.9552 - val_loss: 0.8416 - val_accuracy: 0.7540 - lr: 0.0040\n","Epoch 284/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1461 - accuracy: 0.9522\n","Epoch 00284: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 323ms/step - loss: 0.1461 - accuracy: 0.9522 - val_loss: 2.4034 - val_accuracy: 0.7609 - lr: 0.0034\n","Epoch 285/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1070 - accuracy: 0.9677\n","Epoch 00285: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 327ms/step - loss: 0.1070 - accuracy: 0.9677 - val_loss: 2.9179 - val_accuracy: 0.6627 - lr: 0.0029\n","Epoch 286/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1049 - accuracy: 0.9652\n","Epoch 00286: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 327ms/step - loss: 0.1049 - accuracy: 0.9652 - val_loss: 1.5641 - val_accuracy: 0.7624 - lr: 0.0025\n","Epoch 287/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0922 - accuracy: 0.9694\n","Epoch 00287: val_loss did not improve from 0.10281\n","63/63 [==============================] - 23s 329ms/step - loss: 0.0922 - accuracy: 0.9694 - val_loss: 2.1424 - val_accuracy: 0.8130 - lr: 0.0021\n","Epoch 288/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9755\n","Epoch 00288: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0762 - accuracy: 0.9755 - val_loss: 0.1402 - val_accuracy: 0.9544 - lr: 0.0018\n","Epoch 289/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0833 - accuracy: 0.9734\n","Epoch 00289: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0833 - accuracy: 0.9734 - val_loss: 4.3951 - val_accuracy: 0.7564 - lr: 0.0015\n","Epoch 290/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9777\n","Epoch 00290: val_loss did not improve from 0.10281\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0664 - accuracy: 0.9777 - val_loss: 1.8350 - val_accuracy: 0.8467 - lr: 0.0013\n","Epoch 291/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0604 - accuracy: 0.9800\n","Epoch 00291: val_loss improved from 0.10281 to 0.09814, saving model to /content/drive/MyDrive/output/JP30N02--1-best_result.hdf5\n","63/63 [==============================] - 25s 372ms/step - loss: 0.0604 - accuracy: 0.9800 - val_loss: 0.0981 - val_accuracy: 0.9712 - lr: 0.0011\n","Epoch 292/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0605 - accuracy: 0.9792\n","Epoch 00292: val_loss did not improve from 0.09814\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0605 - accuracy: 0.9792 - val_loss: 1.6400 - val_accuracy: 0.8264 - lr: 9.2647e-04\n","Epoch 293/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0601 - accuracy: 0.9800\n","Epoch 00293: val_loss did not improve from 0.09814\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0601 - accuracy: 0.9800 - val_loss: 0.4717 - val_accuracy: 0.8745 - lr: 7.8750e-04\n","Epoch 294/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0561 - accuracy: 0.9822\n","Epoch 00294: val_loss did not improve from 0.09814\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0561 - accuracy: 0.9822 - val_loss: 0.3261 - val_accuracy: 0.9117 - lr: 6.6937e-04\n","Epoch 295/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0501 - accuracy: 0.9810\n","Epoch 00295: val_loss did not improve from 0.09814\n","63/63 [==============================] - 23s 329ms/step - loss: 0.0501 - accuracy: 0.9810 - val_loss: 0.1779 - val_accuracy: 0.9549 - lr: 5.6897e-04\n","Epoch 296/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0457 - accuracy: 0.9867\n","Epoch 00296: val_loss did not improve from 0.09814\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0457 - accuracy: 0.9867 - val_loss: 0.5774 - val_accuracy: 0.8606 - lr: 4.8362e-04\n","Epoch 297/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0493 - accuracy: 0.9825\n","Epoch 00297: val_loss did not improve from 0.09814\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0493 - accuracy: 0.9825 - val_loss: 0.2105 - val_accuracy: 0.9459 - lr: 4.1108e-04\n","Epoch 298/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0464 - accuracy: 0.9852\n","Epoch 00298: val_loss improved from 0.09814 to 0.09628, saving model to /content/drive/MyDrive/output/JP30N02--1-best_result.hdf5\n","63/63 [==============================] - 25s 371ms/step - loss: 0.0464 - accuracy: 0.9852 - val_loss: 0.0963 - val_accuracy: 0.9722 - lr: 3.4942e-04\n","Epoch 299/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9850\n","Epoch 00299: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0456 - accuracy: 0.9850 - val_loss: 0.1373 - val_accuracy: 0.9633 - lr: 2.9700e-04\n","Epoch 300/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0437 - accuracy: 0.9860\n","Epoch 00300: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0437 - accuracy: 0.9860 - val_loss: 0.3724 - val_accuracy: 0.9231 - lr: 2.5245e-04\n","Epoch 301/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9837\n","Epoch 00301: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0528 - accuracy: 0.9837 - val_loss: 0.3594 - val_accuracy: 0.9082 - lr: 0.0010\n","Epoch 302/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0589 - accuracy: 0.9800\n","Epoch 00302: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 331ms/step - loss: 0.0589 - accuracy: 0.9800 - val_loss: 0.5296 - val_accuracy: 0.8447 - lr: 0.0015\n","Epoch 303/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9757\n","Epoch 00303: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0706 - accuracy: 0.9757 - val_loss: 5.1187 - val_accuracy: 0.7619 - lr: 0.0020\n","Epoch 304/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1021 - accuracy: 0.9702\n","Epoch 00304: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 324ms/step - loss: 0.1021 - accuracy: 0.9702 - val_loss: 1.1814 - val_accuracy: 0.7192 - lr: 0.0025\n","Epoch 305/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0943 - accuracy: 0.9709\n","Epoch 00305: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0943 - accuracy: 0.9709 - val_loss: 2.0084 - val_accuracy: 0.6161 - lr: 0.0030\n","Epoch 306/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1242 - accuracy: 0.9619\n","Epoch 00306: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.1242 - accuracy: 0.9619 - val_loss: 33.9234 - val_accuracy: 0.5223 - lr: 0.0035\n","Epoch 307/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1280 - accuracy: 0.9547\n","Epoch 00307: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 327ms/step - loss: 0.1280 - accuracy: 0.9547 - val_loss: 2.4956 - val_accuracy: 0.7644 - lr: 0.0040\n","Epoch 308/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1241 - accuracy: 0.9617\n","Epoch 00308: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 330ms/step - loss: 0.1241 - accuracy: 0.9617 - val_loss: 4.2560 - val_accuracy: 0.6235 - lr: 0.0040\n","Epoch 309/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1126 - accuracy: 0.9629\n","Epoch 00309: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 328ms/step - loss: 0.1126 - accuracy: 0.9629 - val_loss: 4.5257 - val_accuracy: 0.7619 - lr: 0.0034\n","Epoch 310/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1051 - accuracy: 0.9629\n","Epoch 00310: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.1051 - accuracy: 0.9629 - val_loss: 0.9503 - val_accuracy: 0.8075 - lr: 0.0029\n","Epoch 311/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1141 - accuracy: 0.9657\n","Epoch 00311: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.1141 - accuracy: 0.9657 - val_loss: 4.2326 - val_accuracy: 0.7882 - lr: 0.0025\n","Epoch 312/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0849 - accuracy: 0.9692\n","Epoch 00312: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0849 - accuracy: 0.9692 - val_loss: 0.7849 - val_accuracy: 0.8229 - lr: 0.0021\n","Epoch 313/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9770\n","Epoch 00313: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0696 - accuracy: 0.9770 - val_loss: 0.2376 - val_accuracy: 0.9385 - lr: 0.0018\n","Epoch 314/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9767\n","Epoch 00314: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0713 - accuracy: 0.9767 - val_loss: 0.3181 - val_accuracy: 0.8978 - lr: 0.0015\n","Epoch 315/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0587 - accuracy: 0.9797\n","Epoch 00315: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0587 - accuracy: 0.9797 - val_loss: 11.1842 - val_accuracy: 0.5933 - lr: 0.0013\n","Epoch 316/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 0.9835\n","Epoch 00316: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0473 - accuracy: 0.9835 - val_loss: 1.2082 - val_accuracy: 0.7192 - lr: 0.0011\n","Epoch 317/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9835\n","Epoch 00317: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0468 - accuracy: 0.9835 - val_loss: 0.4991 - val_accuracy: 0.8790 - lr: 9.2647e-04\n","Epoch 318/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0488 - accuracy: 0.9837\n","Epoch 00318: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0488 - accuracy: 0.9837 - val_loss: 0.9419 - val_accuracy: 0.8537 - lr: 7.8750e-04\n","Epoch 319/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9842\n","Epoch 00319: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0413 - accuracy: 0.9842 - val_loss: 0.5540 - val_accuracy: 0.8532 - lr: 6.6937e-04\n","Epoch 320/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0513 - accuracy: 0.9810\n","Epoch 00320: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0513 - accuracy: 0.9810 - val_loss: 0.1688 - val_accuracy: 0.9603 - lr: 5.6897e-04\n","Epoch 321/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9860\n","Epoch 00321: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 340ms/step - loss: 0.0426 - accuracy: 0.9860 - val_loss: 0.4379 - val_accuracy: 0.9444 - lr: 4.8362e-04\n","Epoch 322/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9865\n","Epoch 00322: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0353 - accuracy: 0.9865 - val_loss: 0.2401 - val_accuracy: 0.9663 - lr: 4.1108e-04\n","Epoch 323/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9850\n","Epoch 00323: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0487 - accuracy: 0.9850 - val_loss: 0.1390 - val_accuracy: 0.9722 - lr: 3.4942e-04\n","Epoch 324/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9882\n","Epoch 00324: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0342 - accuracy: 0.9882 - val_loss: 0.1271 - val_accuracy: 0.9782 - lr: 2.9700e-04\n","Epoch 325/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0378 - accuracy: 0.9855\n","Epoch 00325: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0378 - accuracy: 0.9855 - val_loss: 0.2059 - val_accuracy: 0.9554 - lr: 2.5245e-04\n","Epoch 326/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9862\n","Epoch 00326: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0405 - accuracy: 0.9862 - val_loss: 2.0302 - val_accuracy: 0.8090 - lr: 0.0010\n","Epoch 327/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0610 - accuracy: 0.9800\n","Epoch 00327: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 329ms/step - loss: 0.0610 - accuracy: 0.9800 - val_loss: 0.9507 - val_accuracy: 0.7738 - lr: 0.0015\n","Epoch 328/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0789 - accuracy: 0.9752\n","Epoch 00328: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0789 - accuracy: 0.9752 - val_loss: 1.1079 - val_accuracy: 0.8070 - lr: 0.0020\n","Epoch 329/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9757\n","Epoch 00329: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0712 - accuracy: 0.9757 - val_loss: 3.7692 - val_accuracy: 0.7649 - lr: 0.0025\n","Epoch 330/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1081 - accuracy: 0.9682\n","Epoch 00330: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 327ms/step - loss: 0.1081 - accuracy: 0.9682 - val_loss: 2.4397 - val_accuracy: 0.6627 - lr: 0.0030\n","Epoch 331/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1299 - accuracy: 0.9629\n","Epoch 00331: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 329ms/step - loss: 0.1299 - accuracy: 0.9629 - val_loss: 11.7376 - val_accuracy: 0.5407 - lr: 0.0035\n","Epoch 332/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1193 - accuracy: 0.9599\n","Epoch 00332: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.1193 - accuracy: 0.9599 - val_loss: 1.8802 - val_accuracy: 0.7956 - lr: 0.0040\n","Epoch 333/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.9609\n","Epoch 00333: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 329ms/step - loss: 0.1211 - accuracy: 0.9609 - val_loss: 40.3530 - val_accuracy: 0.4003 - lr: 0.0040\n","Epoch 334/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1089 - accuracy: 0.9622\n","Epoch 00334: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 328ms/step - loss: 0.1089 - accuracy: 0.9622 - val_loss: 14.5236 - val_accuracy: 0.6369 - lr: 0.0034\n","Epoch 335/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0966 - accuracy: 0.9692\n","Epoch 00335: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0966 - accuracy: 0.9692 - val_loss: 1.9323 - val_accuracy: 0.6572 - lr: 0.0029\n","Epoch 336/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0921 - accuracy: 0.9707\n","Epoch 00336: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0921 - accuracy: 0.9707 - val_loss: 0.9025 - val_accuracy: 0.7922 - lr: 0.0025\n","Epoch 337/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9795\n","Epoch 00337: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0655 - accuracy: 0.9795 - val_loss: 3.6643 - val_accuracy: 0.6553 - lr: 0.0021\n","Epoch 338/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0722 - accuracy: 0.9757\n","Epoch 00338: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 329ms/step - loss: 0.0722 - accuracy: 0.9757 - val_loss: 0.7326 - val_accuracy: 0.8775 - lr: 0.0018\n","Epoch 339/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9785\n","Epoch 00339: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0618 - accuracy: 0.9785 - val_loss: 1.4073 - val_accuracy: 0.7217 - lr: 0.0015\n","Epoch 340/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0643 - accuracy: 0.9777\n","Epoch 00340: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0643 - accuracy: 0.9777 - val_loss: 1.0644 - val_accuracy: 0.7971 - lr: 0.0013\n","Epoch 341/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0630 - accuracy: 0.9770\n","Epoch 00341: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0630 - accuracy: 0.9770 - val_loss: 1.4960 - val_accuracy: 0.7956 - lr: 0.0011\n","Epoch 342/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9797\n","Epoch 00342: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 329ms/step - loss: 0.0611 - accuracy: 0.9797 - val_loss: 1.6056 - val_accuracy: 0.8353 - lr: 9.2647e-04\n","Epoch 343/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9837\n","Epoch 00343: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0535 - accuracy: 0.9837 - val_loss: 0.6054 - val_accuracy: 0.9162 - lr: 7.8750e-04\n","Epoch 344/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0444 - accuracy: 0.9887\n","Epoch 00344: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0444 - accuracy: 0.9887 - val_loss: 0.5585 - val_accuracy: 0.9157 - lr: 6.6937e-04\n","Epoch 345/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9862\n","Epoch 00345: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0500 - accuracy: 0.9862 - val_loss: 0.2407 - val_accuracy: 0.9554 - lr: 5.6897e-04\n","Epoch 346/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9862\n","Epoch 00346: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0435 - accuracy: 0.9862 - val_loss: 0.1675 - val_accuracy: 0.9643 - lr: 4.8362e-04\n","Epoch 347/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.9867\n","Epoch 00347: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0449 - accuracy: 0.9867 - val_loss: 0.1699 - val_accuracy: 0.9673 - lr: 4.1108e-04\n","Epoch 348/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9832\n","Epoch 00348: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0435 - accuracy: 0.9832 - val_loss: 0.2219 - val_accuracy: 0.9430 - lr: 3.4942e-04\n","Epoch 349/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9885\n","Epoch 00349: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0359 - accuracy: 0.9885 - val_loss: 0.1340 - val_accuracy: 0.9712 - lr: 2.9700e-04\n","Epoch 350/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9867\n","Epoch 00350: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0367 - accuracy: 0.9867 - val_loss: 0.1357 - val_accuracy: 0.9668 - lr: 2.5245e-04\n","Epoch 351/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.9850\n","Epoch 00351: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0492 - accuracy: 0.9850 - val_loss: 0.2154 - val_accuracy: 0.9365 - lr: 0.0010\n","Epoch 352/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0510 - accuracy: 0.9825\n","Epoch 00352: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 328ms/step - loss: 0.0510 - accuracy: 0.9825 - val_loss: 1.2649 - val_accuracy: 0.6964 - lr: 0.0015\n","Epoch 353/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0701 - accuracy: 0.9782\n","Epoch 00353: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0701 - accuracy: 0.9782 - val_loss: 0.3759 - val_accuracy: 0.9246 - lr: 0.0020\n","Epoch 354/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9737\n","Epoch 00354: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0768 - accuracy: 0.9737 - val_loss: 58.7519 - val_accuracy: 0.4826 - lr: 0.0025\n","Epoch 355/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0953 - accuracy: 0.9704\n","Epoch 00355: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0953 - accuracy: 0.9704 - val_loss: 0.4476 - val_accuracy: 0.8690 - lr: 0.0030\n","Epoch 356/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1079 - accuracy: 0.9677\n","Epoch 00356: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 339ms/step - loss: 0.1079 - accuracy: 0.9677 - val_loss: 3.1353 - val_accuracy: 0.8294 - lr: 0.0035\n","Epoch 357/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1390 - accuracy: 0.9607\n","Epoch 00357: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 324ms/step - loss: 0.1390 - accuracy: 0.9607 - val_loss: 7.7404 - val_accuracy: 0.6151 - lr: 0.0040\n","Epoch 358/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1181 - accuracy: 0.9609\n","Epoch 00358: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 342ms/step - loss: 0.1181 - accuracy: 0.9609 - val_loss: 2.5235 - val_accuracy: 0.6305 - lr: 0.0040\n","Epoch 359/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1064 - accuracy: 0.9642\n","Epoch 00359: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.1064 - accuracy: 0.9642 - val_loss: 1.5604 - val_accuracy: 0.7946 - lr: 0.0034\n","Epoch 360/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0946 - accuracy: 0.9679\n","Epoch 00360: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0946 - accuracy: 0.9679 - val_loss: 3.8733 - val_accuracy: 0.6880 - lr: 0.0029\n","Epoch 361/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0862 - accuracy: 0.9719\n","Epoch 00361: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0862 - accuracy: 0.9719 - val_loss: 0.6236 - val_accuracy: 0.8343 - lr: 0.0025\n","Epoch 362/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9777\n","Epoch 00362: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 329ms/step - loss: 0.0760 - accuracy: 0.9777 - val_loss: 1.3144 - val_accuracy: 0.7009 - lr: 0.0021\n","Epoch 363/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0760 - accuracy: 0.9782\n","Epoch 00363: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0760 - accuracy: 0.9782 - val_loss: 0.6127 - val_accuracy: 0.8814 - lr: 0.0018\n","Epoch 364/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0523 - accuracy: 0.9817\n","Epoch 00364: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0523 - accuracy: 0.9817 - val_loss: 0.3205 - val_accuracy: 0.9266 - lr: 0.0015\n","Epoch 365/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0525 - accuracy: 0.9835\n","Epoch 00365: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0525 - accuracy: 0.9835 - val_loss: 0.2701 - val_accuracy: 0.9474 - lr: 0.0013\n","Epoch 366/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0527 - accuracy: 0.9827\n","Epoch 00366: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0527 - accuracy: 0.9827 - val_loss: 1.5514 - val_accuracy: 0.7912 - lr: 0.0011\n","Epoch 367/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9845\n","Epoch 00367: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 330ms/step - loss: 0.0431 - accuracy: 0.9845 - val_loss: 0.1121 - val_accuracy: 0.9653 - lr: 9.2647e-04\n","Epoch 368/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0432 - accuracy: 0.9867\n","Epoch 00368: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0432 - accuracy: 0.9867 - val_loss: 1.2464 - val_accuracy: 0.8289 - lr: 7.8750e-04\n","Epoch 369/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0429 - accuracy: 0.9860\n","Epoch 00369: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0429 - accuracy: 0.9860 - val_loss: 0.1391 - val_accuracy: 0.9648 - lr: 6.6937e-04\n","Epoch 370/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0561 - accuracy: 0.9820\n","Epoch 00370: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0561 - accuracy: 0.9820 - val_loss: 0.6378 - val_accuracy: 0.9162 - lr: 5.6897e-04\n","Epoch 371/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9865\n","Epoch 00371: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 330ms/step - loss: 0.0385 - accuracy: 0.9865 - val_loss: 1.1166 - val_accuracy: 0.8423 - lr: 4.8362e-04\n","Epoch 372/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9867\n","Epoch 00372: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0421 - accuracy: 0.9867 - val_loss: 0.1502 - val_accuracy: 0.9673 - lr: 4.1108e-04\n","Epoch 373/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0380 - accuracy: 0.9860\n","Epoch 00373: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 328ms/step - loss: 0.0380 - accuracy: 0.9860 - val_loss: 0.6002 - val_accuracy: 0.8785 - lr: 3.4942e-04\n","Epoch 374/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9927\n","Epoch 00374: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 328ms/step - loss: 0.0318 - accuracy: 0.9927 - val_loss: 0.1851 - val_accuracy: 0.9519 - lr: 2.9700e-04\n","Epoch 375/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0314 - accuracy: 0.9915\n","Epoch 00375: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0314 - accuracy: 0.9915 - val_loss: 0.1264 - val_accuracy: 0.9722 - lr: 2.5245e-04\n","Epoch 376/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9872\n","Epoch 00376: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0397 - accuracy: 0.9872 - val_loss: 0.4605 - val_accuracy: 0.9499 - lr: 0.0010\n","Epoch 377/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0452 - accuracy: 0.9852\n","Epoch 00377: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0452 - accuracy: 0.9852 - val_loss: 3.2473 - val_accuracy: 0.8110 - lr: 0.0015\n","Epoch 378/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0668 - accuracy: 0.9810\n","Epoch 00378: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0668 - accuracy: 0.9810 - val_loss: 2.3604 - val_accuracy: 0.7966 - lr: 0.0020\n","Epoch 379/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0828 - accuracy: 0.9765\n","Epoch 00379: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0828 - accuracy: 0.9765 - val_loss: 0.3623 - val_accuracy: 0.9097 - lr: 0.0025\n","Epoch 380/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0755 - accuracy: 0.9755\n","Epoch 00380: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0755 - accuracy: 0.9755 - val_loss: 3.5333 - val_accuracy: 0.7138 - lr: 0.0030\n","Epoch 381/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1029 - accuracy: 0.9669\n","Epoch 00381: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.1029 - accuracy: 0.9669 - val_loss: 2.4202 - val_accuracy: 0.7118 - lr: 0.0035\n","Epoch 382/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1115 - accuracy: 0.9639\n","Epoch 00382: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 323ms/step - loss: 0.1115 - accuracy: 0.9639 - val_loss: 1.6042 - val_accuracy: 0.8110 - lr: 0.0040\n","Epoch 383/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1211 - accuracy: 0.9604\n","Epoch 00383: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.1211 - accuracy: 0.9604 - val_loss: 3.6932 - val_accuracy: 0.7287 - lr: 0.0040\n","Epoch 384/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0968 - accuracy: 0.9662\n","Epoch 00384: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0968 - accuracy: 0.9662 - val_loss: 0.3476 - val_accuracy: 0.9167 - lr: 0.0034\n","Epoch 385/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0847 - accuracy: 0.9717\n","Epoch 00385: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 330ms/step - loss: 0.0847 - accuracy: 0.9717 - val_loss: 0.9692 - val_accuracy: 0.7867 - lr: 0.0029\n","Epoch 386/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0796 - accuracy: 0.9737\n","Epoch 00386: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0796 - accuracy: 0.9737 - val_loss: 0.4288 - val_accuracy: 0.8676 - lr: 0.0025\n","Epoch 387/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9734\n","Epoch 00387: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0764 - accuracy: 0.9734 - val_loss: 0.2997 - val_accuracy: 0.9117 - lr: 0.0021\n","Epoch 388/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9765\n","Epoch 00388: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0711 - accuracy: 0.9765 - val_loss: 1.1434 - val_accuracy: 0.7902 - lr: 0.0018\n","Epoch 389/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0550 - accuracy: 0.9835\n","Epoch 00389: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0550 - accuracy: 0.9835 - val_loss: 0.3101 - val_accuracy: 0.9375 - lr: 0.0015\n","Epoch 390/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0478 - accuracy: 0.9827\n","Epoch 00390: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0478 - accuracy: 0.9827 - val_loss: 0.1299 - val_accuracy: 0.9752 - lr: 0.0013\n","Epoch 391/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0429 - accuracy: 0.9867\n","Epoch 00391: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0429 - accuracy: 0.9867 - val_loss: 1.6778 - val_accuracy: 0.8497 - lr: 0.0011\n","Epoch 392/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0515 - accuracy: 0.9827\n","Epoch 00392: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0515 - accuracy: 0.9827 - val_loss: 0.3986 - val_accuracy: 0.8919 - lr: 9.2647e-04\n","Epoch 393/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9872\n","Epoch 00393: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0391 - accuracy: 0.9872 - val_loss: 0.4181 - val_accuracy: 0.8934 - lr: 7.8750e-04\n","Epoch 394/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0381 - accuracy: 0.9857\n","Epoch 00394: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0381 - accuracy: 0.9857 - val_loss: 0.8287 - val_accuracy: 0.8299 - lr: 6.6937e-04\n","Epoch 395/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0396 - accuracy: 0.9860\n","Epoch 00395: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 328ms/step - loss: 0.0396 - accuracy: 0.9860 - val_loss: 0.9757 - val_accuracy: 0.8413 - lr: 5.6897e-04\n","Epoch 396/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0355 - accuracy: 0.9892\n","Epoch 00396: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0355 - accuracy: 0.9892 - val_loss: 0.2512 - val_accuracy: 0.9578 - lr: 4.8362e-04\n","Epoch 397/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9892\n","Epoch 00397: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0329 - accuracy: 0.9892 - val_loss: 0.1614 - val_accuracy: 0.9648 - lr: 4.1108e-04\n","Epoch 398/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9895\n","Epoch 00398: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 329ms/step - loss: 0.0379 - accuracy: 0.9895 - val_loss: 0.8245 - val_accuracy: 0.8527 - lr: 3.4942e-04\n","Epoch 399/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9907\n","Epoch 00399: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0316 - accuracy: 0.9907 - val_loss: 0.1639 - val_accuracy: 0.9494 - lr: 2.9700e-04\n","Epoch 400/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9865\n","Epoch 00400: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 332ms/step - loss: 0.0392 - accuracy: 0.9865 - val_loss: 0.1163 - val_accuracy: 0.9732 - lr: 2.5245e-04\n","Epoch 401/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0521 - accuracy: 0.9857\n","Epoch 00401: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 341ms/step - loss: 0.0521 - accuracy: 0.9857 - val_loss: 0.4565 - val_accuracy: 0.9311 - lr: 0.0010\n","Epoch 402/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.9845\n","Epoch 00402: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0509 - accuracy: 0.9845 - val_loss: 1.0881 - val_accuracy: 0.8309 - lr: 0.0015\n","Epoch 403/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9785\n","Epoch 00403: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0705 - accuracy: 0.9785 - val_loss: 4.0767 - val_accuracy: 0.6746 - lr: 0.0020\n","Epoch 404/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0714 - accuracy: 0.9790\n","Epoch 00404: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0714 - accuracy: 0.9790 - val_loss: 6.4426 - val_accuracy: 0.6300 - lr: 0.0025\n","Epoch 405/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0798 - accuracy: 0.9727\n","Epoch 00405: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0798 - accuracy: 0.9727 - val_loss: 3.7028 - val_accuracy: 0.7237 - lr: 0.0030\n","Epoch 406/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0980 - accuracy: 0.9719\n","Epoch 00406: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0980 - accuracy: 0.9719 - val_loss: 1.1834 - val_accuracy: 0.7525 - lr: 0.0035\n","Epoch 407/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1101 - accuracy: 0.9652\n","Epoch 00407: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 324ms/step - loss: 0.1101 - accuracy: 0.9652 - val_loss: 3.1321 - val_accuracy: 0.7515 - lr: 0.0040\n","Epoch 408/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1202 - accuracy: 0.9607\n","Epoch 00408: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.1202 - accuracy: 0.9607 - val_loss: 362.2177 - val_accuracy: 0.4167 - lr: 0.0040\n","Epoch 409/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1153 - accuracy: 0.9662\n","Epoch 00409: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 324ms/step - loss: 0.1153 - accuracy: 0.9662 - val_loss: 68.0922 - val_accuracy: 0.6285 - lr: 0.0034\n","Epoch 410/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0829 - accuracy: 0.9714\n","Epoch 00410: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0829 - accuracy: 0.9714 - val_loss: 0.9093 - val_accuracy: 0.8189 - lr: 0.0029\n","Epoch 411/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0861 - accuracy: 0.9734\n","Epoch 00411: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0861 - accuracy: 0.9734 - val_loss: 2.9620 - val_accuracy: 0.7460 - lr: 0.0025\n","Epoch 412/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0664 - accuracy: 0.9802\n","Epoch 00412: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 342ms/step - loss: 0.0664 - accuracy: 0.9802 - val_loss: 1.6010 - val_accuracy: 0.8031 - lr: 0.0021\n","Epoch 413/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0596 - accuracy: 0.9795\n","Epoch 00413: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0596 - accuracy: 0.9795 - val_loss: 59.2813 - val_accuracy: 0.6002 - lr: 0.0018\n","Epoch 414/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0608 - accuracy: 0.9807\n","Epoch 00414: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0608 - accuracy: 0.9807 - val_loss: 0.4881 - val_accuracy: 0.8234 - lr: 0.0015\n","Epoch 415/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0492 - accuracy: 0.9837\n","Epoch 00415: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0492 - accuracy: 0.9837 - val_loss: 1.1354 - val_accuracy: 0.7639 - lr: 0.0013\n","Epoch 416/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 0.9812\n","Epoch 00416: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0551 - accuracy: 0.9812 - val_loss: 0.6664 - val_accuracy: 0.8472 - lr: 0.0011\n","Epoch 417/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0467 - accuracy: 0.9837\n","Epoch 00417: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0467 - accuracy: 0.9837 - val_loss: 0.3565 - val_accuracy: 0.9132 - lr: 9.2647e-04\n","Epoch 418/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.9880\n","Epoch 00418: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0417 - accuracy: 0.9880 - val_loss: 0.2268 - val_accuracy: 0.9499 - lr: 7.8750e-04\n","Epoch 419/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0413 - accuracy: 0.9895\n","Epoch 00419: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0413 - accuracy: 0.9895 - val_loss: 0.1808 - val_accuracy: 0.9673 - lr: 6.6937e-04\n","Epoch 420/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9897\n","Epoch 00420: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 322ms/step - loss: 0.0321 - accuracy: 0.9897 - val_loss: 0.3113 - val_accuracy: 0.9276 - lr: 5.6897e-04\n","Epoch 421/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9892\n","Epoch 00421: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0317 - accuracy: 0.9892 - val_loss: 0.1103 - val_accuracy: 0.9727 - lr: 4.8362e-04\n","Epoch 422/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9890\n","Epoch 00422: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0349 - accuracy: 0.9890 - val_loss: 0.4627 - val_accuracy: 0.8849 - lr: 4.1108e-04\n","Epoch 423/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9905\n","Epoch 00423: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0327 - accuracy: 0.9905 - val_loss: 0.3129 - val_accuracy: 0.9216 - lr: 3.4942e-04\n","Epoch 424/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9897\n","Epoch 00424: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0288 - accuracy: 0.9897 - val_loss: 0.1126 - val_accuracy: 0.9782 - lr: 2.9700e-04\n","Epoch 425/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0412 - accuracy: 0.9867\n","Epoch 00425: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0412 - accuracy: 0.9867 - val_loss: 0.1237 - val_accuracy: 0.9762 - lr: 2.5245e-04\n","Epoch 426/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0479 - accuracy: 0.9870\n","Epoch 00426: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0479 - accuracy: 0.9870 - val_loss: 0.8425 - val_accuracy: 0.8313 - lr: 0.0010\n","Epoch 427/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9882\n","Epoch 00427: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0369 - accuracy: 0.9882 - val_loss: 3.1162 - val_accuracy: 0.8299 - lr: 0.0015\n","Epoch 428/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0523 - accuracy: 0.9810\n","Epoch 00428: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0523 - accuracy: 0.9810 - val_loss: 1.3684 - val_accuracy: 0.7693 - lr: 0.0020\n","Epoch 429/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0638 - accuracy: 0.9757\n","Epoch 00429: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0638 - accuracy: 0.9757 - val_loss: 2.8537 - val_accuracy: 0.8115 - lr: 0.0025\n","Epoch 430/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0948 - accuracy: 0.9714\n","Epoch 00430: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0948 - accuracy: 0.9714 - val_loss: 2.1356 - val_accuracy: 0.7728 - lr: 0.0030\n","Epoch 431/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0830 - accuracy: 0.9749\n","Epoch 00431: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0830 - accuracy: 0.9749 - val_loss: 1.2292 - val_accuracy: 0.8006 - lr: 0.0035\n","Epoch 432/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1066 - accuracy: 0.9682\n","Epoch 00432: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 321ms/step - loss: 0.1066 - accuracy: 0.9682 - val_loss: 3.9556 - val_accuracy: 0.6186 - lr: 0.0040\n","Epoch 433/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1185 - accuracy: 0.9599\n","Epoch 00433: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 323ms/step - loss: 0.1185 - accuracy: 0.9599 - val_loss: 19.5893 - val_accuracy: 0.6979 - lr: 0.0040\n","Epoch 434/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1142 - accuracy: 0.9669\n","Epoch 00434: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 324ms/step - loss: 0.1142 - accuracy: 0.9669 - val_loss: 2.2394 - val_accuracy: 0.7480 - lr: 0.0034\n","Epoch 435/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9744\n","Epoch 00435: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0788 - accuracy: 0.9744 - val_loss: 0.3793 - val_accuracy: 0.9137 - lr: 0.0029\n","Epoch 436/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9772\n","Epoch 00436: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0736 - accuracy: 0.9772 - val_loss: 1.1215 - val_accuracy: 0.7068 - lr: 0.0025\n","Epoch 437/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 0.9820\n","Epoch 00437: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0514 - accuracy: 0.9820 - val_loss: 1.5771 - val_accuracy: 0.8125 - lr: 0.0021\n","Epoch 438/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0590 - accuracy: 0.9822\n","Epoch 00438: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0590 - accuracy: 0.9822 - val_loss: 27.9252 - val_accuracy: 0.5833 - lr: 0.0018\n","Epoch 439/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.9840\n","Epoch 00439: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0588 - accuracy: 0.9840 - val_loss: 0.2258 - val_accuracy: 0.9375 - lr: 0.0015\n","Epoch 440/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0514 - accuracy: 0.9825\n","Epoch 00440: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0514 - accuracy: 0.9825 - val_loss: 0.9387 - val_accuracy: 0.7966 - lr: 0.0013\n","Epoch 441/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0414 - accuracy: 0.9872\n","Epoch 00441: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0414 - accuracy: 0.9872 - val_loss: 0.5362 - val_accuracy: 0.9187 - lr: 0.0011\n","Epoch 442/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9887\n","Epoch 00442: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0391 - accuracy: 0.9887 - val_loss: 0.3995 - val_accuracy: 0.9177 - lr: 9.2647e-04\n","Epoch 443/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9877\n","Epoch 00443: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0418 - accuracy: 0.9877 - val_loss: 0.2389 - val_accuracy: 0.9439 - lr: 7.8750e-04\n","Epoch 444/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9880\n","Epoch 00444: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0411 - accuracy: 0.9880 - val_loss: 0.2880 - val_accuracy: 0.9142 - lr: 6.6937e-04\n","Epoch 445/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9890\n","Epoch 00445: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0325 - accuracy: 0.9890 - val_loss: 1.3405 - val_accuracy: 0.8924 - lr: 5.6897e-04\n","Epoch 446/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9875\n","Epoch 00446: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0340 - accuracy: 0.9875 - val_loss: 2.1113 - val_accuracy: 0.8497 - lr: 4.8362e-04\n","Epoch 447/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0291 - accuracy: 0.9900\n","Epoch 00447: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0291 - accuracy: 0.9900 - val_loss: 0.1504 - val_accuracy: 0.9568 - lr: 4.1108e-04\n","Epoch 448/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9905\n","Epoch 00448: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 321ms/step - loss: 0.0340 - accuracy: 0.9905 - val_loss: 0.1668 - val_accuracy: 0.9489 - lr: 3.4942e-04\n","Epoch 449/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9905\n","Epoch 00449: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0283 - accuracy: 0.9905 - val_loss: 0.2275 - val_accuracy: 0.9593 - lr: 2.9700e-04\n","Epoch 450/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0332 - accuracy: 0.9900\n","Epoch 00450: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0332 - accuracy: 0.9900 - val_loss: 0.1798 - val_accuracy: 0.9435 - lr: 2.5245e-04\n","Epoch 451/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0303 - accuracy: 0.9902\n","Epoch 00451: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0303 - accuracy: 0.9902 - val_loss: 0.3268 - val_accuracy: 0.9494 - lr: 0.0010\n","Epoch 452/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9855\n","Epoch 00452: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0421 - accuracy: 0.9855 - val_loss: 0.2496 - val_accuracy: 0.9439 - lr: 0.0015\n","Epoch 453/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0562 - accuracy: 0.9815\n","Epoch 00453: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0562 - accuracy: 0.9815 - val_loss: 0.6912 - val_accuracy: 0.8338 - lr: 0.0020\n","Epoch 454/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.9810\n","Epoch 00454: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0577 - accuracy: 0.9810 - val_loss: 2.0857 - val_accuracy: 0.7822 - lr: 0.0025\n","Epoch 455/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0732 - accuracy: 0.9765\n","Epoch 00455: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0732 - accuracy: 0.9765 - val_loss: 6.6425 - val_accuracy: 0.6959 - lr: 0.0030\n","Epoch 456/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0914 - accuracy: 0.9729\n","Epoch 00456: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0914 - accuracy: 0.9729 - val_loss: 18.7514 - val_accuracy: 0.6453 - lr: 0.0035\n","Epoch 457/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0993 - accuracy: 0.9684\n","Epoch 00457: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0993 - accuracy: 0.9684 - val_loss: 2.9017 - val_accuracy: 0.7267 - lr: 0.0040\n","Epoch 458/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1066 - accuracy: 0.9674\n","Epoch 00458: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.1066 - accuracy: 0.9674 - val_loss: 2.6307 - val_accuracy: 0.7416 - lr: 0.0040\n","Epoch 459/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0983 - accuracy: 0.9699\n","Epoch 00459: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0983 - accuracy: 0.9699 - val_loss: 2.5547 - val_accuracy: 0.6096 - lr: 0.0034\n","Epoch 460/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0847 - accuracy: 0.9747\n","Epoch 00460: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0847 - accuracy: 0.9747 - val_loss: 2.1874 - val_accuracy: 0.7103 - lr: 0.0029\n","Epoch 461/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9734\n","Epoch 00461: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0771 - accuracy: 0.9734 - val_loss: 0.3344 - val_accuracy: 0.8973 - lr: 0.0025\n","Epoch 462/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0593 - accuracy: 0.9805\n","Epoch 00462: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0593 - accuracy: 0.9805 - val_loss: 18.6507 - val_accuracy: 0.7302 - lr: 0.0021\n","Epoch 463/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0535 - accuracy: 0.9830\n","Epoch 00463: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0535 - accuracy: 0.9830 - val_loss: 0.4302 - val_accuracy: 0.9147 - lr: 0.0018\n","Epoch 464/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9832\n","Epoch 00464: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0538 - accuracy: 0.9832 - val_loss: 0.5073 - val_accuracy: 0.8641 - lr: 0.0015\n","Epoch 465/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 0.9817\n","Epoch 00465: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0554 - accuracy: 0.9817 - val_loss: 0.7763 - val_accuracy: 0.8393 - lr: 0.0013\n","Epoch 466/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0456 - accuracy: 0.9865\n","Epoch 00466: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0456 - accuracy: 0.9865 - val_loss: 0.3162 - val_accuracy: 0.9177 - lr: 0.0011\n","Epoch 467/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9897\n","Epoch 00467: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0384 - accuracy: 0.9897 - val_loss: 0.5760 - val_accuracy: 0.8571 - lr: 9.2647e-04\n","Epoch 468/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9855\n","Epoch 00468: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0465 - accuracy: 0.9855 - val_loss: 0.2324 - val_accuracy: 0.9439 - lr: 7.8750e-04\n","Epoch 469/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9887\n","Epoch 00469: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0351 - accuracy: 0.9887 - val_loss: 0.3396 - val_accuracy: 0.9177 - lr: 6.6937e-04\n","Epoch 470/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0373 - accuracy: 0.9887\n","Epoch 00470: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0373 - accuracy: 0.9887 - val_loss: 0.3710 - val_accuracy: 0.9459 - lr: 5.6897e-04\n","Epoch 471/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9895\n","Epoch 00471: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0302 - accuracy: 0.9895 - val_loss: 0.1750 - val_accuracy: 0.9578 - lr: 4.8362e-04\n","Epoch 472/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9875\n","Epoch 00472: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0387 - accuracy: 0.9875 - val_loss: 0.2555 - val_accuracy: 0.9608 - lr: 4.1108e-04\n","Epoch 473/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0269 - accuracy: 0.9912\n","Epoch 00473: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0269 - accuracy: 0.9912 - val_loss: 0.1424 - val_accuracy: 0.9697 - lr: 3.4942e-04\n","Epoch 474/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9922\n","Epoch 00474: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0234 - accuracy: 0.9922 - val_loss: 0.1538 - val_accuracy: 0.9633 - lr: 2.9700e-04\n","Epoch 475/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9917\n","Epoch 00475: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0281 - accuracy: 0.9917 - val_loss: 0.3069 - val_accuracy: 0.9449 - lr: 2.5245e-04\n","Epoch 476/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9897\n","Epoch 00476: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0350 - accuracy: 0.9897 - val_loss: 0.1112 - val_accuracy: 0.9688 - lr: 0.0010\n","Epoch 477/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9865\n","Epoch 00477: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0391 - accuracy: 0.9865 - val_loss: 0.5531 - val_accuracy: 0.8775 - lr: 0.0015\n","Epoch 478/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0425 - accuracy: 0.9860\n","Epoch 00478: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0425 - accuracy: 0.9860 - val_loss: 0.9204 - val_accuracy: 0.8537 - lr: 0.0020\n","Epoch 479/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0679 - accuracy: 0.9782\n","Epoch 00479: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0679 - accuracy: 0.9782 - val_loss: 2.7712 - val_accuracy: 0.7991 - lr: 0.0025\n","Epoch 480/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0721 - accuracy: 0.9770\n","Epoch 00480: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0721 - accuracy: 0.9770 - val_loss: 0.8018 - val_accuracy: 0.8224 - lr: 0.0030\n","Epoch 481/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0893 - accuracy: 0.9737\n","Epoch 00481: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0893 - accuracy: 0.9737 - val_loss: 0.9076 - val_accuracy: 0.8051 - lr: 0.0035\n","Epoch 482/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9747\n","Epoch 00482: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0799 - accuracy: 0.9747 - val_loss: 5.7816 - val_accuracy: 0.6791 - lr: 0.0040\n","Epoch 483/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1112 - accuracy: 0.9632\n","Epoch 00483: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 329ms/step - loss: 0.1112 - accuracy: 0.9632 - val_loss: 175.0325 - val_accuracy: 0.5060 - lr: 0.0040\n","Epoch 484/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0739 - accuracy: 0.9767\n","Epoch 00484: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0739 - accuracy: 0.9767 - val_loss: 3.5231 - val_accuracy: 0.7758 - lr: 0.0034\n","Epoch 485/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0738 - accuracy: 0.9780\n","Epoch 00485: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0738 - accuracy: 0.9780 - val_loss: 0.3037 - val_accuracy: 0.9385 - lr: 0.0029\n","Epoch 486/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0672 - accuracy: 0.9780\n","Epoch 00486: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0672 - accuracy: 0.9780 - val_loss: 1.1894 - val_accuracy: 0.8497 - lr: 0.0025\n","Epoch 487/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0543 - accuracy: 0.9837\n","Epoch 00487: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0543 - accuracy: 0.9837 - val_loss: 1.0746 - val_accuracy: 0.8259 - lr: 0.0021\n","Epoch 488/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9862\n","Epoch 00488: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0494 - accuracy: 0.9862 - val_loss: 1.3948 - val_accuracy: 0.8358 - lr: 0.0018\n","Epoch 489/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0466 - accuracy: 0.9840\n","Epoch 00489: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 328ms/step - loss: 0.0466 - accuracy: 0.9840 - val_loss: 1.4149 - val_accuracy: 0.8199 - lr: 0.0015\n","Epoch 490/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0429 - accuracy: 0.9867\n","Epoch 00490: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0429 - accuracy: 0.9867 - val_loss: 1.6855 - val_accuracy: 0.8189 - lr: 0.0013\n","Epoch 491/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0444 - accuracy: 0.9862\n","Epoch 00491: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0444 - accuracy: 0.9862 - val_loss: 0.4746 - val_accuracy: 0.8720 - lr: 0.0011\n","Epoch 492/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9900\n","Epoch 00492: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0368 - accuracy: 0.9900 - val_loss: 0.2383 - val_accuracy: 0.9430 - lr: 9.2647e-04\n","Epoch 493/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9872\n","Epoch 00493: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0367 - accuracy: 0.9872 - val_loss: 0.2103 - val_accuracy: 0.9454 - lr: 7.8750e-04\n","Epoch 494/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9892\n","Epoch 00494: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0319 - accuracy: 0.9892 - val_loss: 1.5147 - val_accuracy: 0.8199 - lr: 6.6937e-04\n","Epoch 495/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0316 - accuracy: 0.9902\n","Epoch 00495: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0316 - accuracy: 0.9902 - val_loss: 0.1613 - val_accuracy: 0.9628 - lr: 5.6897e-04\n","Epoch 496/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9900\n","Epoch 00496: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0320 - accuracy: 0.9900 - val_loss: 0.1694 - val_accuracy: 0.9583 - lr: 4.8362e-04\n","Epoch 497/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9892\n","Epoch 00497: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0309 - accuracy: 0.9892 - val_loss: 0.1282 - val_accuracy: 0.9653 - lr: 4.1108e-04\n","Epoch 498/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9915\n","Epoch 00498: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0264 - accuracy: 0.9915 - val_loss: 0.1451 - val_accuracy: 0.9628 - lr: 3.4942e-04\n","Epoch 499/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9890\n","Epoch 00499: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0336 - accuracy: 0.9890 - val_loss: 0.5763 - val_accuracy: 0.9241 - lr: 2.9700e-04\n","Epoch 500/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9925\n","Epoch 00500: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0280 - accuracy: 0.9925 - val_loss: 0.1326 - val_accuracy: 0.9712 - lr: 2.5245e-04\n","Epoch 501/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0333 - accuracy: 0.9897\n","Epoch 00501: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0333 - accuracy: 0.9897 - val_loss: 0.3381 - val_accuracy: 0.8864 - lr: 0.0010\n","Epoch 502/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0458 - accuracy: 0.9857\n","Epoch 00502: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0458 - accuracy: 0.9857 - val_loss: 0.8317 - val_accuracy: 0.9137 - lr: 0.0015\n","Epoch 503/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0538 - accuracy: 0.9855\n","Epoch 00503: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0538 - accuracy: 0.9855 - val_loss: 0.7020 - val_accuracy: 0.8919 - lr: 0.0020\n","Epoch 504/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0483 - accuracy: 0.9845\n","Epoch 00504: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0483 - accuracy: 0.9845 - val_loss: 5.0882 - val_accuracy: 0.7232 - lr: 0.0025\n","Epoch 505/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0771 - accuracy: 0.9757\n","Epoch 00505: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0771 - accuracy: 0.9757 - val_loss: 38.0016 - val_accuracy: 0.6592 - lr: 0.0030\n","Epoch 506/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0821 - accuracy: 0.9722\n","Epoch 00506: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0821 - accuracy: 0.9722 - val_loss: 0.6828 - val_accuracy: 0.8323 - lr: 0.0035\n","Epoch 507/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0968 - accuracy: 0.9704\n","Epoch 00507: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0968 - accuracy: 0.9704 - val_loss: 1.8469 - val_accuracy: 0.7470 - lr: 0.0040\n","Epoch 508/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0997 - accuracy: 0.9704\n","Epoch 00508: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0997 - accuracy: 0.9704 - val_loss: 0.8241 - val_accuracy: 0.8487 - lr: 0.0040\n","Epoch 509/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0856 - accuracy: 0.9737\n","Epoch 00509: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0856 - accuracy: 0.9737 - val_loss: 2.6934 - val_accuracy: 0.7168 - lr: 0.0034\n","Epoch 510/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9757\n","Epoch 00510: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 329ms/step - loss: 0.0700 - accuracy: 0.9757 - val_loss: 1.3397 - val_accuracy: 0.7961 - lr: 0.0029\n","Epoch 511/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9812\n","Epoch 00511: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0662 - accuracy: 0.9812 - val_loss: 0.1834 - val_accuracy: 0.9370 - lr: 0.0025\n","Epoch 512/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0548 - accuracy: 0.9817\n","Epoch 00512: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0548 - accuracy: 0.9817 - val_loss: 1.8594 - val_accuracy: 0.6835 - lr: 0.0021\n","Epoch 513/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.9850\n","Epoch 00513: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0503 - accuracy: 0.9850 - val_loss: 2.0178 - val_accuracy: 0.6811 - lr: 0.0018\n","Epoch 514/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0537 - accuracy: 0.9840\n","Epoch 00514: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0537 - accuracy: 0.9840 - val_loss: 1.0233 - val_accuracy: 0.8641 - lr: 0.0015\n","Epoch 515/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0557 - accuracy: 0.9852\n","Epoch 00515: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0557 - accuracy: 0.9852 - val_loss: 1.8260 - val_accuracy: 0.8309 - lr: 0.0013\n","Epoch 516/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9877\n","Epoch 00516: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0410 - accuracy: 0.9877 - val_loss: 0.8475 - val_accuracy: 0.8785 - lr: 0.0011\n","Epoch 517/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0418 - accuracy: 0.9890\n","Epoch 00517: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0418 - accuracy: 0.9890 - val_loss: 0.1998 - val_accuracy: 0.9554 - lr: 9.2647e-04\n","Epoch 518/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0348 - accuracy: 0.9875\n","Epoch 00518: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0348 - accuracy: 0.9875 - val_loss: 0.1558 - val_accuracy: 0.9524 - lr: 7.8750e-04\n","Epoch 519/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9885\n","Epoch 00519: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0367 - accuracy: 0.9885 - val_loss: 0.8456 - val_accuracy: 0.8869 - lr: 6.6937e-04\n","Epoch 520/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9910\n","Epoch 00520: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0319 - accuracy: 0.9910 - val_loss: 0.2743 - val_accuracy: 0.9177 - lr: 5.6897e-04\n","Epoch 521/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9907\n","Epoch 00521: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 329ms/step - loss: 0.0255 - accuracy: 0.9907 - val_loss: 0.7143 - val_accuracy: 0.8532 - lr: 4.8362e-04\n","Epoch 522/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0292 - accuracy: 0.9910\n","Epoch 00522: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0292 - accuracy: 0.9910 - val_loss: 0.1504 - val_accuracy: 0.9663 - lr: 4.1108e-04\n","Epoch 523/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9927\n","Epoch 00523: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 329ms/step - loss: 0.0215 - accuracy: 0.9927 - val_loss: 0.1180 - val_accuracy: 0.9688 - lr: 3.4942e-04\n","Epoch 524/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0250 - accuracy: 0.9920\n","Epoch 00524: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0250 - accuracy: 0.9920 - val_loss: 0.1529 - val_accuracy: 0.9702 - lr: 2.9700e-04\n","Epoch 525/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9917\n","Epoch 00525: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0237 - accuracy: 0.9917 - val_loss: 0.1160 - val_accuracy: 0.9702 - lr: 2.5245e-04\n","Epoch 526/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0338 - accuracy: 0.9867\n","Epoch 00526: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0338 - accuracy: 0.9867 - val_loss: 0.2426 - val_accuracy: 0.9410 - lr: 0.0010\n","Epoch 527/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9862\n","Epoch 00527: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0392 - accuracy: 0.9862 - val_loss: 0.7821 - val_accuracy: 0.8353 - lr: 0.0015\n","Epoch 528/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9855\n","Epoch 00528: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0391 - accuracy: 0.9855 - val_loss: 2.7207 - val_accuracy: 0.7326 - lr: 0.0020\n","Epoch 529/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0643 - accuracy: 0.9807\n","Epoch 00529: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0643 - accuracy: 0.9807 - val_loss: 2.5648 - val_accuracy: 0.8214 - lr: 0.0025\n","Epoch 530/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9792\n","Epoch 00530: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 321ms/step - loss: 0.0726 - accuracy: 0.9792 - val_loss: 1.0294 - val_accuracy: 0.8130 - lr: 0.0030\n","Epoch 531/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0799 - accuracy: 0.9760\n","Epoch 00531: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0799 - accuracy: 0.9760 - val_loss: 39.3309 - val_accuracy: 0.5918 - lr: 0.0035\n","Epoch 532/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0951 - accuracy: 0.9712\n","Epoch 00532: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0951 - accuracy: 0.9712 - val_loss: 1.1407 - val_accuracy: 0.7555 - lr: 0.0040\n","Epoch 533/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0984 - accuracy: 0.9714\n","Epoch 00533: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0984 - accuracy: 0.9714 - val_loss: 83.9568 - val_accuracy: 0.5838 - lr: 0.0040\n","Epoch 534/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0795 - accuracy: 0.9739\n","Epoch 00534: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0795 - accuracy: 0.9739 - val_loss: 255.1651 - val_accuracy: 0.5556 - lr: 0.0034\n","Epoch 535/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9787\n","Epoch 00535: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 329ms/step - loss: 0.0767 - accuracy: 0.9787 - val_loss: 0.6930 - val_accuracy: 0.8839 - lr: 0.0029\n","Epoch 536/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9770\n","Epoch 00536: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 321ms/step - loss: 0.0709 - accuracy: 0.9770 - val_loss: 1.4370 - val_accuracy: 0.7902 - lr: 0.0025\n","Epoch 537/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0523 - accuracy: 0.9827\n","Epoch 00537: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0523 - accuracy: 0.9827 - val_loss: 0.7268 - val_accuracy: 0.8358 - lr: 0.0021\n","Epoch 538/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0497 - accuracy: 0.9822\n","Epoch 00538: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0497 - accuracy: 0.9822 - val_loss: 0.3215 - val_accuracy: 0.9306 - lr: 0.0018\n","Epoch 539/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9817\n","Epoch 00539: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0528 - accuracy: 0.9817 - val_loss: 0.2897 - val_accuracy: 0.9251 - lr: 0.0015\n","Epoch 540/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0377 - accuracy: 0.9890\n","Epoch 00540: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0377 - accuracy: 0.9890 - val_loss: 0.5233 - val_accuracy: 0.9261 - lr: 0.0013\n","Epoch 541/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9857\n","Epoch 00541: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 330ms/step - loss: 0.0405 - accuracy: 0.9857 - val_loss: 0.1823 - val_accuracy: 0.9549 - lr: 0.0011\n","Epoch 542/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0308 - accuracy: 0.9902\n","Epoch 00542: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0308 - accuracy: 0.9902 - val_loss: 1.0856 - val_accuracy: 0.8264 - lr: 9.2647e-04\n","Epoch 543/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9890\n","Epoch 00543: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0347 - accuracy: 0.9890 - val_loss: 0.5550 - val_accuracy: 0.9043 - lr: 7.8750e-04\n","Epoch 544/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9900\n","Epoch 00544: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0350 - accuracy: 0.9900 - val_loss: 0.2218 - val_accuracy: 0.9454 - lr: 6.6937e-04\n","Epoch 545/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9880\n","Epoch 00545: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0379 - accuracy: 0.9880 - val_loss: 0.2668 - val_accuracy: 0.9340 - lr: 5.6897e-04\n","Epoch 546/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0359 - accuracy: 0.9902\n","Epoch 00546: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0359 - accuracy: 0.9902 - val_loss: 0.1468 - val_accuracy: 0.9668 - lr: 4.8362e-04\n","Epoch 547/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0252 - accuracy: 0.9917\n","Epoch 00547: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0252 - accuracy: 0.9917 - val_loss: 0.3689 - val_accuracy: 0.9420 - lr: 4.1108e-04\n","Epoch 548/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9920\n","Epoch 00548: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0233 - accuracy: 0.9920 - val_loss: 0.4150 - val_accuracy: 0.9395 - lr: 3.4942e-04\n","Epoch 549/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9930\n","Epoch 00549: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0236 - accuracy: 0.9930 - val_loss: 0.1736 - val_accuracy: 0.9712 - lr: 2.9700e-04\n","Epoch 550/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0274 - accuracy: 0.9902\n","Epoch 00550: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0274 - accuracy: 0.9902 - val_loss: 0.1491 - val_accuracy: 0.9633 - lr: 2.5245e-04\n","Epoch 551/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0284 - accuracy: 0.9922\n","Epoch 00551: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 328ms/step - loss: 0.0284 - accuracy: 0.9922 - val_loss: 21.8300 - val_accuracy: 0.7098 - lr: 0.0010\n","Epoch 552/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0494 - accuracy: 0.9837\n","Epoch 00552: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0494 - accuracy: 0.9837 - val_loss: 0.6736 - val_accuracy: 0.8428 - lr: 0.0015\n","Epoch 553/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0475 - accuracy: 0.9872\n","Epoch 00553: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0475 - accuracy: 0.9872 - val_loss: 2.4203 - val_accuracy: 0.7927 - lr: 0.0020\n","Epoch 554/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9817\n","Epoch 00554: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0612 - accuracy: 0.9817 - val_loss: 2.4864 - val_accuracy: 0.7336 - lr: 0.0025\n","Epoch 555/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0678 - accuracy: 0.9775\n","Epoch 00555: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0678 - accuracy: 0.9775 - val_loss: 0.6185 - val_accuracy: 0.8819 - lr: 0.0030\n","Epoch 556/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0794 - accuracy: 0.9765\n","Epoch 00556: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0794 - accuracy: 0.9765 - val_loss: 169.8536 - val_accuracy: 0.5739 - lr: 0.0035\n","Epoch 557/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0857 - accuracy: 0.9732\n","Epoch 00557: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 329ms/step - loss: 0.0857 - accuracy: 0.9732 - val_loss: 1.0754 - val_accuracy: 0.8065 - lr: 0.0040\n","Epoch 558/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0906 - accuracy: 0.9729\n","Epoch 00558: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 323ms/step - loss: 0.0906 - accuracy: 0.9729 - val_loss: 49.6050 - val_accuracy: 0.6052 - lr: 0.0040\n","Epoch 559/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0859 - accuracy: 0.9739\n","Epoch 00559: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0859 - accuracy: 0.9739 - val_loss: 1.7131 - val_accuracy: 0.8051 - lr: 0.0034\n","Epoch 560/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0768 - accuracy: 0.9775\n","Epoch 00560: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0768 - accuracy: 0.9775 - val_loss: 0.2391 - val_accuracy: 0.9162 - lr: 0.0029\n","Epoch 561/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0612 - accuracy: 0.9802\n","Epoch 00561: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0612 - accuracy: 0.9802 - val_loss: 0.6346 - val_accuracy: 0.8393 - lr: 0.0025\n","Epoch 562/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0481 - accuracy: 0.9850\n","Epoch 00562: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0481 - accuracy: 0.9850 - val_loss: 0.7588 - val_accuracy: 0.8358 - lr: 0.0021\n","Epoch 563/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9867\n","Epoch 00563: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0385 - accuracy: 0.9867 - val_loss: 47.3223 - val_accuracy: 0.7207 - lr: 0.0018\n","Epoch 564/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9860\n","Epoch 00564: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 337ms/step - loss: 0.0424 - accuracy: 0.9860 - val_loss: 9.9915 - val_accuracy: 0.7758 - lr: 0.0015\n","Epoch 565/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0436 - accuracy: 0.9862\n","Epoch 00565: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0436 - accuracy: 0.9862 - val_loss: 3.5301 - val_accuracy: 0.8433 - lr: 0.0013\n","Epoch 566/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9890\n","Epoch 00566: val_loss did not improve from 0.09628\n","63/63 [==============================] - 21s 320ms/step - loss: 0.0369 - accuracy: 0.9890 - val_loss: 0.5463 - val_accuracy: 0.8651 - lr: 0.0011\n","Epoch 567/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9895\n","Epoch 00567: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0351 - accuracy: 0.9895 - val_loss: 0.1046 - val_accuracy: 0.9792 - lr: 9.2647e-04\n","Epoch 568/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9910\n","Epoch 00568: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0265 - accuracy: 0.9910 - val_loss: 0.1755 - val_accuracy: 0.9529 - lr: 7.8750e-04\n","Epoch 569/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9905\n","Epoch 00569: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 320ms/step - loss: 0.0266 - accuracy: 0.9905 - val_loss: 0.1732 - val_accuracy: 0.9529 - lr: 6.6937e-04\n","Epoch 570/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9922\n","Epoch 00570: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0218 - accuracy: 0.9922 - val_loss: 0.1578 - val_accuracy: 0.9618 - lr: 5.6897e-04\n","Epoch 571/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9912\n","Epoch 00571: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0281 - accuracy: 0.9912 - val_loss: 0.5459 - val_accuracy: 0.8819 - lr: 4.8362e-04\n","Epoch 572/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9925\n","Epoch 00572: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0246 - accuracy: 0.9925 - val_loss: 0.1252 - val_accuracy: 0.9742 - lr: 4.1108e-04\n","Epoch 573/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9937\n","Epoch 00573: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 322ms/step - loss: 0.0216 - accuracy: 0.9937 - val_loss: 0.1648 - val_accuracy: 0.9578 - lr: 3.4942e-04\n","Epoch 574/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9935\n","Epoch 00574: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0185 - accuracy: 0.9935 - val_loss: 0.1048 - val_accuracy: 0.9747 - lr: 2.9700e-04\n","Epoch 575/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0184 - accuracy: 0.9927\n","Epoch 00575: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0184 - accuracy: 0.9927 - val_loss: 1.0469 - val_accuracy: 0.9112 - lr: 2.5245e-04\n","Epoch 576/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9892\n","Epoch 00576: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0242 - accuracy: 0.9892 - val_loss: 0.9978 - val_accuracy: 0.8388 - lr: 0.0010\n","Epoch 577/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9895\n","Epoch 00577: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 323ms/step - loss: 0.0318 - accuracy: 0.9895 - val_loss: 1.6472 - val_accuracy: 0.7842 - lr: 0.0015\n","Epoch 578/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0472 - accuracy: 0.9852\n","Epoch 00578: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0472 - accuracy: 0.9852 - val_loss: 0.3336 - val_accuracy: 0.9251 - lr: 0.0020\n","Epoch 579/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 0.9832\n","Epoch 00579: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 330ms/step - loss: 0.0551 - accuracy: 0.9832 - val_loss: 0.3057 - val_accuracy: 0.9464 - lr: 0.0025\n","Epoch 580/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0783 - accuracy: 0.9772\n","Epoch 00580: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0783 - accuracy: 0.9772 - val_loss: 0.5891 - val_accuracy: 0.8586 - lr: 0.0030\n","Epoch 581/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9767\n","Epoch 00581: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0767 - accuracy: 0.9767 - val_loss: 14.5868 - val_accuracy: 0.6880 - lr: 0.0035\n","Epoch 582/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0838 - accuracy: 0.9714\n","Epoch 00582: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0838 - accuracy: 0.9714 - val_loss: 1.9127 - val_accuracy: 0.7426 - lr: 0.0040\n","Epoch 583/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.1180 - accuracy: 0.9647\n","Epoch 00583: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 329ms/step - loss: 0.1180 - accuracy: 0.9647 - val_loss: 1.3726 - val_accuracy: 0.7237 - lr: 0.0040\n","Epoch 584/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9757\n","Epoch 00584: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0715 - accuracy: 0.9757 - val_loss: 0.7559 - val_accuracy: 0.8542 - lr: 0.0034\n","Epoch 585/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0663 - accuracy: 0.9780\n","Epoch 00585: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0663 - accuracy: 0.9780 - val_loss: 0.3225 - val_accuracy: 0.8765 - lr: 0.0029\n","Epoch 586/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0617 - accuracy: 0.9797\n","Epoch 00586: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 321ms/step - loss: 0.0617 - accuracy: 0.9797 - val_loss: 0.9617 - val_accuracy: 0.7951 - lr: 0.0025\n","Epoch 587/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0553 - accuracy: 0.9820\n","Epoch 00587: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0553 - accuracy: 0.9820 - val_loss: 236.4930 - val_accuracy: 0.5888 - lr: 0.0021\n","Epoch 588/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0507 - accuracy: 0.9855\n","Epoch 00588: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0507 - accuracy: 0.9855 - val_loss: 6.1123 - val_accuracy: 0.7495 - lr: 0.0018\n","Epoch 589/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0424 - accuracy: 0.9835\n","Epoch 00589: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0424 - accuracy: 0.9835 - val_loss: 0.4176 - val_accuracy: 0.9191 - lr: 0.0015\n","Epoch 590/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0379 - accuracy: 0.9877\n","Epoch 00590: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0379 - accuracy: 0.9877 - val_loss: 0.7056 - val_accuracy: 0.8874 - lr: 0.0013\n","Epoch 591/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0366 - accuracy: 0.9887\n","Epoch 00591: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0366 - accuracy: 0.9887 - val_loss: 0.1259 - val_accuracy: 0.9658 - lr: 0.0011\n","Epoch 592/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9902\n","Epoch 00592: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0361 - accuracy: 0.9902 - val_loss: 2.2814 - val_accuracy: 0.8299 - lr: 9.2647e-04\n","Epoch 593/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9897\n","Epoch 00593: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 328ms/step - loss: 0.0302 - accuracy: 0.9897 - val_loss: 0.3730 - val_accuracy: 0.9048 - lr: 7.8750e-04\n","Epoch 594/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9917\n","Epoch 00594: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0220 - accuracy: 0.9917 - val_loss: 0.2145 - val_accuracy: 0.9568 - lr: 6.6937e-04\n","Epoch 595/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9882\n","Epoch 00595: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0327 - accuracy: 0.9882 - val_loss: 0.2198 - val_accuracy: 0.9509 - lr: 5.6897e-04\n","Epoch 596/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9915\n","Epoch 00596: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0301 - accuracy: 0.9915 - val_loss: 0.1745 - val_accuracy: 0.9688 - lr: 4.8362e-04\n","Epoch 597/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0181 - accuracy: 0.9947\n","Epoch 00597: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0181 - accuracy: 0.9947 - val_loss: 0.1768 - val_accuracy: 0.9732 - lr: 4.1108e-04\n","Epoch 598/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.9942\n","Epoch 00598: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0185 - accuracy: 0.9942 - val_loss: 0.1221 - val_accuracy: 0.9663 - lr: 3.4942e-04\n","Epoch 599/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0249 - accuracy: 0.9920\n","Epoch 00599: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0249 - accuracy: 0.9920 - val_loss: 1.6370 - val_accuracy: 0.8686 - lr: 2.9700e-04\n","Epoch 600/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9952\n","Epoch 00600: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0177 - accuracy: 0.9952 - val_loss: 0.1733 - val_accuracy: 0.9707 - lr: 2.5245e-04\n","Epoch 601/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9917\n","Epoch 00601: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0318 - accuracy: 0.9917 - val_loss: 0.5490 - val_accuracy: 0.9315 - lr: 0.0010\n","Epoch 602/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0385 - accuracy: 0.9880\n","Epoch 00602: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 323ms/step - loss: 0.0385 - accuracy: 0.9880 - val_loss: 0.8553 - val_accuracy: 0.8328 - lr: 0.0015\n","Epoch 603/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0395 - accuracy: 0.9877\n","Epoch 00603: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0395 - accuracy: 0.9877 - val_loss: 0.6161 - val_accuracy: 0.8512 - lr: 0.0020\n","Epoch 604/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0588 - accuracy: 0.9830\n","Epoch 00604: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0588 - accuracy: 0.9830 - val_loss: 2.5717 - val_accuracy: 0.7946 - lr: 0.0025\n","Epoch 605/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0552 - accuracy: 0.9825\n","Epoch 00605: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0552 - accuracy: 0.9825 - val_loss: 245.4999 - val_accuracy: 0.5278 - lr: 0.0030\n","Epoch 606/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0921 - accuracy: 0.9739\n","Epoch 00606: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0921 - accuracy: 0.9739 - val_loss: 239.0963 - val_accuracy: 0.6617 - lr: 0.0035\n","Epoch 607/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0759 - accuracy: 0.9749\n","Epoch 00607: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 332ms/step - loss: 0.0759 - accuracy: 0.9749 - val_loss: 146.7809 - val_accuracy: 0.4018 - lr: 0.0040\n","Epoch 608/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0979 - accuracy: 0.9737\n","Epoch 00608: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0979 - accuracy: 0.9737 - val_loss: 0.5489 - val_accuracy: 0.8403 - lr: 0.0040\n","Epoch 609/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0904 - accuracy: 0.9704\n","Epoch 00609: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0904 - accuracy: 0.9704 - val_loss: 0.6229 - val_accuracy: 0.8323 - lr: 0.0034\n","Epoch 610/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9757\n","Epoch 00610: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0750 - accuracy: 0.9757 - val_loss: 0.4506 - val_accuracy: 0.9028 - lr: 0.0029\n","Epoch 611/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0577 - accuracy: 0.9817\n","Epoch 00611: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0577 - accuracy: 0.9817 - val_loss: 1.7298 - val_accuracy: 0.8125 - lr: 0.0025\n","Epoch 612/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0586 - accuracy: 0.9800\n","Epoch 00612: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 329ms/step - loss: 0.0586 - accuracy: 0.9800 - val_loss: 0.7664 - val_accuracy: 0.8467 - lr: 0.0021\n","Epoch 613/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0410 - accuracy: 0.9867\n","Epoch 00613: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0410 - accuracy: 0.9867 - val_loss: 3.1493 - val_accuracy: 0.7793 - lr: 0.0018\n","Epoch 614/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9870\n","Epoch 00614: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0426 - accuracy: 0.9870 - val_loss: 0.3259 - val_accuracy: 0.9375 - lr: 0.0015\n","Epoch 615/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0422 - accuracy: 0.9865\n","Epoch 00615: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0422 - accuracy: 0.9865 - val_loss: 0.2854 - val_accuracy: 0.9335 - lr: 0.0013\n","Epoch 616/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9887\n","Epoch 00616: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0384 - accuracy: 0.9887 - val_loss: 0.3030 - val_accuracy: 0.9256 - lr: 0.0011\n","Epoch 617/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0311 - accuracy: 0.9897\n","Epoch 00617: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0311 - accuracy: 0.9897 - val_loss: 0.4800 - val_accuracy: 0.8621 - lr: 9.2647e-04\n","Epoch 618/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9922\n","Epoch 00618: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 338ms/step - loss: 0.0318 - accuracy: 0.9922 - val_loss: 209.3617 - val_accuracy: 0.7034 - lr: 7.8750e-04\n","Epoch 619/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0327 - accuracy: 0.9920\n","Epoch 00619: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0327 - accuracy: 0.9920 - val_loss: 0.1446 - val_accuracy: 0.9643 - lr: 6.6937e-04\n","Epoch 620/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9915\n","Epoch 00620: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0239 - accuracy: 0.9915 - val_loss: 0.5490 - val_accuracy: 0.8720 - lr: 5.6897e-04\n","Epoch 621/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9930\n","Epoch 00621: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0257 - accuracy: 0.9930 - val_loss: 0.1529 - val_accuracy: 0.9623 - lr: 4.8362e-04\n","Epoch 622/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9925\n","Epoch 00622: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0278 - accuracy: 0.9925 - val_loss: 0.2271 - val_accuracy: 0.9593 - lr: 4.1108e-04\n","Epoch 623/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0264 - accuracy: 0.9917\n","Epoch 00623: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0264 - accuracy: 0.9917 - val_loss: 0.1019 - val_accuracy: 0.9757 - lr: 3.4942e-04\n","Epoch 624/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0278 - accuracy: 0.9922\n","Epoch 00624: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0278 - accuracy: 0.9922 - val_loss: 0.2571 - val_accuracy: 0.9097 - lr: 2.9700e-04\n","Epoch 625/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9910\n","Epoch 00625: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0239 - accuracy: 0.9910 - val_loss: 0.5473 - val_accuracy: 0.8750 - lr: 2.5245e-04\n","Epoch 626/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9905\n","Epoch 00626: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0279 - accuracy: 0.9905 - val_loss: 59.3136 - val_accuracy: 0.7594 - lr: 0.0010\n","Epoch 627/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9887\n","Epoch 00627: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0313 - accuracy: 0.9887 - val_loss: 232.0729 - val_accuracy: 0.6374 - lr: 0.0015\n","Epoch 628/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0443 - accuracy: 0.9880\n","Epoch 00628: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 329ms/step - loss: 0.0443 - accuracy: 0.9880 - val_loss: 0.4060 - val_accuracy: 0.8983 - lr: 0.0020\n","Epoch 629/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0570 - accuracy: 0.9847\n","Epoch 00629: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0570 - accuracy: 0.9847 - val_loss: 1.2689 - val_accuracy: 0.7937 - lr: 0.0025\n","Epoch 630/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0608 - accuracy: 0.9832\n","Epoch 00630: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0608 - accuracy: 0.9832 - val_loss: 1.3399 - val_accuracy: 0.7728 - lr: 0.0030\n","Epoch 631/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9802\n","Epoch 00631: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0725 - accuracy: 0.9802 - val_loss: 516.2573 - val_accuracy: 0.5471 - lr: 0.0035\n","Epoch 632/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0810 - accuracy: 0.9734\n","Epoch 00632: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0810 - accuracy: 0.9734 - val_loss: 140.7136 - val_accuracy: 0.5977 - lr: 0.0040\n","Epoch 633/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0870 - accuracy: 0.9734\n","Epoch 00633: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0870 - accuracy: 0.9734 - val_loss: 222.4054 - val_accuracy: 0.5640 - lr: 0.0040\n","Epoch 634/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0709 - accuracy: 0.9785\n","Epoch 00634: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0709 - accuracy: 0.9785 - val_loss: 1.1127 - val_accuracy: 0.8452 - lr: 0.0034\n","Epoch 635/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0618 - accuracy: 0.9782\n","Epoch 00635: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0618 - accuracy: 0.9782 - val_loss: 2.2833 - val_accuracy: 0.7584 - lr: 0.0029\n","Epoch 636/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9795\n","Epoch 00636: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0725 - accuracy: 0.9795 - val_loss: 124.6757 - val_accuracy: 0.6602 - lr: 0.0025\n","Epoch 637/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0470 - accuracy: 0.9865\n","Epoch 00637: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0470 - accuracy: 0.9865 - val_loss: 0.5682 - val_accuracy: 0.9315 - lr: 0.0021\n","Epoch 638/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0393 - accuracy: 0.9872\n","Epoch 00638: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0393 - accuracy: 0.9872 - val_loss: 0.1507 - val_accuracy: 0.9603 - lr: 0.0018\n","Epoch 639/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9877\n","Epoch 00639: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0421 - accuracy: 0.9877 - val_loss: 0.3302 - val_accuracy: 0.9191 - lr: 0.0015\n","Epoch 640/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9882\n","Epoch 00640: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0384 - accuracy: 0.9882 - val_loss: 0.7108 - val_accuracy: 0.8934 - lr: 0.0013\n","Epoch 641/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9872\n","Epoch 00641: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0409 - accuracy: 0.9872 - val_loss: 0.4502 - val_accuracy: 0.9023 - lr: 0.0011\n","Epoch 642/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9910\n","Epoch 00642: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0289 - accuracy: 0.9910 - val_loss: 0.3902 - val_accuracy: 0.8864 - lr: 9.2647e-04\n","Epoch 643/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9900\n","Epoch 00643: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0309 - accuracy: 0.9900 - val_loss: 0.2257 - val_accuracy: 0.9529 - lr: 7.8750e-04\n","Epoch 644/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9932\n","Epoch 00644: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 338ms/step - loss: 0.0239 - accuracy: 0.9932 - val_loss: 0.1090 - val_accuracy: 0.9702 - lr: 6.6937e-04\n","Epoch 645/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9917\n","Epoch 00645: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 328ms/step - loss: 0.0253 - accuracy: 0.9917 - val_loss: 0.1527 - val_accuracy: 0.9668 - lr: 5.6897e-04\n","Epoch 646/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0218 - accuracy: 0.9920\n","Epoch 00646: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0218 - accuracy: 0.9920 - val_loss: 0.1547 - val_accuracy: 0.9678 - lr: 4.8362e-04\n","Epoch 647/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9920\n","Epoch 00647: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0254 - accuracy: 0.9920 - val_loss: 0.1513 - val_accuracy: 0.9643 - lr: 4.1108e-04\n","Epoch 648/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9922\n","Epoch 00648: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0341 - accuracy: 0.9922 - val_loss: 0.1244 - val_accuracy: 0.9692 - lr: 3.4942e-04\n","Epoch 649/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0242 - accuracy: 0.9920\n","Epoch 00649: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0242 - accuracy: 0.9920 - val_loss: 0.1042 - val_accuracy: 0.9762 - lr: 2.9700e-04\n","Epoch 650/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9917\n","Epoch 00650: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0257 - accuracy: 0.9917 - val_loss: 0.1302 - val_accuracy: 0.9712 - lr: 2.5245e-04\n","Epoch 651/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0318 - accuracy: 0.9902\n","Epoch 00651: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 338ms/step - loss: 0.0318 - accuracy: 0.9902 - val_loss: 1.7388 - val_accuracy: 0.8527 - lr: 0.0010\n","Epoch 652/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0429 - accuracy: 0.9880\n","Epoch 00652: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0429 - accuracy: 0.9880 - val_loss: 1.1554 - val_accuracy: 0.7951 - lr: 0.0015\n","Epoch 653/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0435 - accuracy: 0.9860\n","Epoch 00653: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0435 - accuracy: 0.9860 - val_loss: 0.9557 - val_accuracy: 0.8909 - lr: 0.0020\n","Epoch 654/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0522 - accuracy: 0.9832\n","Epoch 00654: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0522 - accuracy: 0.9832 - val_loss: 0.5118 - val_accuracy: 0.9077 - lr: 0.0025\n","Epoch 655/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0651 - accuracy: 0.9787\n","Epoch 00655: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 323ms/step - loss: 0.0651 - accuracy: 0.9787 - val_loss: 20.0040 - val_accuracy: 0.6949 - lr: 0.0030\n","Epoch 656/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9757\n","Epoch 00656: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0725 - accuracy: 0.9757 - val_loss: 0.4896 - val_accuracy: 0.8562 - lr: 0.0035\n","Epoch 657/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0902 - accuracy: 0.9724\n","Epoch 00657: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0902 - accuracy: 0.9724 - val_loss: 24.9319 - val_accuracy: 0.7426 - lr: 0.0040\n","Epoch 658/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0962 - accuracy: 0.9722\n","Epoch 00658: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0962 - accuracy: 0.9722 - val_loss: 4.9481 - val_accuracy: 0.7188 - lr: 0.0040\n","Epoch 659/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0686 - accuracy: 0.9780\n","Epoch 00659: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0686 - accuracy: 0.9780 - val_loss: 3.6604 - val_accuracy: 0.7812 - lr: 0.0034\n","Epoch 660/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0680 - accuracy: 0.9790\n","Epoch 00660: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0680 - accuracy: 0.9790 - val_loss: 133.7466 - val_accuracy: 0.5928 - lr: 0.0029\n","Epoch 661/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9837\n","Epoch 00661: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0487 - accuracy: 0.9837 - val_loss: 0.5914 - val_accuracy: 0.8517 - lr: 0.0025\n","Epoch 662/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0437 - accuracy: 0.9862\n","Epoch 00662: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 329ms/step - loss: 0.0437 - accuracy: 0.9862 - val_loss: 1.0108 - val_accuracy: 0.7981 - lr: 0.0021\n","Epoch 663/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0426 - accuracy: 0.9860\n","Epoch 00663: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0426 - accuracy: 0.9860 - val_loss: 0.1574 - val_accuracy: 0.9598 - lr: 0.0018\n","Epoch 664/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9862\n","Epoch 00664: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0411 - accuracy: 0.9862 - val_loss: 1.6391 - val_accuracy: 0.8229 - lr: 0.0015\n","Epoch 665/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9890\n","Epoch 00665: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 329ms/step - loss: 0.0370 - accuracy: 0.9890 - val_loss: 0.3030 - val_accuracy: 0.9296 - lr: 0.0013\n","Epoch 666/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0411 - accuracy: 0.9882\n","Epoch 00666: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0411 - accuracy: 0.9882 - val_loss: 0.1243 - val_accuracy: 0.9712 - lr: 0.0011\n","Epoch 667/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9920\n","Epoch 00667: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0255 - accuracy: 0.9920 - val_loss: 0.2532 - val_accuracy: 0.9484 - lr: 9.2647e-04\n","Epoch 668/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0294 - accuracy: 0.9922\n","Epoch 00668: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0294 - accuracy: 0.9922 - val_loss: 1.3959 - val_accuracy: 0.8472 - lr: 7.8750e-04\n","Epoch 669/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9927\n","Epoch 00669: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 323ms/step - loss: 0.0222 - accuracy: 0.9927 - val_loss: 0.2371 - val_accuracy: 0.9370 - lr: 6.6937e-04\n","Epoch 670/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9912\n","Epoch 00670: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0243 - accuracy: 0.9912 - val_loss: 0.1013 - val_accuracy: 0.9707 - lr: 5.6897e-04\n","Epoch 671/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0243 - accuracy: 0.9940\n","Epoch 00671: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0243 - accuracy: 0.9940 - val_loss: 0.1268 - val_accuracy: 0.9673 - lr: 4.8362e-04\n","Epoch 672/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0276 - accuracy: 0.9915\n","Epoch 00672: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 321ms/step - loss: 0.0276 - accuracy: 0.9915 - val_loss: 0.1327 - val_accuracy: 0.9683 - lr: 4.1108e-04\n","Epoch 673/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0199 - accuracy: 0.9947\n","Epoch 00673: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0199 - accuracy: 0.9947 - val_loss: 0.1110 - val_accuracy: 0.9727 - lr: 3.4942e-04\n","Epoch 674/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9945\n","Epoch 00674: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0263 - accuracy: 0.9945 - val_loss: 0.1525 - val_accuracy: 0.9633 - lr: 2.9700e-04\n","Epoch 675/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0256 - accuracy: 0.9937\n","Epoch 00675: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 323ms/step - loss: 0.0256 - accuracy: 0.9937 - val_loss: 0.1073 - val_accuracy: 0.9767 - lr: 2.5245e-04\n","Epoch 676/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9930\n","Epoch 00676: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0244 - accuracy: 0.9930 - val_loss: 0.3718 - val_accuracy: 0.9449 - lr: 0.0010\n","Epoch 677/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0368 - accuracy: 0.9872\n","Epoch 00677: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0368 - accuracy: 0.9872 - val_loss: 0.9025 - val_accuracy: 0.8323 - lr: 0.0015\n","Epoch 678/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0387 - accuracy: 0.9890\n","Epoch 00678: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0387 - accuracy: 0.9890 - val_loss: 0.6935 - val_accuracy: 0.8328 - lr: 0.0020\n","Epoch 679/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0509 - accuracy: 0.9840\n","Epoch 00679: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 328ms/step - loss: 0.0509 - accuracy: 0.9840 - val_loss: 146.4259 - val_accuracy: 0.5441 - lr: 0.0025\n","Epoch 680/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 0.9820\n","Epoch 00680: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0636 - accuracy: 0.9820 - val_loss: 3.3176 - val_accuracy: 0.7654 - lr: 0.0030\n","Epoch 681/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9790\n","Epoch 00681: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0670 - accuracy: 0.9790 - val_loss: 2.8185 - val_accuracy: 0.7510 - lr: 0.0035\n","Epoch 682/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9757\n","Epoch 00682: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0735 - accuracy: 0.9757 - val_loss: 138.9415 - val_accuracy: 0.6538 - lr: 0.0040\n","Epoch 683/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0868 - accuracy: 0.9734\n","Epoch 00683: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 328ms/step - loss: 0.0868 - accuracy: 0.9734 - val_loss: 0.8355 - val_accuracy: 0.8309 - lr: 0.0040\n","Epoch 684/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0796 - accuracy: 0.9755\n","Epoch 00684: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0796 - accuracy: 0.9755 - val_loss: 0.8241 - val_accuracy: 0.8155 - lr: 0.0034\n","Epoch 685/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0615 - accuracy: 0.9805\n","Epoch 00685: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0615 - accuracy: 0.9805 - val_loss: 0.2341 - val_accuracy: 0.9355 - lr: 0.0029\n","Epoch 686/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0595 - accuracy: 0.9810\n","Epoch 00686: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0595 - accuracy: 0.9810 - val_loss: 1.2363 - val_accuracy: 0.8284 - lr: 0.0025\n","Epoch 687/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0486 - accuracy: 0.9850\n","Epoch 00687: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0486 - accuracy: 0.9850 - val_loss: 0.6731 - val_accuracy: 0.8229 - lr: 0.0021\n","Epoch 688/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0384 - accuracy: 0.9852\n","Epoch 00688: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0384 - accuracy: 0.9852 - val_loss: 1.2396 - val_accuracy: 0.8626 - lr: 0.0018\n","Epoch 689/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 0.9865\n","Epoch 00689: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0427 - accuracy: 0.9865 - val_loss: 23.6413 - val_accuracy: 0.7034 - lr: 0.0015\n","Epoch 690/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0465 - accuracy: 0.9875\n","Epoch 00690: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0465 - accuracy: 0.9875 - val_loss: 0.4484 - val_accuracy: 0.8472 - lr: 0.0013\n","Epoch 691/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.9912\n","Epoch 00691: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 329ms/step - loss: 0.0300 - accuracy: 0.9912 - val_loss: 0.4696 - val_accuracy: 0.8616 - lr: 0.0011\n","Epoch 692/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0296 - accuracy: 0.9902\n","Epoch 00692: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0296 - accuracy: 0.9902 - val_loss: 0.2906 - val_accuracy: 0.9330 - lr: 9.2647e-04\n","Epoch 693/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9927\n","Epoch 00693: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0231 - accuracy: 0.9927 - val_loss: 0.1932 - val_accuracy: 0.9370 - lr: 7.8750e-04\n","Epoch 694/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0323 - accuracy: 0.9915\n","Epoch 00694: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0323 - accuracy: 0.9915 - val_loss: 0.6981 - val_accuracy: 0.8601 - lr: 6.6937e-04\n","Epoch 695/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9922\n","Epoch 00695: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 330ms/step - loss: 0.0251 - accuracy: 0.9922 - val_loss: 0.1922 - val_accuracy: 0.9534 - lr: 5.6897e-04\n","Epoch 696/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0253 - accuracy: 0.9935\n","Epoch 00696: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0253 - accuracy: 0.9935 - val_loss: 0.1856 - val_accuracy: 0.9554 - lr: 4.8362e-04\n","Epoch 697/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0206 - accuracy: 0.9927\n","Epoch 00697: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0206 - accuracy: 0.9927 - val_loss: 0.1764 - val_accuracy: 0.9618 - lr: 4.1108e-04\n","Epoch 698/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9947\n","Epoch 00698: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0173 - accuracy: 0.9947 - val_loss: 0.2478 - val_accuracy: 0.9370 - lr: 3.4942e-04\n","Epoch 699/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9930\n","Epoch 00699: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 328ms/step - loss: 0.0225 - accuracy: 0.9930 - val_loss: 0.6326 - val_accuracy: 0.8710 - lr: 2.9700e-04\n","Epoch 700/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0188 - accuracy: 0.9932\n","Epoch 00700: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0188 - accuracy: 0.9932 - val_loss: 0.2278 - val_accuracy: 0.9474 - lr: 2.5245e-04\n","Epoch 701/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9915\n","Epoch 00701: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0222 - accuracy: 0.9915 - val_loss: 0.7233 - val_accuracy: 0.8318 - lr: 0.0010\n","Epoch 702/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0255 - accuracy: 0.9927\n","Epoch 00702: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 322ms/step - loss: 0.0255 - accuracy: 0.9927 - val_loss: 1.7948 - val_accuracy: 0.8304 - lr: 0.0015\n","Epoch 703/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9912\n","Epoch 00703: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0321 - accuracy: 0.9912 - val_loss: 60.4297 - val_accuracy: 0.6731 - lr: 0.0020\n","Epoch 704/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0670 - accuracy: 0.9827\n","Epoch 00704: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0670 - accuracy: 0.9827 - val_loss: 3.1320 - val_accuracy: 0.7470 - lr: 0.0025\n","Epoch 705/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0551 - accuracy: 0.9830\n","Epoch 00705: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0551 - accuracy: 0.9830 - val_loss: 0.1934 - val_accuracy: 0.9449 - lr: 0.0030\n","Epoch 706/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0646 - accuracy: 0.9787\n","Epoch 00706: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0646 - accuracy: 0.9787 - val_loss: 0.6691 - val_accuracy: 0.9008 - lr: 0.0035\n","Epoch 707/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9749\n","Epoch 00707: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0787 - accuracy: 0.9749 - val_loss: 2.3641 - val_accuracy: 0.7733 - lr: 0.0040\n","Epoch 708/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9747\n","Epoch 00708: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0802 - accuracy: 0.9747 - val_loss: 1.1979 - val_accuracy: 0.7783 - lr: 0.0040\n","Epoch 709/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0828 - accuracy: 0.9760\n","Epoch 00709: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0828 - accuracy: 0.9760 - val_loss: 0.9988 - val_accuracy: 0.7579 - lr: 0.0034\n","Epoch 710/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0673 - accuracy: 0.9787\n","Epoch 00710: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0673 - accuracy: 0.9787 - val_loss: 27.0820 - val_accuracy: 0.7192 - lr: 0.0029\n","Epoch 711/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0623 - accuracy: 0.9820\n","Epoch 00711: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0623 - accuracy: 0.9820 - val_loss: 20.2210 - val_accuracy: 0.7143 - lr: 0.0025\n","Epoch 712/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0554 - accuracy: 0.9812\n","Epoch 00712: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0554 - accuracy: 0.9812 - val_loss: 110.6222 - val_accuracy: 0.6136 - lr: 0.0021\n","Epoch 713/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0415 - accuracy: 0.9850\n","Epoch 00713: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0415 - accuracy: 0.9850 - val_loss: 0.9135 - val_accuracy: 0.8909 - lr: 0.0018\n","Epoch 714/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0350 - accuracy: 0.9887\n","Epoch 00714: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0350 - accuracy: 0.9887 - val_loss: 0.7975 - val_accuracy: 0.8958 - lr: 0.0015\n","Epoch 715/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0370 - accuracy: 0.9870\n","Epoch 00715: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0370 - accuracy: 0.9870 - val_loss: 0.1894 - val_accuracy: 0.9573 - lr: 0.0013\n","Epoch 716/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0342 - accuracy: 0.9882\n","Epoch 00716: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 321ms/step - loss: 0.0342 - accuracy: 0.9882 - val_loss: 3.3055 - val_accuracy: 0.8487 - lr: 0.0011\n","Epoch 717/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0295 - accuracy: 0.9895\n","Epoch 00717: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0295 - accuracy: 0.9895 - val_loss: 1.2558 - val_accuracy: 0.8328 - lr: 9.2647e-04\n","Epoch 718/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9902\n","Epoch 00718: val_loss did not improve from 0.09628\n","63/63 [==============================] - 21s 334ms/step - loss: 0.0317 - accuracy: 0.9902 - val_loss: 0.3981 - val_accuracy: 0.9494 - lr: 7.8750e-04\n","Epoch 719/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0280 - accuracy: 0.9902\n","Epoch 00719: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0280 - accuracy: 0.9902 - val_loss: 0.3627 - val_accuracy: 0.9038 - lr: 6.6937e-04\n","Epoch 720/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0196 - accuracy: 0.9935\n","Epoch 00720: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0196 - accuracy: 0.9935 - val_loss: 0.3100 - val_accuracy: 0.9152 - lr: 5.6897e-04\n","Epoch 721/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9922\n","Epoch 00721: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0186 - accuracy: 0.9922 - val_loss: 0.6280 - val_accuracy: 0.8576 - lr: 4.8362e-04\n","Epoch 722/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0158 - accuracy: 0.9955\n","Epoch 00722: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 340ms/step - loss: 0.0158 - accuracy: 0.9955 - val_loss: 0.9417 - val_accuracy: 0.8542 - lr: 4.1108e-04\n","Epoch 723/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9925\n","Epoch 00723: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0244 - accuracy: 0.9925 - val_loss: 0.2915 - val_accuracy: 0.9355 - lr: 3.4942e-04\n","Epoch 724/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0142 - accuracy: 0.9955\n","Epoch 00724: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 337ms/step - loss: 0.0142 - accuracy: 0.9955 - val_loss: 0.2216 - val_accuracy: 0.9712 - lr: 2.9700e-04\n","Epoch 725/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9942\n","Epoch 00725: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0222 - accuracy: 0.9942 - val_loss: 0.2041 - val_accuracy: 0.9752 - lr: 2.5245e-04\n","Epoch 726/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9920\n","Epoch 00726: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0228 - accuracy: 0.9920 - val_loss: 0.2067 - val_accuracy: 0.9494 - lr: 0.0010\n","Epoch 727/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0334 - accuracy: 0.9907\n","Epoch 00727: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0334 - accuracy: 0.9907 - val_loss: 0.5542 - val_accuracy: 0.8859 - lr: 0.0015\n","Epoch 728/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0325 - accuracy: 0.9890\n","Epoch 00728: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0325 - accuracy: 0.9890 - val_loss: 0.4569 - val_accuracy: 0.8924 - lr: 0.0020\n","Epoch 729/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0469 - accuracy: 0.9865\n","Epoch 00729: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0469 - accuracy: 0.9865 - val_loss: 43.6357 - val_accuracy: 0.6518 - lr: 0.0025\n","Epoch 730/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0628 - accuracy: 0.9842\n","Epoch 00730: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0628 - accuracy: 0.9842 - val_loss: 60.7649 - val_accuracy: 0.6905 - lr: 0.0030\n","Epoch 731/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0591 - accuracy: 0.9825\n","Epoch 00731: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0591 - accuracy: 0.9825 - val_loss: 2.0972 - val_accuracy: 0.7197 - lr: 0.0035\n","Epoch 732/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9795\n","Epoch 00732: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0731 - accuracy: 0.9795 - val_loss: 2.2805 - val_accuracy: 0.7932 - lr: 0.0040\n","Epoch 733/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0762 - accuracy: 0.9724\n","Epoch 00733: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0762 - accuracy: 0.9724 - val_loss: 6.9498 - val_accuracy: 0.7455 - lr: 0.0040\n","Epoch 734/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0644 - accuracy: 0.9802\n","Epoch 00734: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0644 - accuracy: 0.9802 - val_loss: 0.7788 - val_accuracy: 0.8919 - lr: 0.0034\n","Epoch 735/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0557 - accuracy: 0.9825\n","Epoch 00735: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0557 - accuracy: 0.9825 - val_loss: 0.7924 - val_accuracy: 0.8646 - lr: 0.0029\n","Epoch 736/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0508 - accuracy: 0.9830\n","Epoch 00736: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0508 - accuracy: 0.9830 - val_loss: 1.4339 - val_accuracy: 0.7237 - lr: 0.0025\n","Epoch 737/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0440 - accuracy: 0.9875\n","Epoch 00737: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0440 - accuracy: 0.9875 - val_loss: 0.3521 - val_accuracy: 0.9167 - lr: 0.0021\n","Epoch 738/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0461 - accuracy: 0.9865\n","Epoch 00738: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0461 - accuracy: 0.9865 - val_loss: 0.5982 - val_accuracy: 0.8274 - lr: 0.0018\n","Epoch 739/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0341 - accuracy: 0.9895\n","Epoch 00739: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0341 - accuracy: 0.9895 - val_loss: 0.1819 - val_accuracy: 0.9588 - lr: 0.0015\n","Epoch 740/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0412 - accuracy: 0.9890\n","Epoch 00740: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0412 - accuracy: 0.9890 - val_loss: 0.2680 - val_accuracy: 0.9380 - lr: 0.0013\n","Epoch 741/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0189 - accuracy: 0.9915\n","Epoch 00741: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0189 - accuracy: 0.9915 - val_loss: 1.3661 - val_accuracy: 0.8537 - lr: 0.0011\n","Epoch 742/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0279 - accuracy: 0.9905\n","Epoch 00742: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 323ms/step - loss: 0.0279 - accuracy: 0.9905 - val_loss: 0.2071 - val_accuracy: 0.9494 - lr: 9.2647e-04\n","Epoch 743/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0232 - accuracy: 0.9922\n","Epoch 00743: val_loss did not improve from 0.09628\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0232 - accuracy: 0.9922 - val_loss: 0.1996 - val_accuracy: 0.9459 - lr: 7.8750e-04\n","Epoch 744/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0268 - accuracy: 0.9915\n","Epoch 00744: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0268 - accuracy: 0.9915 - val_loss: 0.2024 - val_accuracy: 0.9559 - lr: 6.6937e-04\n","Epoch 745/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0202 - accuracy: 0.9917\n","Epoch 00745: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0202 - accuracy: 0.9917 - val_loss: 1.3997 - val_accuracy: 0.8373 - lr: 5.6897e-04\n","Epoch 746/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0193 - accuracy: 0.9935\n","Epoch 00746: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0193 - accuracy: 0.9935 - val_loss: 0.1490 - val_accuracy: 0.9653 - lr: 4.8362e-04\n","Epoch 747/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9925\n","Epoch 00747: val_loss did not improve from 0.09628\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0233 - accuracy: 0.9925 - val_loss: 0.1452 - val_accuracy: 0.9792 - lr: 4.1108e-04\n","Epoch 748/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0257 - accuracy: 0.9925\n","Epoch 00748: val_loss improved from 0.09628 to 0.09610, saving model to /content/drive/MyDrive/output/JP30N02--1-best_result.hdf5\n","63/63 [==============================] - 25s 365ms/step - loss: 0.0257 - accuracy: 0.9925 - val_loss: 0.0961 - val_accuracy: 0.9831 - lr: 3.4942e-04\n","Epoch 749/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9945\n","Epoch 00749: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0171 - accuracy: 0.9945 - val_loss: 0.0979 - val_accuracy: 0.9772 - lr: 2.9700e-04\n","Epoch 750/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0221 - accuracy: 0.9940\n","Epoch 00750: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 320ms/step - loss: 0.0221 - accuracy: 0.9940 - val_loss: 0.7607 - val_accuracy: 0.9216 - lr: 2.5245e-04\n","Epoch 751/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9915\n","Epoch 00751: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0215 - accuracy: 0.9915 - val_loss: 0.2181 - val_accuracy: 0.9578 - lr: 0.0010\n","Epoch 752/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0288 - accuracy: 0.9920\n","Epoch 00752: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0288 - accuracy: 0.9920 - val_loss: 1.0214 - val_accuracy: 0.7535 - lr: 0.0015\n","Epoch 753/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9905\n","Epoch 00753: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0335 - accuracy: 0.9905 - val_loss: 13.7462 - val_accuracy: 0.7649 - lr: 0.0020\n","Epoch 754/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0463 - accuracy: 0.9882\n","Epoch 00754: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0463 - accuracy: 0.9882 - val_loss: 56.4943 - val_accuracy: 0.6062 - lr: 0.0025\n","Epoch 755/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9800\n","Epoch 00755: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0711 - accuracy: 0.9800 - val_loss: 91.7128 - val_accuracy: 0.6538 - lr: 0.0030\n","Epoch 756/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0561 - accuracy: 0.9825\n","Epoch 00756: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0561 - accuracy: 0.9825 - val_loss: 2.5524 - val_accuracy: 0.8661 - lr: 0.0035\n","Epoch 757/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0895 - accuracy: 0.9722\n","Epoch 00757: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 320ms/step - loss: 0.0895 - accuracy: 0.9722 - val_loss: 2.5121 - val_accuracy: 0.6875 - lr: 0.0040\n","Epoch 758/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0712 - accuracy: 0.9805\n","Epoch 00758: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0712 - accuracy: 0.9805 - val_loss: 2.3502 - val_accuracy: 0.5853 - lr: 0.0040\n","Epoch 759/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9775\n","Epoch 00759: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0700 - accuracy: 0.9775 - val_loss: 108.0447 - val_accuracy: 0.5575 - lr: 0.0034\n","Epoch 760/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0560 - accuracy: 0.9837\n","Epoch 00760: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0560 - accuracy: 0.9837 - val_loss: 0.4273 - val_accuracy: 0.9053 - lr: 0.0029\n","Epoch 761/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0561 - accuracy: 0.9842\n","Epoch 00761: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0561 - accuracy: 0.9842 - val_loss: 0.3678 - val_accuracy: 0.9315 - lr: 0.0025\n","Epoch 762/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9842\n","Epoch 00762: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 321ms/step - loss: 0.0480 - accuracy: 0.9842 - val_loss: 0.3808 - val_accuracy: 0.9400 - lr: 0.0021\n","Epoch 763/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0337 - accuracy: 0.9897\n","Epoch 00763: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0337 - accuracy: 0.9897 - val_loss: 1.1284 - val_accuracy: 0.7946 - lr: 0.0018\n","Epoch 764/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0455 - accuracy: 0.9862\n","Epoch 00764: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0455 - accuracy: 0.9862 - val_loss: 0.5322 - val_accuracy: 0.8656 - lr: 0.0015\n","Epoch 765/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9845\n","Epoch 00765: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0409 - accuracy: 0.9845 - val_loss: 0.1773 - val_accuracy: 0.9559 - lr: 0.0013\n","Epoch 766/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0289 - accuracy: 0.9905\n","Epoch 00766: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0289 - accuracy: 0.9905 - val_loss: 0.8375 - val_accuracy: 0.8457 - lr: 0.0011\n","Epoch 767/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9905\n","Epoch 00767: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0263 - accuracy: 0.9905 - val_loss: 1.1062 - val_accuracy: 0.8353 - lr: 9.2647e-04\n","Epoch 768/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9922\n","Epoch 00768: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0239 - accuracy: 0.9922 - val_loss: 1.3821 - val_accuracy: 0.8442 - lr: 7.8750e-04\n","Epoch 769/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9922\n","Epoch 00769: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0247 - accuracy: 0.9922 - val_loss: 0.4299 - val_accuracy: 0.8884 - lr: 6.6937e-04\n","Epoch 770/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9922\n","Epoch 00770: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0212 - accuracy: 0.9922 - val_loss: 0.2420 - val_accuracy: 0.9489 - lr: 5.6897e-04\n","Epoch 771/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0239 - accuracy: 0.9925\n","Epoch 00771: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0239 - accuracy: 0.9925 - val_loss: 0.3242 - val_accuracy: 0.9196 - lr: 4.8362e-04\n","Epoch 772/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0251 - accuracy: 0.9935\n","Epoch 00772: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 321ms/step - loss: 0.0251 - accuracy: 0.9935 - val_loss: 0.4845 - val_accuracy: 0.8904 - lr: 4.1108e-04\n","Epoch 773/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9935\n","Epoch 00773: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 322ms/step - loss: 0.0234 - accuracy: 0.9935 - val_loss: 0.2030 - val_accuracy: 0.9395 - lr: 3.4942e-04\n","Epoch 774/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9922\n","Epoch 00774: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0228 - accuracy: 0.9922 - val_loss: 0.1418 - val_accuracy: 0.9752 - lr: 2.9700e-04\n","Epoch 775/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0111 - accuracy: 0.9962\n","Epoch 00775: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0111 - accuracy: 0.9962 - val_loss: 0.1524 - val_accuracy: 0.9757 - lr: 2.5245e-04\n","Epoch 776/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9927\n","Epoch 00776: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 339ms/step - loss: 0.0246 - accuracy: 0.9927 - val_loss: 0.1775 - val_accuracy: 0.9573 - lr: 0.0010\n","Epoch 777/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9917\n","Epoch 00777: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0259 - accuracy: 0.9917 - val_loss: 1.5172 - val_accuracy: 0.8616 - lr: 0.0015\n","Epoch 778/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0448 - accuracy: 0.9877\n","Epoch 00778: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0448 - accuracy: 0.9877 - val_loss: 0.7420 - val_accuracy: 0.8378 - lr: 0.0020\n","Epoch 779/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0406 - accuracy: 0.9872\n","Epoch 00779: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0406 - accuracy: 0.9872 - val_loss: 0.4990 - val_accuracy: 0.8492 - lr: 0.0025\n","Epoch 780/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0497 - accuracy: 0.9850\n","Epoch 00780: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0497 - accuracy: 0.9850 - val_loss: 0.3211 - val_accuracy: 0.9251 - lr: 0.0030\n","Epoch 781/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0676 - accuracy: 0.9817\n","Epoch 00781: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0676 - accuracy: 0.9817 - val_loss: 6.7511 - val_accuracy: 0.7778 - lr: 0.0035\n","Epoch 782/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0865 - accuracy: 0.9709\n","Epoch 00782: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0865 - accuracy: 0.9709 - val_loss: 0.6707 - val_accuracy: 0.8988 - lr: 0.0040\n","Epoch 783/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0662 - accuracy: 0.9780\n","Epoch 00783: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0662 - accuracy: 0.9780 - val_loss: 1.9067 - val_accuracy: 0.7688 - lr: 0.0040\n","Epoch 784/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0581 - accuracy: 0.9807\n","Epoch 00784: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0581 - accuracy: 0.9807 - val_loss: 2.3316 - val_accuracy: 0.7664 - lr: 0.0034\n","Epoch 785/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0651 - accuracy: 0.9825\n","Epoch 00785: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0651 - accuracy: 0.9825 - val_loss: 0.6129 - val_accuracy: 0.8542 - lr: 0.0029\n","Epoch 786/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0543 - accuracy: 0.9845\n","Epoch 00786: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 329ms/step - loss: 0.0543 - accuracy: 0.9845 - val_loss: 1.1298 - val_accuracy: 0.8115 - lr: 0.0025\n","Epoch 787/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0474 - accuracy: 0.9857\n","Epoch 00787: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0474 - accuracy: 0.9857 - val_loss: 0.3502 - val_accuracy: 0.9097 - lr: 0.0021\n","Epoch 788/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0340 - accuracy: 0.9895\n","Epoch 00788: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0340 - accuracy: 0.9895 - val_loss: 0.6053 - val_accuracy: 0.8442 - lr: 0.0018\n","Epoch 789/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0403 - accuracy: 0.9887\n","Epoch 00789: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0403 - accuracy: 0.9887 - val_loss: 0.5762 - val_accuracy: 0.9320 - lr: 0.0015\n","Epoch 790/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9892\n","Epoch 00790: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0309 - accuracy: 0.9892 - val_loss: 0.2548 - val_accuracy: 0.9598 - lr: 0.0013\n","Epoch 791/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0286 - accuracy: 0.9900\n","Epoch 00791: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0286 - accuracy: 0.9900 - val_loss: 0.1738 - val_accuracy: 0.9549 - lr: 0.0011\n","Epoch 792/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9910\n","Epoch 00792: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0283 - accuracy: 0.9910 - val_loss: 0.5955 - val_accuracy: 0.8631 - lr: 9.2647e-04\n","Epoch 793/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9910\n","Epoch 00793: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0263 - accuracy: 0.9910 - val_loss: 0.3058 - val_accuracy: 0.9177 - lr: 7.8750e-04\n","Epoch 794/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9925\n","Epoch 00794: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0220 - accuracy: 0.9925 - val_loss: 0.1746 - val_accuracy: 0.9668 - lr: 6.6937e-04\n","Epoch 795/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9930\n","Epoch 00795: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0228 - accuracy: 0.9930 - val_loss: 1.8297 - val_accuracy: 0.8661 - lr: 5.6897e-04\n","Epoch 796/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0229 - accuracy: 0.9932\n","Epoch 00796: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0229 - accuracy: 0.9932 - val_loss: 0.1628 - val_accuracy: 0.9603 - lr: 4.8362e-04\n","Epoch 797/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9935\n","Epoch 00797: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0169 - accuracy: 0.9935 - val_loss: 0.2137 - val_accuracy: 0.9494 - lr: 4.1108e-04\n","Epoch 798/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0178 - accuracy: 0.9937\n","Epoch 00798: val_loss did not improve from 0.09610\n","63/63 [==============================] - 21s 322ms/step - loss: 0.0178 - accuracy: 0.9937 - val_loss: 0.1750 - val_accuracy: 0.9573 - lr: 3.4942e-04\n","Epoch 799/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9947\n","Epoch 00799: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0168 - accuracy: 0.9947 - val_loss: 1.3522 - val_accuracy: 0.8438 - lr: 2.9700e-04\n","Epoch 800/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0132 - accuracy: 0.9937\n","Epoch 00800: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0132 - accuracy: 0.9937 - val_loss: 0.2725 - val_accuracy: 0.9142 - lr: 2.5245e-04\n","Epoch 801/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0265 - accuracy: 0.9925\n","Epoch 00801: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0265 - accuracy: 0.9925 - val_loss: 0.2719 - val_accuracy: 0.9588 - lr: 0.0010\n","Epoch 802/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9912\n","Epoch 00802: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0270 - accuracy: 0.9912 - val_loss: 0.5992 - val_accuracy: 0.9226 - lr: 0.0015\n","Epoch 803/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0391 - accuracy: 0.9877\n","Epoch 00803: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 339ms/step - loss: 0.0391 - accuracy: 0.9877 - val_loss: 19.2206 - val_accuracy: 0.7366 - lr: 0.0020\n","Epoch 804/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0408 - accuracy: 0.9880\n","Epoch 00804: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 321ms/step - loss: 0.0408 - accuracy: 0.9880 - val_loss: 1.0214 - val_accuracy: 0.8423 - lr: 0.0025\n","Epoch 805/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0599 - accuracy: 0.9817\n","Epoch 00805: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0599 - accuracy: 0.9817 - val_loss: 1.3931 - val_accuracy: 0.7738 - lr: 0.0030\n","Epoch 806/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0772 - accuracy: 0.9775\n","Epoch 00806: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0772 - accuracy: 0.9775 - val_loss: 0.9233 - val_accuracy: 0.8438 - lr: 0.0035\n","Epoch 807/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0758 - accuracy: 0.9770\n","Epoch 00807: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 320ms/step - loss: 0.0758 - accuracy: 0.9770 - val_loss: 2.3952 - val_accuracy: 0.6131 - lr: 0.0040\n","Epoch 808/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.9790\n","Epoch 00808: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0826 - accuracy: 0.9790 - val_loss: 0.5787 - val_accuracy: 0.8993 - lr: 0.0040\n","Epoch 809/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9797\n","Epoch 00809: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0713 - accuracy: 0.9797 - val_loss: 0.5954 - val_accuracy: 0.9018 - lr: 0.0034\n","Epoch 810/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0725 - accuracy: 0.9787\n","Epoch 00810: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0725 - accuracy: 0.9787 - val_loss: 1.1586 - val_accuracy: 0.8214 - lr: 0.0029\n","Epoch 811/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0549 - accuracy: 0.9840\n","Epoch 00811: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0549 - accuracy: 0.9840 - val_loss: 0.1527 - val_accuracy: 0.9563 - lr: 0.0025\n","Epoch 812/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0416 - accuracy: 0.9855\n","Epoch 00812: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0416 - accuracy: 0.9855 - val_loss: 1.0649 - val_accuracy: 0.8438 - lr: 0.0021\n","Epoch 813/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0480 - accuracy: 0.9865\n","Epoch 00813: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0480 - accuracy: 0.9865 - val_loss: 0.8589 - val_accuracy: 0.8279 - lr: 0.0018\n","Epoch 814/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0349 - accuracy: 0.9897\n","Epoch 00814: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0349 - accuracy: 0.9897 - val_loss: 0.1254 - val_accuracy: 0.9623 - lr: 0.0015\n","Epoch 815/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0273 - accuracy: 0.9925\n","Epoch 00815: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0273 - accuracy: 0.9925 - val_loss: 0.2919 - val_accuracy: 0.9256 - lr: 0.0013\n","Epoch 816/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0306 - accuracy: 0.9907\n","Epoch 00816: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0306 - accuracy: 0.9907 - val_loss: 1.3977 - val_accuracy: 0.8229 - lr: 0.0011\n","Epoch 817/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0309 - accuracy: 0.9912\n","Epoch 00817: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 330ms/step - loss: 0.0309 - accuracy: 0.9912 - val_loss: 0.2104 - val_accuracy: 0.9653 - lr: 9.2647e-04\n","Epoch 818/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9917\n","Epoch 00818: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0246 - accuracy: 0.9917 - val_loss: 0.3753 - val_accuracy: 0.9554 - lr: 7.8750e-04\n","Epoch 819/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9950\n","Epoch 00819: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0207 - accuracy: 0.9950 - val_loss: 0.4177 - val_accuracy: 0.9504 - lr: 6.6937e-04\n","Epoch 820/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9937\n","Epoch 00820: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0220 - accuracy: 0.9937 - val_loss: 0.2113 - val_accuracy: 0.9474 - lr: 5.6897e-04\n","Epoch 821/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9910\n","Epoch 00821: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0244 - accuracy: 0.9910 - val_loss: 0.2269 - val_accuracy: 0.9449 - lr: 4.8362e-04\n","Epoch 822/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0168 - accuracy: 0.9950\n","Epoch 00822: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 333ms/step - loss: 0.0168 - accuracy: 0.9950 - val_loss: 0.1896 - val_accuracy: 0.9549 - lr: 4.1108e-04\n","Epoch 823/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0141 - accuracy: 0.9952\n","Epoch 00823: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0141 - accuracy: 0.9952 - val_loss: 0.1398 - val_accuracy: 0.9722 - lr: 3.4942e-04\n","Epoch 824/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0263 - accuracy: 0.9927\n","Epoch 00824: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0263 - accuracy: 0.9927 - val_loss: 0.1917 - val_accuracy: 0.9539 - lr: 2.9700e-04\n","Epoch 825/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9937\n","Epoch 00825: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0237 - accuracy: 0.9937 - val_loss: 0.1457 - val_accuracy: 0.9757 - lr: 2.5245e-04\n","Epoch 826/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0247 - accuracy: 0.9920\n","Epoch 00826: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0247 - accuracy: 0.9920 - val_loss: 1.5305 - val_accuracy: 0.8477 - lr: 0.0010\n","Epoch 827/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9917\n","Epoch 00827: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0236 - accuracy: 0.9917 - val_loss: 2.5994 - val_accuracy: 0.8259 - lr: 0.0015\n","Epoch 828/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9870\n","Epoch 00828: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0409 - accuracy: 0.9870 - val_loss: 0.3046 - val_accuracy: 0.9226 - lr: 0.0020\n","Epoch 829/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0452 - accuracy: 0.9852\n","Epoch 00829: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0452 - accuracy: 0.9852 - val_loss: 1.7714 - val_accuracy: 0.8438 - lr: 0.0025\n","Epoch 830/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0488 - accuracy: 0.9837\n","Epoch 00830: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0488 - accuracy: 0.9837 - val_loss: 2.5961 - val_accuracy: 0.7827 - lr: 0.0030\n","Epoch 831/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0611 - accuracy: 0.9787\n","Epoch 00831: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 332ms/step - loss: 0.0611 - accuracy: 0.9787 - val_loss: 18.8627 - val_accuracy: 0.6910 - lr: 0.0035\n","Epoch 832/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9810\n","Epoch 00832: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0697 - accuracy: 0.9810 - val_loss: 0.7748 - val_accuracy: 0.8289 - lr: 0.0040\n","Epoch 833/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0803 - accuracy: 0.9739\n","Epoch 00833: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0803 - accuracy: 0.9739 - val_loss: 2.6232 - val_accuracy: 0.7862 - lr: 0.0040\n","Epoch 834/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0621 - accuracy: 0.9790\n","Epoch 00834: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0621 - accuracy: 0.9790 - val_loss: 0.4425 - val_accuracy: 0.8626 - lr: 0.0034\n","Epoch 835/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0500 - accuracy: 0.9850\n","Epoch 00835: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0500 - accuracy: 0.9850 - val_loss: 3.3812 - val_accuracy: 0.7659 - lr: 0.0029\n","Epoch 836/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0468 - accuracy: 0.9855\n","Epoch 00836: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0468 - accuracy: 0.9855 - val_loss: 0.6467 - val_accuracy: 0.8189 - lr: 0.0025\n","Epoch 837/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9882\n","Epoch 00837: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0431 - accuracy: 0.9882 - val_loss: 0.7425 - val_accuracy: 0.8140 - lr: 0.0021\n","Epoch 838/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0352 - accuracy: 0.9877\n","Epoch 00838: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0352 - accuracy: 0.9877 - val_loss: 0.7954 - val_accuracy: 0.8244 - lr: 0.0018\n","Epoch 839/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0345 - accuracy: 0.9890\n","Epoch 00839: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0345 - accuracy: 0.9890 - val_loss: 1.6369 - val_accuracy: 0.8284 - lr: 0.0015\n","Epoch 840/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9892\n","Epoch 00840: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0322 - accuracy: 0.9892 - val_loss: 0.4350 - val_accuracy: 0.9013 - lr: 0.0013\n","Epoch 841/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0300 - accuracy: 0.9912\n","Epoch 00841: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0300 - accuracy: 0.9912 - val_loss: 1.4834 - val_accuracy: 0.8145 - lr: 0.0011\n","Epoch 842/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9940\n","Epoch 00842: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0200 - accuracy: 0.9940 - val_loss: 0.6335 - val_accuracy: 0.9350 - lr: 9.2647e-04\n","Epoch 843/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9905\n","Epoch 00843: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0319 - accuracy: 0.9905 - val_loss: 0.5073 - val_accuracy: 0.9350 - lr: 7.8750e-04\n","Epoch 844/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9932\n","Epoch 00844: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0212 - accuracy: 0.9932 - val_loss: 0.1147 - val_accuracy: 0.9678 - lr: 6.6937e-04\n","Epoch 845/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9947\n","Epoch 00845: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0205 - accuracy: 0.9947 - val_loss: 0.1433 - val_accuracy: 0.9722 - lr: 5.6897e-04\n","Epoch 846/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9940\n","Epoch 00846: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0186 - accuracy: 0.9940 - val_loss: 0.2353 - val_accuracy: 0.9444 - lr: 4.8362e-04\n","Epoch 847/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9932\n","Epoch 00847: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0222 - accuracy: 0.9932 - val_loss: 0.1329 - val_accuracy: 0.9712 - lr: 4.1108e-04\n","Epoch 848/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9937\n","Epoch 00848: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0205 - accuracy: 0.9937 - val_loss: 0.5382 - val_accuracy: 0.8760 - lr: 3.4942e-04\n","Epoch 849/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0165 - accuracy: 0.9950\n","Epoch 00849: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0165 - accuracy: 0.9950 - val_loss: 0.1017 - val_accuracy: 0.9782 - lr: 2.9700e-04\n","Epoch 850/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9937\n","Epoch 00850: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0169 - accuracy: 0.9937 - val_loss: 0.1059 - val_accuracy: 0.9826 - lr: 2.5245e-04\n","Epoch 851/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0259 - accuracy: 0.9917\n","Epoch 00851: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0259 - accuracy: 0.9917 - val_loss: 1.1692 - val_accuracy: 0.9157 - lr: 0.0010\n","Epoch 852/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0245 - accuracy: 0.9920\n","Epoch 00852: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0245 - accuracy: 0.9920 - val_loss: 0.9175 - val_accuracy: 0.8805 - lr: 0.0015\n","Epoch 853/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0421 - accuracy: 0.9870\n","Epoch 00853: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0421 - accuracy: 0.9870 - val_loss: 6.9161 - val_accuracy: 0.7500 - lr: 0.0020\n","Epoch 854/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0503 - accuracy: 0.9872\n","Epoch 00854: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0503 - accuracy: 0.9872 - val_loss: 0.7426 - val_accuracy: 0.8576 - lr: 0.0025\n","Epoch 855/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0481 - accuracy: 0.9852\n","Epoch 00855: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0481 - accuracy: 0.9852 - val_loss: 0.7489 - val_accuracy: 0.8338 - lr: 0.0030\n","Epoch 856/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0555 - accuracy: 0.9805\n","Epoch 00856: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0555 - accuracy: 0.9805 - val_loss: 63.6137 - val_accuracy: 0.5982 - lr: 0.0035\n","Epoch 857/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0696 - accuracy: 0.9810\n","Epoch 00857: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0696 - accuracy: 0.9810 - val_loss: 1.5781 - val_accuracy: 0.7316 - lr: 0.0040\n","Epoch 858/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0734 - accuracy: 0.9787\n","Epoch 00858: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0734 - accuracy: 0.9787 - val_loss: 0.9319 - val_accuracy: 0.7872 - lr: 0.0040\n","Epoch 859/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0645 - accuracy: 0.9797\n","Epoch 00859: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0645 - accuracy: 0.9797 - val_loss: 14.9220 - val_accuracy: 0.6106 - lr: 0.0034\n","Epoch 860/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0633 - accuracy: 0.9807\n","Epoch 00860: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0633 - accuracy: 0.9807 - val_loss: 0.4444 - val_accuracy: 0.8770 - lr: 0.0029\n","Epoch 861/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0434 - accuracy: 0.9865\n","Epoch 00861: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0434 - accuracy: 0.9865 - val_loss: 28.7643 - val_accuracy: 0.6022 - lr: 0.0025\n","Epoch 862/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9850\n","Epoch 00862: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0428 - accuracy: 0.9850 - val_loss: 2.0654 - val_accuracy: 0.8328 - lr: 0.0021\n","Epoch 863/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0390 - accuracy: 0.9890\n","Epoch 00863: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0390 - accuracy: 0.9890 - val_loss: 0.4927 - val_accuracy: 0.8333 - lr: 0.0018\n","Epoch 864/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9887\n","Epoch 00864: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0302 - accuracy: 0.9887 - val_loss: 0.2437 - val_accuracy: 0.9420 - lr: 0.0015\n","Epoch 865/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0272 - accuracy: 0.9915\n","Epoch 00865: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0272 - accuracy: 0.9915 - val_loss: 0.4377 - val_accuracy: 0.8606 - lr: 0.0013\n","Epoch 866/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0336 - accuracy: 0.9885\n","Epoch 00866: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0336 - accuracy: 0.9885 - val_loss: 0.9523 - val_accuracy: 0.7847 - lr: 0.0011\n","Epoch 867/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0233 - accuracy: 0.9930\n","Epoch 00867: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0233 - accuracy: 0.9930 - val_loss: 0.2203 - val_accuracy: 0.9157 - lr: 9.2647e-04\n","Epoch 868/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0299 - accuracy: 0.9900\n","Epoch 00868: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0299 - accuracy: 0.9900 - val_loss: 0.2039 - val_accuracy: 0.9608 - lr: 7.8750e-04\n","Epoch 869/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0216 - accuracy: 0.9925\n","Epoch 00869: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0216 - accuracy: 0.9925 - val_loss: 0.6849 - val_accuracy: 0.8542 - lr: 6.6937e-04\n","Epoch 870/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0160 - accuracy: 0.9945\n","Epoch 00870: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0160 - accuracy: 0.9945 - val_loss: 0.3220 - val_accuracy: 0.9018 - lr: 5.6897e-04\n","Epoch 871/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0281 - accuracy: 0.9925\n","Epoch 00871: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0281 - accuracy: 0.9925 - val_loss: 0.1230 - val_accuracy: 0.9782 - lr: 4.8362e-04\n","Epoch 872/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9947\n","Epoch 00872: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0172 - accuracy: 0.9947 - val_loss: 1.4371 - val_accuracy: 0.8611 - lr: 4.1108e-04\n","Epoch 873/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9952\n","Epoch 00873: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0126 - accuracy: 0.9952 - val_loss: 0.8190 - val_accuracy: 0.9182 - lr: 3.4942e-04\n","Epoch 874/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9947\n","Epoch 00874: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0150 - accuracy: 0.9947 - val_loss: 0.1476 - val_accuracy: 0.9767 - lr: 2.9700e-04\n","Epoch 875/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9940\n","Epoch 00875: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0179 - accuracy: 0.9940 - val_loss: 0.1275 - val_accuracy: 0.9792 - lr: 2.5245e-04\n","Epoch 876/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9950\n","Epoch 00876: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0167 - accuracy: 0.9950 - val_loss: 0.1344 - val_accuracy: 0.9648 - lr: 0.0010\n","Epoch 877/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0298 - accuracy: 0.9895\n","Epoch 00877: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0298 - accuracy: 0.9895 - val_loss: 1.1272 - val_accuracy: 0.9251 - lr: 0.0015\n","Epoch 878/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0315 - accuracy: 0.9900\n","Epoch 00878: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0315 - accuracy: 0.9900 - val_loss: 27.5446 - val_accuracy: 0.6438 - lr: 0.0020\n","Epoch 879/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0329 - accuracy: 0.9902\n","Epoch 00879: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0329 - accuracy: 0.9902 - val_loss: 23.7973 - val_accuracy: 0.7366 - lr: 0.0025\n","Epoch 880/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0431 - accuracy: 0.9885\n","Epoch 00880: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 321ms/step - loss: 0.0431 - accuracy: 0.9885 - val_loss: 1.2135 - val_accuracy: 0.7381 - lr: 0.0030\n","Epoch 881/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0643 - accuracy: 0.9787\n","Epoch 00881: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0643 - accuracy: 0.9787 - val_loss: 1.7177 - val_accuracy: 0.7981 - lr: 0.0035\n","Epoch 882/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0730 - accuracy: 0.9797\n","Epoch 00882: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0730 - accuracy: 0.9797 - val_loss: 2.4370 - val_accuracy: 0.7639 - lr: 0.0040\n","Epoch 883/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0700 - accuracy: 0.9795\n","Epoch 00883: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0700 - accuracy: 0.9795 - val_loss: 12.4999 - val_accuracy: 0.6865 - lr: 0.0040\n","Epoch 884/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0564 - accuracy: 0.9827\n","Epoch 00884: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0564 - accuracy: 0.9827 - val_loss: 0.2754 - val_accuracy: 0.9509 - lr: 0.0034\n","Epoch 885/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0473 - accuracy: 0.9855\n","Epoch 00885: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0473 - accuracy: 0.9855 - val_loss: 0.9325 - val_accuracy: 0.8353 - lr: 0.0029\n","Epoch 886/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0506 - accuracy: 0.9840\n","Epoch 00886: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0506 - accuracy: 0.9840 - val_loss: 1.0940 - val_accuracy: 0.7098 - lr: 0.0025\n","Epoch 887/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0427 - accuracy: 0.9882\n","Epoch 00887: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0427 - accuracy: 0.9882 - val_loss: 1.7527 - val_accuracy: 0.7222 - lr: 0.0021\n","Epoch 888/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0331 - accuracy: 0.9887\n","Epoch 00888: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0331 - accuracy: 0.9887 - val_loss: 0.3442 - val_accuracy: 0.8958 - lr: 0.0018\n","Epoch 889/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0310 - accuracy: 0.9892\n","Epoch 00889: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0310 - accuracy: 0.9892 - val_loss: 0.3233 - val_accuracy: 0.9058 - lr: 0.0015\n","Epoch 890/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0244 - accuracy: 0.9917\n","Epoch 00890: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0244 - accuracy: 0.9917 - val_loss: 0.5190 - val_accuracy: 0.8745 - lr: 0.0013\n","Epoch 891/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0267 - accuracy: 0.9910\n","Epoch 00891: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0267 - accuracy: 0.9910 - val_loss: 0.1605 - val_accuracy: 0.9559 - lr: 0.0011\n","Epoch 892/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0214 - accuracy: 0.9917\n","Epoch 00892: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 328ms/step - loss: 0.0214 - accuracy: 0.9917 - val_loss: 0.2206 - val_accuracy: 0.9484 - lr: 9.2647e-04\n","Epoch 893/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0236 - accuracy: 0.9927\n","Epoch 00893: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0236 - accuracy: 0.9927 - val_loss: 0.4968 - val_accuracy: 0.9534 - lr: 7.8750e-04\n","Epoch 894/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0212 - accuracy: 0.9927\n","Epoch 00894: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0212 - accuracy: 0.9927 - val_loss: 1.9761 - val_accuracy: 0.8547 - lr: 6.6937e-04\n","Epoch 895/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9935\n","Epoch 00895: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0225 - accuracy: 0.9935 - val_loss: 0.1578 - val_accuracy: 0.9727 - lr: 5.6897e-04\n","Epoch 896/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9945\n","Epoch 00896: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 321ms/step - loss: 0.0169 - accuracy: 0.9945 - val_loss: 0.2060 - val_accuracy: 0.9444 - lr: 4.8362e-04\n","Epoch 897/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0213 - accuracy: 0.9927\n","Epoch 00897: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0213 - accuracy: 0.9927 - val_loss: 0.2138 - val_accuracy: 0.9320 - lr: 4.1108e-04\n","Epoch 898/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9952\n","Epoch 00898: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 328ms/step - loss: 0.0147 - accuracy: 0.9952 - val_loss: 0.1751 - val_accuracy: 0.9722 - lr: 3.4942e-04\n","Epoch 899/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0173 - accuracy: 0.9947\n","Epoch 00899: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0173 - accuracy: 0.9947 - val_loss: 0.1241 - val_accuracy: 0.9792 - lr: 2.9700e-04\n","Epoch 900/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9947\n","Epoch 00900: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0169 - accuracy: 0.9947 - val_loss: 0.1526 - val_accuracy: 0.9762 - lr: 2.5245e-04\n","Epoch 901/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9925\n","Epoch 00901: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 319ms/step - loss: 0.0231 - accuracy: 0.9925 - val_loss: 2.4268 - val_accuracy: 0.8413 - lr: 0.0010\n","Epoch 902/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9925\n","Epoch 00902: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0290 - accuracy: 0.9925 - val_loss: 1.4588 - val_accuracy: 0.8239 - lr: 0.0015\n","Epoch 903/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0351 - accuracy: 0.9910\n","Epoch 00903: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0351 - accuracy: 0.9910 - val_loss: 0.7914 - val_accuracy: 0.8512 - lr: 0.0020\n","Epoch 904/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0405 - accuracy: 0.9880\n","Epoch 00904: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 323ms/step - loss: 0.0405 - accuracy: 0.9880 - val_loss: 0.3104 - val_accuracy: 0.9276 - lr: 0.0025\n","Epoch 905/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0397 - accuracy: 0.9865\n","Epoch 00905: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0397 - accuracy: 0.9865 - val_loss: 2.0398 - val_accuracy: 0.8740 - lr: 0.0030\n","Epoch 906/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0466 - accuracy: 0.9842\n","Epoch 00906: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0466 - accuracy: 0.9842 - val_loss: 1.5241 - val_accuracy: 0.7594 - lr: 0.0035\n","Epoch 907/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0740 - accuracy: 0.9797\n","Epoch 00907: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0740 - accuracy: 0.9797 - val_loss: 2.9349 - val_accuracy: 0.7083 - lr: 0.0040\n","Epoch 908/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0769 - accuracy: 0.9775\n","Epoch 00908: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0769 - accuracy: 0.9775 - val_loss: 2.2075 - val_accuracy: 0.8259 - lr: 0.0040\n","Epoch 909/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0750 - accuracy: 0.9815\n","Epoch 00909: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0750 - accuracy: 0.9815 - val_loss: 0.5514 - val_accuracy: 0.9206 - lr: 0.0034\n","Epoch 910/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0528 - accuracy: 0.9817\n","Epoch 00910: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0528 - accuracy: 0.9817 - val_loss: 0.2018 - val_accuracy: 0.9509 - lr: 0.0029\n","Epoch 911/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0543 - accuracy: 0.9842\n","Epoch 00911: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0543 - accuracy: 0.9842 - val_loss: 2.8272 - val_accuracy: 0.8601 - lr: 0.0025\n","Epoch 912/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0361 - accuracy: 0.9890\n","Epoch 00912: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0361 - accuracy: 0.9890 - val_loss: 0.1566 - val_accuracy: 0.9608 - lr: 0.0021\n","Epoch 913/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0313 - accuracy: 0.9882\n","Epoch 00913: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0313 - accuracy: 0.9882 - val_loss: 20.2920 - val_accuracy: 0.7173 - lr: 0.0018\n","Epoch 914/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0353 - accuracy: 0.9905\n","Epoch 00914: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 323ms/step - loss: 0.0353 - accuracy: 0.9905 - val_loss: 0.1573 - val_accuracy: 0.9663 - lr: 0.0015\n","Epoch 915/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0266 - accuracy: 0.9897\n","Epoch 00915: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0266 - accuracy: 0.9897 - val_loss: 1.0863 - val_accuracy: 0.8204 - lr: 0.0013\n","Epoch 916/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9935\n","Epoch 00916: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0200 - accuracy: 0.9935 - val_loss: 0.2976 - val_accuracy: 0.9067 - lr: 0.0011\n","Epoch 917/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9925\n","Epoch 00917: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 323ms/step - loss: 0.0254 - accuracy: 0.9925 - val_loss: 0.1865 - val_accuracy: 0.9484 - lr: 9.2647e-04\n","Epoch 918/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0262 - accuracy: 0.9912\n","Epoch 00918: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0262 - accuracy: 0.9912 - val_loss: 2.2995 - val_accuracy: 0.7837 - lr: 7.8750e-04\n","Epoch 919/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9942\n","Epoch 00919: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0174 - accuracy: 0.9942 - val_loss: 0.1235 - val_accuracy: 0.9678 - lr: 6.6937e-04\n","Epoch 920/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0171 - accuracy: 0.9960\n","Epoch 00920: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0171 - accuracy: 0.9960 - val_loss: 0.1346 - val_accuracy: 0.9742 - lr: 5.6897e-04\n","Epoch 921/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9940\n","Epoch 00921: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0151 - accuracy: 0.9940 - val_loss: 0.1650 - val_accuracy: 0.9702 - lr: 4.8362e-04\n","Epoch 922/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9922\n","Epoch 00922: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0194 - accuracy: 0.9922 - val_loss: 0.2609 - val_accuracy: 0.9653 - lr: 4.1108e-04\n","Epoch 923/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0169 - accuracy: 0.9950\n","Epoch 00923: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 329ms/step - loss: 0.0169 - accuracy: 0.9950 - val_loss: 0.1400 - val_accuracy: 0.9692 - lr: 3.4942e-04\n","Epoch 924/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0149 - accuracy: 0.9945\n","Epoch 00924: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0149 - accuracy: 0.9945 - val_loss: 0.1256 - val_accuracy: 0.9797 - lr: 2.9700e-04\n","Epoch 925/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0194 - accuracy: 0.9937\n","Epoch 00925: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0194 - accuracy: 0.9937 - val_loss: 0.1167 - val_accuracy: 0.9802 - lr: 2.5245e-04\n","Epoch 926/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0227 - accuracy: 0.9930\n","Epoch 00926: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 328ms/step - loss: 0.0227 - accuracy: 0.9930 - val_loss: 0.7731 - val_accuracy: 0.8204 - lr: 0.0010\n","Epoch 927/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9932\n","Epoch 00927: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0275 - accuracy: 0.9932 - val_loss: 0.2109 - val_accuracy: 0.9618 - lr: 0.0015\n","Epoch 928/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.9892\n","Epoch 00928: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0320 - accuracy: 0.9892 - val_loss: 0.6575 - val_accuracy: 0.8318 - lr: 0.0020\n","Epoch 929/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0369 - accuracy: 0.9887\n","Epoch 00929: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0369 - accuracy: 0.9887 - val_loss: 2.0746 - val_accuracy: 0.7956 - lr: 0.0025\n","Epoch 930/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0489 - accuracy: 0.9850\n","Epoch 00930: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0489 - accuracy: 0.9850 - val_loss: 1.3872 - val_accuracy: 0.8626 - lr: 0.0030\n","Epoch 931/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0655 - accuracy: 0.9792\n","Epoch 00931: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 339ms/step - loss: 0.0655 - accuracy: 0.9792 - val_loss: 6.8719 - val_accuracy: 0.8209 - lr: 0.0035\n","Epoch 932/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0581 - accuracy: 0.9822\n","Epoch 00932: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0581 - accuracy: 0.9822 - val_loss: 0.7206 - val_accuracy: 0.8661 - lr: 0.0040\n","Epoch 933/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0713 - accuracy: 0.9765\n","Epoch 00933: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0713 - accuracy: 0.9765 - val_loss: 25.2804 - val_accuracy: 0.6265 - lr: 0.0040\n","Epoch 934/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0785 - accuracy: 0.9807\n","Epoch 00934: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 323ms/step - loss: 0.0785 - accuracy: 0.9807 - val_loss: 2.7900 - val_accuracy: 0.7510 - lr: 0.0034\n","Epoch 935/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0609 - accuracy: 0.9790\n","Epoch 00935: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 330ms/step - loss: 0.0609 - accuracy: 0.9790 - val_loss: 0.6610 - val_accuracy: 0.8021 - lr: 0.0029\n","Epoch 936/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0445 - accuracy: 0.9882\n","Epoch 00936: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0445 - accuracy: 0.9882 - val_loss: 1.0385 - val_accuracy: 0.8026 - lr: 0.0025\n","Epoch 937/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0487 - accuracy: 0.9862\n","Epoch 00937: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0487 - accuracy: 0.9862 - val_loss: 0.2741 - val_accuracy: 0.9454 - lr: 0.0021\n","Epoch 938/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0270 - accuracy: 0.9912\n","Epoch 00938: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0270 - accuracy: 0.9912 - val_loss: 0.9379 - val_accuracy: 0.8189 - lr: 0.0018\n","Epoch 939/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0335 - accuracy: 0.9877\n","Epoch 00939: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 330ms/step - loss: 0.0335 - accuracy: 0.9877 - val_loss: 0.3294 - val_accuracy: 0.8765 - lr: 0.0015\n","Epoch 940/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0258 - accuracy: 0.9915\n","Epoch 00940: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0258 - accuracy: 0.9915 - val_loss: 0.2305 - val_accuracy: 0.9499 - lr: 0.0013\n","Epoch 941/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0275 - accuracy: 0.9907\n","Epoch 00941: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0275 - accuracy: 0.9907 - val_loss: 0.6351 - val_accuracy: 0.8467 - lr: 0.0011\n","Epoch 942/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0228 - accuracy: 0.9925\n","Epoch 00942: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0228 - accuracy: 0.9925 - val_loss: 1.0753 - val_accuracy: 0.8442 - lr: 9.2647e-04\n","Epoch 943/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0179 - accuracy: 0.9932\n","Epoch 00943: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0179 - accuracy: 0.9932 - val_loss: 0.3089 - val_accuracy: 0.9301 - lr: 7.8750e-04\n","Epoch 944/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0285 - accuracy: 0.9910\n","Epoch 00944: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0285 - accuracy: 0.9910 - val_loss: 1.1890 - val_accuracy: 0.8557 - lr: 6.6937e-04\n","Epoch 945/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0234 - accuracy: 0.9925\n","Epoch 00945: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 331ms/step - loss: 0.0234 - accuracy: 0.9925 - val_loss: 0.5751 - val_accuracy: 0.8800 - lr: 5.6897e-04\n","Epoch 946/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0220 - accuracy: 0.9937\n","Epoch 00946: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 329ms/step - loss: 0.0220 - accuracy: 0.9937 - val_loss: 0.1354 - val_accuracy: 0.9608 - lr: 4.8362e-04\n","Epoch 947/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0203 - accuracy: 0.9940\n","Epoch 00947: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0203 - accuracy: 0.9940 - val_loss: 0.1732 - val_accuracy: 0.9420 - lr: 4.1108e-04\n","Epoch 948/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0186 - accuracy: 0.9947\n","Epoch 00948: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0186 - accuracy: 0.9947 - val_loss: 0.1488 - val_accuracy: 0.9683 - lr: 3.4942e-04\n","Epoch 949/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0237 - accuracy: 0.9935\n","Epoch 00949: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0237 - accuracy: 0.9935 - val_loss: 0.1496 - val_accuracy: 0.9737 - lr: 2.9700e-04\n","Epoch 950/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0150 - accuracy: 0.9960\n","Epoch 00950: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 329ms/step - loss: 0.0150 - accuracy: 0.9960 - val_loss: 0.2957 - val_accuracy: 0.9360 - lr: 2.5245e-04\n","Epoch 951/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9925\n","Epoch 00951: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0204 - accuracy: 0.9925 - val_loss: 0.2637 - val_accuracy: 0.9573 - lr: 0.0010\n","Epoch 952/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0231 - accuracy: 0.9930\n","Epoch 00952: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0231 - accuracy: 0.9930 - val_loss: 0.2305 - val_accuracy: 0.9474 - lr: 0.0015\n","Epoch 953/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0367 - accuracy: 0.9900\n","Epoch 00953: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0367 - accuracy: 0.9900 - val_loss: 1.6308 - val_accuracy: 0.8378 - lr: 0.0020\n","Epoch 954/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0330 - accuracy: 0.9887\n","Epoch 00954: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0330 - accuracy: 0.9887 - val_loss: 1.1647 - val_accuracy: 0.8105 - lr: 0.0025\n","Epoch 955/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0419 - accuracy: 0.9862\n","Epoch 00955: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 328ms/step - loss: 0.0419 - accuracy: 0.9862 - val_loss: 0.8422 - val_accuracy: 0.9087 - lr: 0.0030\n","Epoch 956/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0699 - accuracy: 0.9800\n","Epoch 00956: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0699 - accuracy: 0.9800 - val_loss: 103.7965 - val_accuracy: 0.4970 - lr: 0.0035\n","Epoch 957/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0705 - accuracy: 0.9792\n","Epoch 00957: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0705 - accuracy: 0.9792 - val_loss: 0.6512 - val_accuracy: 0.8854 - lr: 0.0040\n","Epoch 958/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0731 - accuracy: 0.9785\n","Epoch 00958: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0731 - accuracy: 0.9785 - val_loss: 0.6023 - val_accuracy: 0.8671 - lr: 0.0040\n","Epoch 959/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0547 - accuracy: 0.9830\n","Epoch 00959: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0547 - accuracy: 0.9830 - val_loss: 11.0762 - val_accuracy: 0.6587 - lr: 0.0034\n","Epoch 960/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0428 - accuracy: 0.9867\n","Epoch 00960: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 329ms/step - loss: 0.0428 - accuracy: 0.9867 - val_loss: 1.1415 - val_accuracy: 0.7728 - lr: 0.0029\n","Epoch 961/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0374 - accuracy: 0.9882\n","Epoch 00961: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 329ms/step - loss: 0.0374 - accuracy: 0.9882 - val_loss: 0.8492 - val_accuracy: 0.8413 - lr: 0.0025\n","Epoch 962/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0339 - accuracy: 0.9882\n","Epoch 00962: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0339 - accuracy: 0.9882 - val_loss: 0.1908 - val_accuracy: 0.9504 - lr: 0.0021\n","Epoch 963/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0322 - accuracy: 0.9900\n","Epoch 00963: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0322 - accuracy: 0.9900 - val_loss: 0.2862 - val_accuracy: 0.9291 - lr: 0.0018\n","Epoch 964/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0389 - accuracy: 0.9887\n","Epoch 00964: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0389 - accuracy: 0.9887 - val_loss: 0.2553 - val_accuracy: 0.9187 - lr: 0.0015\n","Epoch 965/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0321 - accuracy: 0.9915\n","Epoch 00965: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0321 - accuracy: 0.9915 - val_loss: 0.9850 - val_accuracy: 0.7877 - lr: 0.0013\n","Epoch 966/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0319 - accuracy: 0.9905\n","Epoch 00966: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0319 - accuracy: 0.9905 - val_loss: 1.0323 - val_accuracy: 0.8110 - lr: 0.0011\n","Epoch 967/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0215 - accuracy: 0.9922\n","Epoch 00967: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0215 - accuracy: 0.9922 - val_loss: 2.3383 - val_accuracy: 0.8239 - lr: 9.2647e-04\n","Epoch 968/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0210 - accuracy: 0.9932\n","Epoch 00968: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0210 - accuracy: 0.9932 - val_loss: 0.1753 - val_accuracy: 0.9727 - lr: 7.8750e-04\n","Epoch 969/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0205 - accuracy: 0.9932\n","Epoch 00969: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 342ms/step - loss: 0.0205 - accuracy: 0.9932 - val_loss: 0.1703 - val_accuracy: 0.9683 - lr: 6.6937e-04\n","Epoch 970/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0108 - accuracy: 0.9955\n","Epoch 00970: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0108 - accuracy: 0.9955 - val_loss: 0.1627 - val_accuracy: 0.9722 - lr: 5.6897e-04\n","Epoch 971/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0191 - accuracy: 0.9950\n","Epoch 00971: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0191 - accuracy: 0.9950 - val_loss: 0.1608 - val_accuracy: 0.9722 - lr: 4.8362e-04\n","Epoch 972/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0207 - accuracy: 0.9932\n","Epoch 00972: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0207 - accuracy: 0.9932 - val_loss: 0.2935 - val_accuracy: 0.9236 - lr: 4.1108e-04\n","Epoch 973/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0156 - accuracy: 0.9955\n","Epoch 00973: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0156 - accuracy: 0.9955 - val_loss: 0.1973 - val_accuracy: 0.9742 - lr: 3.4942e-04\n","Epoch 974/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0201 - accuracy: 0.9942\n","Epoch 00974: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0201 - accuracy: 0.9942 - val_loss: 0.1556 - val_accuracy: 0.9737 - lr: 2.9700e-04\n","Epoch 975/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0126 - accuracy: 0.9957\n","Epoch 00975: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 328ms/step - loss: 0.0126 - accuracy: 0.9957 - val_loss: 0.1318 - val_accuracy: 0.9697 - lr: 2.5245e-04\n","Epoch 976/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0301 - accuracy: 0.9910\n","Epoch 00976: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 330ms/step - loss: 0.0301 - accuracy: 0.9910 - val_loss: 0.2723 - val_accuracy: 0.9638 - lr: 0.0010\n","Epoch 977/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0290 - accuracy: 0.9912\n","Epoch 00977: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0290 - accuracy: 0.9912 - val_loss: 2.9007 - val_accuracy: 0.7555 - lr: 0.0015\n","Epoch 978/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9915\n","Epoch 00978: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 325ms/step - loss: 0.0317 - accuracy: 0.9915 - val_loss: 0.8909 - val_accuracy: 0.8016 - lr: 0.0020\n","Epoch 979/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9897\n","Epoch 00979: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0347 - accuracy: 0.9897 - val_loss: 1.2552 - val_accuracy: 0.8051 - lr: 0.0025\n","Epoch 980/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0442 - accuracy: 0.9877\n","Epoch 00980: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0442 - accuracy: 0.9877 - val_loss: 2.5238 - val_accuracy: 0.7937 - lr: 0.0030\n","Epoch 981/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 0.9815\n","Epoch 00981: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 328ms/step - loss: 0.0636 - accuracy: 0.9815 - val_loss: 1.8306 - val_accuracy: 0.7197 - lr: 0.0035\n","Epoch 982/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9800\n","Epoch 00982: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 325ms/step - loss: 0.0718 - accuracy: 0.9800 - val_loss: 2.3391 - val_accuracy: 0.7941 - lr: 0.0040\n","Epoch 983/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0767 - accuracy: 0.9772\n","Epoch 00983: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0767 - accuracy: 0.9772 - val_loss: 9.7191 - val_accuracy: 0.6285 - lr: 0.0040\n","Epoch 984/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0687 - accuracy: 0.9787\n","Epoch 00984: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 324ms/step - loss: 0.0687 - accuracy: 0.9787 - val_loss: 2.2204 - val_accuracy: 0.7411 - lr: 0.0034\n","Epoch 985/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0486 - accuracy: 0.9845\n","Epoch 00985: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0486 - accuracy: 0.9845 - val_loss: 0.5786 - val_accuracy: 0.8705 - lr: 0.0029\n","Epoch 986/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0449 - accuracy: 0.9862\n","Epoch 00986: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0449 - accuracy: 0.9862 - val_loss: 1.5099 - val_accuracy: 0.7555 - lr: 0.0025\n","Epoch 987/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0304 - accuracy: 0.9915\n","Epoch 00987: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0304 - accuracy: 0.9915 - val_loss: 1.5214 - val_accuracy: 0.8145 - lr: 0.0021\n","Epoch 988/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0409 - accuracy: 0.9887\n","Epoch 00988: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0409 - accuracy: 0.9887 - val_loss: 1.5972 - val_accuracy: 0.8204 - lr: 0.0018\n","Epoch 989/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0172 - accuracy: 0.9942\n","Epoch 00989: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0172 - accuracy: 0.9942 - val_loss: 0.1901 - val_accuracy: 0.9554 - lr: 0.0015\n","Epoch 990/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0347 - accuracy: 0.9905\n","Epoch 00990: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0347 - accuracy: 0.9905 - val_loss: 0.3109 - val_accuracy: 0.9241 - lr: 0.0013\n","Epoch 991/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9907\n","Epoch 00991: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 326ms/step - loss: 0.0302 - accuracy: 0.9907 - val_loss: 0.2204 - val_accuracy: 0.9663 - lr: 0.0011\n","Epoch 992/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0254 - accuracy: 0.9920\n","Epoch 00992: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 326ms/step - loss: 0.0254 - accuracy: 0.9920 - val_loss: 3.0060 - val_accuracy: 0.8472 - lr: 9.2647e-04\n","Epoch 993/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0204 - accuracy: 0.9937\n","Epoch 00993: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0204 - accuracy: 0.9937 - val_loss: 0.5123 - val_accuracy: 0.8710 - lr: 7.8750e-04\n","Epoch 994/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0174 - accuracy: 0.9940\n","Epoch 00994: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 323ms/step - loss: 0.0174 - accuracy: 0.9940 - val_loss: 0.1542 - val_accuracy: 0.9707 - lr: 6.6937e-04\n","Epoch 995/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0177 - accuracy: 0.9937\n","Epoch 00995: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 322ms/step - loss: 0.0177 - accuracy: 0.9937 - val_loss: 0.1434 - val_accuracy: 0.9722 - lr: 5.6897e-04\n","Epoch 996/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0219 - accuracy: 0.9937\n","Epoch 00996: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 327ms/step - loss: 0.0219 - accuracy: 0.9937 - val_loss: 0.1326 - val_accuracy: 0.9663 - lr: 4.8362e-04\n","Epoch 997/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0154 - accuracy: 0.9950\n","Epoch 00997: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0154 - accuracy: 0.9950 - val_loss: 0.2472 - val_accuracy: 0.9335 - lr: 4.1108e-04\n","Epoch 998/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0151 - accuracy: 0.9952\n","Epoch 00998: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 327ms/step - loss: 0.0151 - accuracy: 0.9952 - val_loss: 0.1888 - val_accuracy: 0.9573 - lr: 3.4942e-04\n","Epoch 999/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0147 - accuracy: 0.9945\n","Epoch 00999: val_loss did not improve from 0.09610\n","63/63 [==============================] - 22s 324ms/step - loss: 0.0147 - accuracy: 0.9945 - val_loss: 0.1199 - val_accuracy: 0.9777 - lr: 2.9700e-04\n","Epoch 1000/1000\n","63/63 [==============================] - ETA: 0s - loss: 0.0153 - accuracy: 0.9935\n","Epoch 01000: val_loss did not improve from 0.09610\n","63/63 [==============================] - 23s 328ms/step - loss: 0.0153 - accuracy: 0.9935 - val_loss: 0.1381 - val_accuracy: 0.9683 - lr: 2.5245e-04\n","Best Model Results: /content/drive/MyDrive/output/JP30N02--1-best_result.hdf5\n","8/8 [==============================] - 2s 97ms/step - loss: 0.1031 - accuracy: 0.9762\n","loss 0.10308942943811417\n","acc 0.976190447807312\n","Finished: /content/drive/MyDrive/output/JP30N02--1\n"]},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}],"source":["work_on_keffnet(show_model=False, run_fit=True, test_results=True)"]},{"cell_type":"markdown","metadata":{"id":"qm50d5uZxkvA"},"source":["# Test Results"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"sGjwYVi6TNN_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644823403667,"user_tz":180,"elapsed":4183,"user":{"displayName":"Joao Schuler","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10970916006391371112"}},"outputId":"32bd654e-732e-4b46-8815-c2eadd3722d6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running: /content/drive/MyDrive/output/JP30N02--1\n","Best Model Results: /content/drive/MyDrive/output/JP30N02--1-best_result.hdf5\n","8/8 [==============================] - 2s 55ms/step - loss: 0.1031 - accuracy: 0.9762\n","loss 0.10308942943811417\n","acc 0.976190447807312\n","Finished: /content/drive/MyDrive/output/JP30N02--1\n"]}],"source":["work_on_keffnet(show_model=False, run_fit=False, test_results=True)"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"qYPjSr31VdXG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1644823413316,"user_tz":180,"elapsed":9289,"user":{"displayName":"Joao Schuler","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"10970916006391371112"}},"outputId":"db9097ff-be2f-45cb-e0b3-b968a81af256"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running: /content/drive/MyDrive/output/JP30N02--1\n","Test Original\n","              precision    recall  f1-score   support\n","\n","           0     0.9683    0.9683    0.9683        63\n","           1     0.9841    0.9841    0.9841        63\n","           2     0.9355    0.9206    0.9280        63\n","           3     0.9242    0.9683    0.9457        63\n","           4     1.0000    0.9683    0.9839        63\n","           5     1.0000    1.0000    1.0000        63\n","           6     1.0000    1.0000    1.0000        63\n","           7     1.0000    1.0000    1.0000        63\n","\n","    accuracy                         0.9762       504\n","   macro avg     0.9765    0.9762    0.9762       504\n","weighted avg     0.9765    0.9762    0.9762       504\n","\n","Test Flip X\n","              precision    recall  f1-score   support\n","\n","           0     0.9683    0.9683    0.9683        63\n","           1     1.0000    1.0000    1.0000        63\n","           2     0.9206    0.9206    0.9206        63\n","           3     0.9385    0.9683    0.9531        63\n","           4     1.0000    0.9683    0.9839        63\n","           5     1.0000    1.0000    1.0000        63\n","           6     1.0000    1.0000    1.0000        63\n","           7     1.0000    1.0000    1.0000        63\n","\n","    accuracy                         0.9782       504\n","   macro avg     0.9784    0.9782    0.9782       504\n","weighted avg     0.9784    0.9782    0.9782       504\n","\n","Test Original + Flip X\n","              precision    recall  f1-score   support\n","\n","           0     0.9683    0.9683    0.9683        63\n","           1     1.0000    1.0000    1.0000        63\n","           2     0.9206    0.9206    0.9206        63\n","           3     0.9385    0.9683    0.9531        63\n","           4     1.0000    0.9683    0.9839        63\n","           5     1.0000    1.0000    1.0000        63\n","           6     1.0000    1.0000    1.0000        63\n","           7     1.0000    1.0000    1.0000        63\n","\n","    accuracy                         0.9782       504\n","   macro avg     0.9784    0.9782    0.9782       504\n","weighted avg     0.9784    0.9782    0.9782       504\n","\n","Test Flip Y\n","              precision    recall  f1-score   support\n","\n","           0     0.9683    0.9683    0.9683        63\n","           1     0.9692    1.0000    0.9844        63\n","           2     0.9344    0.9048    0.9194        63\n","           3     0.9385    0.9683    0.9531        63\n","           4     1.0000    0.9683    0.9839        63\n","           5     1.0000    1.0000    1.0000        63\n","           6     1.0000    1.0000    1.0000        63\n","           7     1.0000    1.0000    1.0000        63\n","\n","    accuracy                         0.9762       504\n","   macro avg     0.9763    0.9762    0.9761       504\n","weighted avg     0.9763    0.9762    0.9761       504\n","\n","Test Original + Flip Y\n","              precision    recall  f1-score   support\n","\n","           0     0.9683    0.9683    0.9683        63\n","           1     0.9841    0.9841    0.9841        63\n","           2     0.9355    0.9206    0.9280        63\n","           3     0.9242    0.9683    0.9457        63\n","           4     1.0000    0.9683    0.9839        63\n","           5     1.0000    1.0000    1.0000        63\n","           6     1.0000    1.0000    1.0000        63\n","           7     1.0000    1.0000    1.0000        63\n","\n","    accuracy                         0.9762       504\n","   macro avg     0.9765    0.9762    0.9762       504\n","weighted avg     0.9765    0.9762    0.9762       504\n","\n","Test Original + Flip X + Flip Y\n","              precision    recall  f1-score   support\n","\n","           0     0.9683    0.9683    0.9683        63\n","           1     0.9844    1.0000    0.9921        63\n","           2     0.9206    0.9206    0.9206        63\n","           3     0.9385    0.9683    0.9531        63\n","           4     1.0000    0.9524    0.9756        63\n","           5     1.0000    1.0000    1.0000        63\n","           6     1.0000    1.0000    1.0000        63\n","           7     1.0000    1.0000    1.0000        63\n","\n","    accuracy                         0.9762       504\n","   macro avg     0.9765    0.9762    0.9762       504\n","weighted avg     0.9765    0.9762    0.9762       504\n","\n","Cropped and Resized\n","Cropped shape: (504, 192, 192, 3)\n","              precision    recall  f1-score   support\n","\n","           0     0.9231    0.9524    0.9375        63\n","           1     0.9844    1.0000    0.9921        63\n","           2     0.8730    0.8730    0.8730        63\n","           3     0.9231    0.9524    0.9375        63\n","           4     1.0000    0.9365    0.9672        63\n","           5     1.0000    0.9841    0.9920        63\n","           6     1.0000    0.9841    0.9920        63\n","           7     0.9844    1.0000    0.9921        63\n","\n","    accuracy                         0.9603       504\n","   macro avg     0.9610    0.9603    0.9604       504\n","weighted avg     0.9610    0.9603    0.9604       504\n","\n","Original + Cropped Resized\n","              precision    recall  f1-score   support\n","\n","           0     0.9683    0.9683    0.9683        63\n","           1     0.9841    0.9841    0.9841        63\n","           2     0.9194    0.9048    0.9120        63\n","           3     0.9104    0.9683    0.9385        63\n","           4     1.0000    0.9524    0.9756        63\n","           5     1.0000    1.0000    1.0000        63\n","           6     1.0000    0.9841    0.9920        63\n","           7     0.9844    1.0000    0.9921        63\n","\n","    accuracy                         0.9702       504\n","   macro avg     0.9708    0.9702    0.9703       504\n","weighted avg     0.9708    0.9702    0.9703       504\n","\n","Original + Flip X + Cropped Resized\n","              precision    recall  f1-score   support\n","\n","           0     0.9683    0.9683    0.9683        63\n","           1     0.9844    1.0000    0.9921        63\n","           2     0.9206    0.9206    0.9206        63\n","           3     0.9385    0.9683    0.9531        63\n","           4     1.0000    0.9524    0.9756        63\n","           5     1.0000    1.0000    1.0000        63\n","           6     1.0000    1.0000    1.0000        63\n","           7     1.0000    1.0000    1.0000        63\n","\n","    accuracy                         0.9762       504\n","   macro avg     0.9765    0.9762    0.9762       504\n","weighted avg     0.9765    0.9762    0.9762       504\n","\n","Test Black and White\n","              precision    recall  f1-score   support\n","\n","           0     0.2419    0.2381    0.2400        63\n","           1     0.0000    0.0000    0.0000        63\n","           2     0.0000    0.0000    0.0000        63\n","           3     0.4000    0.0317    0.0588        63\n","           4     0.2500    0.0159    0.0299        63\n","           5     0.0000    0.0000    0.0000        63\n","           6     0.5806    0.2857    0.3830        63\n","           7     0.1570    0.9841    0.2707        63\n","\n","    accuracy                         0.1944       504\n","   macro avg     0.2037    0.1944    0.1228       504\n","weighted avg     0.2037    0.1944    0.1228       504\n","\n","Test Original + Black and White\n","              precision    recall  f1-score   support\n","\n","           0     0.9839    0.9683    0.9760        63\n","           1     1.0000    0.9524    0.9756        63\n","           2     0.9615    0.7937    0.8696        63\n","           3     1.0000    0.9365    0.9672        63\n","           4     1.0000    0.9365    0.9672        63\n","           5     1.0000    0.9841    0.9920        63\n","           6     1.0000    0.9841    0.9920        63\n","           7     0.7159    1.0000    0.8344        63\n","\n","    accuracy                         0.9444       504\n","   macro avg     0.9577    0.9444    0.9468       504\n","weighted avg     0.9577    0.9444    0.9468       504\n","\n","Test Original + Flip X + Flip Y + BW\n","              precision    recall  f1-score   support\n","\n","           0     0.9683    0.9683    0.9683        63\n","           1     0.9844    1.0000    0.9921        63\n","           2     0.9206    0.9206    0.9206        63\n","           3     0.9385    0.9683    0.9531        63\n","           4     1.0000    0.9524    0.9756        63\n","           5     1.0000    1.0000    1.0000        63\n","           6     1.0000    1.0000    1.0000        63\n","           7     1.0000    1.0000    1.0000        63\n","\n","    accuracy                         0.9762       504\n","   macro avg     0.9765    0.9762    0.9762       504\n","weighted avg     0.9765    0.9762    0.9762       504\n","\n","Finished: /content/drive/MyDrive/output/JP30N02--1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["work_on_keffnet(show_model=False, run_fit=False, test_results=False, calc_f1=True)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"JP30N02 - kEffNet - colorectal - RAM - 4xVal - baseline - 1000epochs.ipynb","provenance":[],"machine_shape":"hm","background_execution":"on"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":0}